[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Calcite parser for SparkSQL 0.1.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-calcite-parser ---
[INFO] Deleting /home/cherry/spark-calcite-parser/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ spark-calcite-parser ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cherry/spark-calcite-parser/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ spark-calcite-parser ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-scala-plugin:2.15.2:compile (default) @ spark-calcite-parser ---
[INFO] Checking for multiple versions of scala
[WARNING]  Expected all dependencies to require Scala version: 2.10.4
[WARNING]  com.intel.ssg.bdt:spark-calcite-parser:0.1.0-SNAPSHOT requires scala version: 2.10.4
[WARNING]  com.intel.ssg.bdt:spark-calcite-parser:0.1.0-SNAPSHOT requires scala version: 2.10.4
[WARNING]  org.scala-lang:scala-compiler:2.10.4 requires scala version: 2.10.4
[WARNING]  org.scala-lang:scala-reflect:2.10.4 requires scala version: 2.10.4
[WARNING]  com.fasterxml.jackson.module:jackson-module-scala_2.10:2.4.4 requires scala version: 2.10.4
[WARNING]  com.twitter:chill_2.10:0.5.0 requires scala version: 2.10.4
[WARNING]  com.typesafe.akka:akka-remote_2.10:2.3.11 requires scala version: 2.10.4
[WARNING]  com.typesafe.akka:akka-actor_2.10:2.3.11 requires scala version: 2.10.4
[WARNING]  com.typesafe.akka:akka-slf4j_2.10:2.3.11 requires scala version: 2.10.4
[WARNING]  org.apache.spark:spark-core_2.10:1.5.0 requires scala version: 2.10.4
[WARNING]  org.json4s:json4s-jackson_2.10:3.2.10 requires scala version: 2.10.0
[WARNING] Multiple versions of scala libraries detected!
[INFO] includes = [**/*.java,**/*.scala,]
[INFO] excludes = []
[INFO] /home/cherry/spark-calcite-parser/src/main/scala:-1: info: compiling
[INFO] Compiling 6 source files to /home/cherry/spark-calcite-parser/target/classes at 1446778189456
[INFO] prepare-compile in 0 s
[INFO] compile in 10 s
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ spark-calcite-parser ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 20942 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ spark-calcite-parser ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-scala-plugin:2.15.2:testCompile (default) @ spark-calcite-parser ---
[INFO] Checking for multiple versions of scala
[WARNING]  Expected all dependencies to require Scala version: 2.10.4
[WARNING]  com.intel.ssg.bdt:spark-calcite-parser:0.1.0-SNAPSHOT requires scala version: 2.10.4
[WARNING]  com.intel.ssg.bdt:spark-calcite-parser:0.1.0-SNAPSHOT requires scala version: 2.10.4
[WARNING]  org.scala-lang:scala-compiler:2.10.4 requires scala version: 2.10.4
[WARNING]  org.scala-lang:scala-reflect:2.10.4 requires scala version: 2.10.4
[WARNING]  com.fasterxml.jackson.module:jackson-module-scala_2.10:2.4.4 requires scala version: 2.10.4
[WARNING]  com.twitter:chill_2.10:0.5.0 requires scala version: 2.10.4
[WARNING]  com.typesafe.akka:akka-remote_2.10:2.3.11 requires scala version: 2.10.4
[WARNING]  com.typesafe.akka:akka-actor_2.10:2.3.11 requires scala version: 2.10.4
[WARNING]  com.typesafe.akka:akka-slf4j_2.10:2.3.11 requires scala version: 2.10.4
[WARNING]  org.apache.spark:spark-core_2.10:1.5.0 requires scala version: 2.10.4
[WARNING]  org.json4s:json4s-jackson_2.10:3.2.10 requires scala version: 2.10.0
[WARNING] Multiple versions of scala libraries detected!
[INFO] includes = [**/*.java,**/*.scala,]
[INFO] excludes = []
[INFO] /home/cherry/spark-calcite-parser/src/test/scala:-1: info: compiling
[INFO] Compiling 1 source files to /home/cherry/spark-calcite-parser/target/test-classes at 1446778205732
[INFO] prepare-compile in 0 s
[INFO] compile in 9 s
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ spark-calcite-parser ---
[INFO] 
[INFO] --- scalatest-maven-plugin:1.0:test (test) @ spark-calcite-parser ---
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
[36mDiscovery starting.[0m
[36mDiscovery completed in 18 seconds, 597 milliseconds.[0m
[36mRun starting. Expected test count is: 1015[0m
[32mHiveCompSuite:[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ
[32m- 0000[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0001[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0002[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF3
[32m- 0003[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VTABLE
[32m- 0004[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM UPUNIQ
[32m- 0005[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.AA
[32m- 0006[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.BB
[32m- 0007[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.CC
[32m- 0008[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.DD
[32m- 0009[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.EE
[32m- 0010[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.FF
[32m- 0011[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.GG
[32m- 0012[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.HH
[32m- 0013[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.SRCH1
[32m- 0014[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.BADG1
[32m- 0015[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.BADG2
[32m- 0016[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM BASE_VS1
[32m- 0017[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM USIG
[32m- 0018[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM U_SIG
[32m- 0019[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ
[32m- 0020[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0021[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0022[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_M
[32m- 0023[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ_M
[32m- 0024[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_C
[32m- 0025[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_P
[32m- 0026[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_F
[32m- 0027[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_P
[32m- 0028[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F1
[32m- 0029[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F2
[32m- 0030[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F3
[32m- 0031[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F4
[32m- 0032[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F5
[32m- 0033[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F6
[32m- 0034[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F7
[32m- 0035[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F8
[32m- 0036[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F9
[32m- 0037[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F10
[32m- 0038[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P1
[32m- 0039[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P2
[32m- 0040[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P3
[32m- 0041[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P4
[32m- 0042[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P5
[32m- 0043[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P6
[32m- 0044[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P7
[32m- 0045[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P8
[32m- 0046[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P9
[32m- 0047[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P10
[32m- 0048[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_F
[32m- 0049[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM DEPT
[32m- 0050[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM EMP
[32m- 0051[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM EXPERIENCE
[32m- 0052[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_P
[32m- 0053[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ_P
[32m- 0054[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM ACR_SCH_P
[32m- 0055[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS_P
[32m- 0056[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TTT
[32m- 0057[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE SUBSTR(EMPNAME,1,3) = 'Ali'
[32m- 0058[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE ABS(GRADE) = 12
[32m- 0059[0m
Calcite parsing passed, start to transform. SELECT * FROM CANWEPARSELENGTH18.CHARACTERS18VIEW18
[32m- 0060[0m
Calcite parsing passed, start to transform. SELECT CORRELATIONNAMES18.CHARS18NAME18CHARS FROM CHARACTER18TABLE18 CORRELATIONNAMES18 WHERE CORRELATIONNAMES18.CHARS18NAME18CHARS = 'VAL4'
[32m- 0061[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM HU.WORKS WHERE EMPNUM='E8'
[32m- 0063[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM DV1
[32m- 0064[0m
Calcite parsing passed, start to transform. SELECT HOURS FROM DV1 ORDER BY HOURS DESC
[32m- 0065[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VS2 WHERE C1 = 0
[32m- 0066[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VS2 WHERE C1 = 1
[32m- 0067[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VS3
[32m- 0068[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VS4
[32m- 0069[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VS5
[32m- 0070[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VS6
[32m- 0071[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM USIG WHERE C1 = 0
[32m- 0072[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM USIG WHERE C1 = 2
[32m- 0073[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM USIG WHERE C_1 = 0
[32m- 0074[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM USIG WHERE C_1 = 2
[32m- 0075[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM USIG WHERE C1 = 4
[32m- 0076[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM U_SIG WHERE C1 = 0
[32m- 0077[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM U_SIG WHERE C1 = 4
[32m- 0078[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF U_CN WHERE U_CN.GRADE IN (SELECT UCN.GRADE FROM HU.STAFF UCN WHERE UCN.GRADE > 10)
calcite cannot do SELECT COUNT(*) FROM HU.STAFF U_CN WHERE U_CN.GRADE IN (SELECT UCN.GRADE FROM HU.STAFF UCN WHERE UCN.GRADE > 10)
[31m- 0079 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE GRADE > 10
[32m- 0080[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < 10
[32m- 0081[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, EMPNAME, GRADE, CITY FROM HU.STAFF3 WHERE EMPNUM = 'E1'
[32m- 0082[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF3 WHERE CITY = 'Greenmount' OR GRADE = 15
[32m- 0083[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF3
[32m- 0084[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF3
[32m- 0085[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, EMPNAME, GRADE, CITY FROM HU.VSTAFF3 WHERE EMPNUM = 'E1'
[32m- 0086[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.VSTAFF3 WHERE CITY = 'Greenmount' OR GRADE = 15
[32m- 0087[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.VSTAFF3
[32m- 0088[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.VSTAFF3
[32m- 0089[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,HOURS FROM WORKS WHERE PNUM='P2' ORDER BY EMPNUM DESC
[32m- 0090[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,HOURS FROM WORKS WHERE PNUM='P2' ORDER BY 2 ASC
[32m- 0091[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,HOURS FROM WORKS WHERE PNUM = 'P2' ORDER BY 2 DESC,EMPNUM DESC
[32m- 0092[0m
Calcite parsing passed, start to transform. SELECT WORKS.EMPNUM FROM WORKS WHERE WORKS.PNUM = 'P2' UNION SELECT STAFF.EMPNUM FROM STAFF WHERE STAFF.GRADE=13 ORDER BY 1 DESC
calcite cannot do SELECT WORKS.EMPNUM FROM WORKS WHERE WORKS.PNUM = 'P2' UNION SELECT STAFF.EMPNUM FROM STAFF WHERE STAFF.GRADE=13 ORDER BY 1 DESC
[31m- 0093 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT WORKS.EMPNUM FROM WORKS WHERE WORKS.PNUM = 'P2' UNION ALL SELECT STAFF.EMPNUM FROM STAFF WHERE STAFF.GRADE = 13
[32m- 0094[0m
Calcite parsing passed, start to transform. SELECT EMPNAME,PNUM,HOURS FROM STAFF,WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM UNION SELECT EMPNAME,PNUM,HOURS FROM STAFF,WORKS WHERE NOT EXISTS (SELECT HOURS FROM WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM)
calcite cannot do SELECT EMPNAME,PNUM,HOURS FROM STAFF,WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM UNION SELECT EMPNAME,PNUM,HOURS FROM STAFF,WORKS WHERE NOT EXISTS (SELECT HOURS FROM WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM)
[31m- 0095 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=80 UNION SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=40 UNION SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=20 ORDER BY 3,1
calcite cannot do SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=80 UNION SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=40 UNION SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=20 ORDER BY 3,1
[31m- 0096 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=12 UNION ALL (SELECT PNUM,EMPNUM,HOURS FROM WORKS UNION SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=80) ORDER BY 2,1
calcite cannot do SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=12 UNION ALL (SELECT PNUM,EMPNUM,HOURS FROM WORKS UNION SELECT PNUM,EMPNUM,HOURS FROM WORKS WHERE HOURS=80) ORDER BY 2,1
[31m- 0097 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,HOURS FROM WORKS WHERE PNUM = 'P8' ORDER BY EMPNUM DESC
[32m- 0098[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM WORKS WHERE HOURS IS NULL
[32m- 0099[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, HOURS FROM WORKS WHERE PNUM = 'P9' ORDER BY EMPNUM DESC
[32m- 0100[0m
Calcite parsing passed, start to transform. SELECT ALL EMPNUM FROM WORKS WHERE HOURS = 12
[32m- 0101[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM WORKS WHERE HOURS = 12
[32m- 0102[0m
Calcite parsing passed, start to transform. SELECT DISTINCT EMPNUM FROM WORKS WHERE HOURS = 12
[32m- 0103[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM WORKS WHERE EMPNUM = 'E16'
[32m- 0104[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,HOURS FROM WORKS WHERE EMPNUM = 'E1' AND PNUM = 'P4'
[32m- 0105[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,HOURS FROM   WORKS WHERE  EMPNUM='E18' AND PNUM='P18'
[32m- 0106[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM   WORKS WHERE  HOURS IS NULL
[32m- 0107[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S
[32m- 0108[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S
[32m- 0109[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S
[32m- 0110[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0111[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0112[0m
Calcite parsing passed, start to transform. SELECT * FROM TMP WHERE T2 = 23 AND T3 = 'xxxx'
[32m- 0113[0m
Calcite parsing passed, start to transform. SELECT * FROM TMP WHERE T2 = 23
[32m- 0114[0m
Calcite parsing passed, start to transform. SELECT * FROM   TMP WHERE  T2 IS NULL
[32m- 0115[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_SS WHERE GRADE = 15
[32m- 0116[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE GRADE = 26
[32m- 0117[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  GRADE=130
[32m- 0118[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE GRADE = 11
[32m- 0119[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE GRADE = 11
[32m- 0120[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0121[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0122[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0123[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0124[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0125[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0126[0m
Calcite parsing passed, start to transform. SELECT COUNT(DISTINCT HOURS) FROM WORKS
[32m- 0127[0m
Calcite parsing passed, start to transform. SELECT SUM(ALL HOURS) FROM WORKS
[32m- 0128[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS) FROM WORKS
[32m- 0129[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0130[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS) FROM WORKS WHERE PNUM = 'P2'
[32m- 0131[0m
Calcite parsing passed, start to transform. SELECT SUM(DISTINCT HOURS) FROM WORKS WHERE PNUM = 'P2'
[32m- 0132[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS)+10 FROM WORKS WHERE PNUM = 'P2'
[32m- 0133[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF WHERE GRADE = (SELECT MAX(GRADE) FROM STAFF) ORDER BY EMPNUM
calcite cannot do SELECT EMPNUM FROM STAFF WHERE GRADE = (SELECT MAX(GRADE) FROM STAFF) ORDER BY EMPNUM
[31m- 0134 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF WHERE GRADE = (SELECT MIN(GRADE) FROM STAFF)
calcite cannot do SELECT EMPNUM FROM STAFF WHERE GRADE = (SELECT MIN(GRADE) FROM STAFF)
[31m- 0135 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT AVG(GRADE) FROM STAFF
[32m- 0136[0m
Calcite parsing passed, start to transform. SELECT AVG(GRADE) FROM   TEMP_S
[32m- 0137[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM PROJ WHERE BUDGET BETWEEN 40000 AND 60000
[32m- 0138[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM PROJ WHERE BUDGET >= 40000 AND BUDGET <= 60000
[32m- 0139[0m
Calcite parsing passed, start to transform. SELECT CITY FROM STAFF WHERE GRADE NOT BETWEEN 12 AND 13
[32m- 0140[0m
Calcite parsing passed, start to transform. SELECT CITY FROM STAFF WHERE NOT(GRADE BETWEEN 12 AND 13)
[32m- 0141[0m
Calcite parsing passed, start to transform. SELECT STAFF.EMPNAME FROM STAFF WHERE STAFF.EMPNUM IN (SELECT WORKS.EMPNUM FROM WORKS WHERE WORKS.PNUM IN (SELECT PROJ.PNUM FROM PROJ WHERE PROJ.CITY='Tampa'))
calcite cannot do SELECT STAFF.EMPNAME FROM STAFF WHERE STAFF.EMPNUM IN (SELECT WORKS.EMPNUM FROM WORKS WHERE WORKS.PNUM IN (SELECT PROJ.PNUM FROM PROJ WHERE PROJ.CITY='Tampa'))
[31m- 0142 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0143 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'WORKS' '.' in function specification; line 1 pos 58[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT WORKS.HOURS FROM WORKS WHERE WORKS.PNUM NOT IN (SELECT PROJ.PNUM FROM PROJ WHERE PROJ.BUDGET BETWEEN 5000 AND 40000)
calcite cannot do SELECT WORKS.HOURS FROM WORKS WHERE WORKS.PNUM NOT IN (SELECT PROJ.PNUM FROM PROJ WHERE PROJ.BUDGET BETWEEN 5000 AND 40000)
[31m- 0144 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT WORKS.HOURS FROM WORKS WHERE NOT (WORKS.PNUM IN (SELECT PROJ.PNUM FROM PROJ WHERE PROJ.BUDGET BETWEEN 5000 AND 40000))
calcite cannot do SELECT WORKS.HOURS FROM WORKS WHERE NOT (WORKS.PNUM IN (SELECT PROJ.PNUM FROM PROJ WHERE PROJ.BUDGET BETWEEN 5000 AND 40000))
[31m- 0145 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT HOURS FROM WORKS WHERE PNUM NOT IN (SELECT PNUM FROM WORKS WHERE PNUM IN ('P1','P2','P4','P5','P6'))
calcite cannot do SELECT HOURS FROM WORKS WHERE PNUM NOT IN (SELECT PNUM FROM WORKS WHERE PNUM IN ('P1','P2','P4','P5','P6'))
[31m- 0146 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT HOURS FROM WORKS WHERE NOT (PNUM IN (SELECT PNUM FROM WORKS WHERE PNUM IN ('P1','P2','P4','P5','P6')))
calcite cannot do SELECT HOURS FROM WORKS WHERE NOT (PNUM IN (SELECT PNUM FROM WORKS WHERE PNUM IN ('P1','P2','P4','P5','P6')))
[31m- 0147 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF WHERE EMPNAME LIKE 'Al%'
[32m- 0148[0m
Calcite parsing passed, start to transform. SELECT CITY FROM STAFF WHERE EMPNAME LIKE 'B__t%'
[32m- 0149[0m
Calcite parsing passed, start to transform. SELECT CITY FROM STAFF WHERE CITY LIKE 'XiS___S%%' ESCAPE 'S'
[32m- 0150[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE EMPNUM  NOT LIKE '_36'
[32m- 0151[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE NOT(EMPNUM  LIKE '_36')
[32m- 0152[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF WHERE CITY IS NULL
[32m- 0153[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0154[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE CITY IS NOT NULL
[32m- 0155[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE NOT (CITY IS NULL)
[32m- 0156[0m
Calcite parsing passed, start to transform. SELECT STAFF.EMPNAME FROM STAFF WHERE NOT EXISTS (SELECT * FROM PROJ WHERE NOT EXISTS (SELECT * FROM WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM AND WORKS.PNUM=PROJ.PNUM))
calcite cannot do SELECT STAFF.EMPNAME FROM STAFF WHERE NOT EXISTS (SELECT * FROM PROJ WHERE NOT EXISTS (SELECT * FROM WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM AND WORKS.PNUM=PROJ.PNUM))
[31m- 0157 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0158 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'BUDGET' 'FROM' in function specification; line 1 pos 42[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0159 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'BUDGET' '/' in function specification; line 1 pos 46[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0160 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'BUDGET' '/' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S
[32m- 0161[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S
[32m- 0162[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S
[32m- 0163[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM WORKS WHERE PNUM > 'P1' GROUP BY PNUM HAVING COUNT(*) > 1
[32m- 0164[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM WORKS GROUP BY PNUM HAVING COUNT(*) > 2
[32m- 0165[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, PNUM, HOURS FROM WORKS GROUP BY PNUM, EMPNUM, HOURS HAVING MIN(HOURS) > 12 AND MAX(HOURS) < 80
[32m- 0166[0m
Calcite parsing passed, start to transform. SELECT WORKS.PNUM FROM WORKS GROUP BY WORKS.PNUM HAVING WORKS.PNUM IN (SELECT PROJ.PNUM FROM PROJ GROUP BY PROJ.PNUM HAVING SUM(PROJ.BUDGET) > 25000)
calcite cannot do SELECT WORKS.PNUM FROM WORKS GROUP BY WORKS.PNUM HAVING WORKS.PNUM IN (SELECT PROJ.PNUM FROM PROJ GROUP BY PROJ.PNUM HAVING SUM(PROJ.BUDGET) > 25000)
[31m- 0167 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS) FROM WORKS HAVING MIN(PNUM) > 'P0'
[32m- 0168[0m
Calcite parsing passed, start to transform. SELECT PNUM, SUM(HOURS) FROM WORKS GROUP BY PNUM
[32m- 0169[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM WORKS GROUP BY EMPNUM
[32m- 0170[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,HOURS FROM WORKS GROUP BY EMPNUM,HOURS
[32m- 0171[0m
Calcite parsing passed, start to transform. SELECT * FROM WORKS GROUP BY PNUM,EMPNUM,HOURS
[32m- 0172[0m
Calcite parsing passed, start to transform. SELECT PNUM,EMPNUM FROM WORKS GROUP BY EMPNUM,PNUM,HOURS
[32m- 0173[0m
Calcite parsing passed, start to transform. SELECT SUM(GRADE) FROM STAFF WHERE CITY IS NULL GROUP BY CITY
[32m- 0174[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0175[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,EMPNAME,GRADE,STAFF.CITY, PNAME, PROJ.CITY FROM STAFF, PROJ WHERE STAFF.CITY = PROJ.CITY
[32m- 0176[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,EMPNAME,GRADE,STAFF.CITY,PNUM,PNAME, PTYPE,BUDGET,PROJ.CITY FROM STAFF, PROJ WHERE STAFF.CITY = PROJ.CITY AND GRADE <> 12
[32m- 0177[0m
Calcite parsing passed, start to transform. SELECT DISTINCT STAFF.CITY, PROJ.CITY FROM STAFF, WORKS, PROJ WHERE STAFF.EMPNUM = WORKS.EMPNUM AND WORKS.PNUM = PROJ.PNUM
[32m- 0178[0m
Calcite parsing passed, start to transform. SELECT FIRST1.EMPNUM, SECOND2.EMPNUM FROM STAFF FIRST1, STAFF SECOND2 WHERE FIRST1.CITY = SECOND2.CITY AND FIRST1.EMPNUM < SECOND2.EMPNUM
[32m- 0179[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM  AA
[32m- 0180[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM BB
[32m- 0181[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CC
[32m- 0182[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM DD
[32m- 0183[0m
Calcite parsing passed, start to transform. SELECT INTTEST FROM EE
[32m- 0184[0m
Calcite parsing passed, start to transform. SELECT INTTEST FROM FF
[32m- 0185[0m
Calcite parsing passed, start to transform. SELECT * FROM HH
[32m- 0186[0m
Calcite parsing passed, start to transform. SELECT * FROM MM
[32m- 0187[0m
Calcite parsing passed, start to transform. SELECT * FROM NN
[32m- 0188[0m
Calcite parsing passed, start to transform. SELECT NUMTEST FROM OO
[32m- 0189[0m
Calcite parsing passed, start to transform. SELECT * FROM QQ
[32m- 0190[0m
Calcite parsing passed, start to transform. SELECT * FROM RR
[32m- 0191[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF WHERE GRADE < (SELECT MAX(GRADE) FROM STAFF)
calcite cannot do SELECT EMPNUM FROM STAFF WHERE GRADE < (SELECT MAX(GRADE) FROM STAFF)
[31m- 0192 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT * FROM STAFF WHERE GRADE <= (SELECT AVG(GRADE)-1 FROM STAFF)
calcite cannot do SELECT * FROM STAFF WHERE GRADE <= (SELECT AVG(GRADE)-1 FROM STAFF)
[31m- 0193 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS WHERE PNUM = 'P2') ORDER BY EMPNAME
calcite cannot do SELECT EMPNAME FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS WHERE PNUM = 'P2') ORDER BY EMPNAME
[31m- 0194 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS WHERE PNUM IN (SELECT PNUM FROM PROJ WHERE PTYPE = 'Design'))
calcite cannot do SELECT EMPNAME FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS WHERE PNUM IN (SELECT PNUM FROM PROJ WHERE PTYPE = 'Design'))
[31m- 0195 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, EMPNAME FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS WHERE PNUM IN (SELECT PNUM FROM PROJ WHERE PTYPE IN (SELECT PTYPE FROM PROJ WHERE PNUM IN (SELECT PNUM FROM WORKS WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS WHERE PNUM IN (SELECT PNUM FROM PROJ WHERE PTYPE = 'Design')))))) ORDER BY EMPNUM
calcite cannot do SELECT EMPNUM, EMPNAME FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS WHERE PNUM IN (SELECT PNUM FROM PROJ WHERE PTYPE IN (SELECT PTYPE FROM PROJ WHERE PNUM IN (SELECT PNUM FROM WORKS WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS WHERE PNUM IN (SELECT PNUM FROM PROJ WHERE PTYPE = 'Design')))))) ORDER BY EMPNUM
[31m- 0196 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0197 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'AVG' '(' in function specification; line 1 pos 53[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT DISTINCT EMPNUM FROM WORKS WORKSX WHERE NOT EXISTS (SELECT * FROM WORKS WORKSY WHERE EMPNUM = 'E2' AND NOT EXISTS (SELECT * FROM WORKS WORKSZ WHERE WORKSZ.EMPNUM = WORKSX.EMPNUM AND WORKSZ.PNUM = WORKSY.PNUM))
calcite cannot do SELECT DISTINCT EMPNUM FROM WORKS WORKSX WHERE NOT EXISTS (SELECT * FROM WORKS WORKSY WHERE EMPNUM = 'E2' AND NOT EXISTS (SELECT * FROM WORKS WORKSZ WHERE WORKSZ.EMPNUM = WORKSX.EMPNUM AND WORKSZ.PNUM = WORKSY.PNUM))
[31m- 0198 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM PROJ WHERE PROJ.CITY = (SELECT STAFF.CITY FROM STAFF WHERE EMPNUM = 'E1')
calcite cannot do SELECT PNUM FROM PROJ WHERE PROJ.CITY = (SELECT STAFF.CITY FROM STAFF WHERE EMPNUM = 'E1')
[31m- 0199 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM PROJ WHERE PROJ.CITY = (SELECT STAFF.CITY FROM STAFF WHERE EMPNUM > 'E1' )
calcite cannot do SELECT PNUM FROM PROJ WHERE PROJ.CITY = (SELECT STAFF.CITY FROM STAFF WHERE EMPNUM > 'E1' )
[31m- 0200 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE STAFF.CITY = (SELECT PROJ.CITY FROM PROJ WHERE PNUM > 'P7')
calcite cannot do SELECT COUNT(*) FROM STAFF WHERE STAFF.CITY = (SELECT PROJ.CITY FROM PROJ WHERE PNUM > 'P7')
[31m- 0201 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE NOT (STAFF.CITY = (SELECT PROJ.CITY FROM PROJ WHERE PNUM > 'P7' ))
calcite cannot do SELECT COUNT(*) FROM STAFF WHERE NOT (STAFF.CITY = (SELECT PROJ.CITY FROM PROJ WHERE PNUM > 'P7' ))
[31m- 0202 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM PROJ WHERE CITY <> 'Deale'
[32m- 0203[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE EMPNUM = 'E1'
[32m- 0204[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE EMPNUM = 'E1' AND EMPNUM = 'E1'
[32m- 0205[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,GRADE FROM   STAFF ORDER  BY GRADE,EMPNUM
[32m- 0206[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,CITY FROM   STAFF WHERE  EMPNUM='E1' OR NOT(EMPNUM='E1')
[32m- 0208[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,CITY FROM   STAFF WHERE  EMPNUM='E1' AND NOT(EMPNUM='E1')
[32m- 0209[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM   WORKS WHERE HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8') OR NOT(HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8'))
calcite cannot do SELECT EMPNUM,PNUM FROM   WORKS WHERE HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8') OR NOT(HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8'))
[31m- 0210 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM   WORKS WHERE HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8') AND NOT(HOURS< (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8'))
calcite cannot do SELECT EMPNUM,PNUM FROM   WORKS WHERE HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8') AND NOT(HOURS< (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8'))
[31m- 0211 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM   WORKS WHERE HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8') AND   HOURS IN (SELECT HOURS FROM WORKS)
calcite cannot do SELECT EMPNUM,PNUM FROM   WORKS WHERE HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8') AND   HOURS IN (SELECT HOURS FROM WORKS)
[31m- 0212 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM   WORKS WHERE HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8') OR    HOURS IN (SELECT HOURS FROM WORKS) ORDER BY EMPNUM
calcite cannot do SELECT EMPNUM,PNUM FROM   WORKS WHERE HOURS < (SELECT HOURS FROM WORKS WHERE EMPNUM = 'E8') OR    HOURS IN (SELECT HOURS FROM WORKS) ORDER BY EMPNUM
[31m- 0213 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS),AVG(HOURS),MIN(HOURS),MAX(HOURS) FROM    WORKS WHERE   EMPNUM='E1'
[32m- 0214[0m
Calcite parsing passed, start to transform. SELECT PNUM,AVG(HOURS),MIN(HOURS),MAX(HOURS) FROM    WORKS WHERE   EMPNUM='E8' GROUP BY PNUM
[32m- 0215[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS),AVG(HOURS),MIN(HOURS),MAX(HOURS) FROM    WORKS WHERE   EMPNUM='E8' GROUP BY PNUM
[32m- 0216[0m
Calcite parsing passed, start to transform. SELECT PNUM,AVG(HOURS),MIN(HOURS),MAX(HOURS) FROM    WORKS GROUP BY PNUM ORDER BY PNUM
[32m- 0217[0m
Calcite parsing passed, start to transform. SELECT +MAX(DISTINCT HOURS) FROM WORKS
[32m- 0218[0m
Calcite parsing passed, start to transform. SELECT -MAX(DISTINCT HOURS) FROM WORKS
[32m- 0219[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM WORKS1 WHERE HOURS IS NULL
[32m- 0220[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE EMPNUM='E9'
[32m- 0221[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE HOURS IS NULL
[32m- 0222[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VTABLE
[32m- 0223[0m
Calcite parsing passed, start to transform. SELECT +COL1+COL2 - COL3*COL4/COL1 FROM VTABLE WHERE COL1=10
[32m- 0224[0m
Calcite parsing passed, start to transform. SELECT (-COL2+COL1)*COL3 - COL3/COL1 FROM VTABLE WHERE COL4 IS NULL
[32m- 0225[0m
Calcite parsing passed, start to transform. SELECT COUNT(*),SUM(NUMKEY) FROM UPUNIQ
[32m- 0226[0m
Calcite parsing passed, start to transform. SELECT COUNT(*),SUM(NUMKEY) FROM UPUNIQ
[32m- 0227[0m
Calcite parsing passed, start to transform. SELECT GRADE,CITY FROM STAFF WHERE EMPNUM = 'E8'
[32m- 0228[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JJ WHERE FLOATTEST > 123455 AND FLOATTEST < 123457
[32m- 0229[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JJ WHERE FLOATTEST > 122 AND FLOATTEST < 124
[32m- 0230[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JJ WHERE FLOATTEST > -124 AND FLOATTEST < -122
[32m- 0231[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM WORKS WHERE EMPNUM='UPP' AND PNUM='low'
[32m- 0232[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM WORKS WHERE EMPNUM='upp' OR PNUM='LOW'
[32m- 0233[0m
Calcite parsing passed, start to transform. SELECT REALTEST FROM GG
[32m- 0234[0m
Calcite parsing passed, start to transform. SELECT * FROM GG WHERE REALTEST > 1.234561 and REALTEST < 1.234573
[32m- 0235[0m
Calcite parsing passed, start to transform. SELECT DOUBLETEST FROM II
[32m- 0236[0m
Calcite parsing passed, start to transform. SELECT * FROM II WHERE DOUBLETEST > 123456.123450 and DOUBLETEST < 123456.123462
[32m- 0237[0m
Calcite parsing passed, start to transform. SELECT FLOATTEST FROM JJ
[32m- 0238[0m
Calcite parsing passed, start to transform. SELECT * FROM JJ WHERE FLOATTEST > 12.345672 and FLOATTEST < 12.345684
[32m- 0239[0m
Calcite parsing passed, start to transform. SELECT FLOATTEST FROM KK
[32m- 0240[0m
Calcite parsing passed, start to transform. SELECT * FROM KK WHERE FLOATTEST > 123456.123450 and FLOATTEST < 123456.123462
[32m- 0241[0m
Calcite parsing passed, start to transform. SELECT * FROM LL
[32m- 0242[0m
Calcite parsing passed, start to transform. SELECT * FROM LL WHERE NUMTEST > 123456.123450 and NUMTEST < 123456.123462
[32m- 0243[0m
Calcite parsing passed, start to transform. SELECT * FROM PP
[32m- 0244[0m
Calcite parsing passed, start to transform. SELECT * FROM SS
[32m- 0245[0m
Calcite parsing passed, start to transform. SELECT FLOATTEST FROM JJ ORDER BY FLOATTEST DESC
[32m- 0246[0m
Calcite parsing passed, start to transform. SELECT * FROM TEXT240
[32m- 0247[0m
Calcite parsing passed, start to transform. SELECT GRADE, HOURS, BUDGET FROM STAFF, WORKS, PROJ
[32m- 0248[0m
Calcite parsing passed, start to transform. SELECT CITY FROM STAFF WHERE EMPNAME LIKE 'yan____%'
[32m- 0249[0m
Calcite parsing passed, start to transform. SELECT CITY FROM STAFF WHERE EMPNAME LIKE 'YAN____%'
[32m- 0250[0m
Calcite parsing passed, start to transform. SELECT COL1, EMPNUM, GRADE FROM CUGINI.VTABLE, STAFF WHERE COL1 < 200 AND GRADE > 12
[32m- 0251[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE EMPNUM = 'E9'
[32m- 0252[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE HOURS > 85
[32m- 0253[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM   PROJ WHERE  PNAME BETWEEN 'A' AND 'F'
[32m- 0254[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM   PROJ WHERE PNAME >= 'A' AND PNAME <= 'F'
[32m- 0255[0m
Calcite parsing passed, start to transform. SELECT CITY FROM   STAFF WHERE  EMPNAME NOT BETWEEN 'A' AND 'E'
[32m- 0256[0m
Calcite parsing passed, start to transform. SELECT CITY FROM   STAFF WHERE  NOT( EMPNAME BETWEEN 'A' AND 'E' )
[32m- 0257[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM   STAFF WHERE  EMPNAME LIKE 'Ali%'
[32m- 0258[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM   STAFF WHERE  EMPNAME LIKE 'ALI%'
[32m- 0259[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S WHERE EMPNUM='E1' AND GRADE=11 AND CITY='Deale'
[32m- 0260[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF1
[32m- 0261[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF1 WHERE GRADE > 12
[32m- 0262[0m
Calcite parsing passed, start to transform. SELECT SUM(GRADE) FROM STAFF1
[32m- 0263[0m
Calcite parsing passed, start to transform. SELECT NUMKEY FROM UPUNIQ ORDER BY NUMKEY DESC
[32m- 0264[0m
Calcite parsing passed, start to transform. SELECT MAX(NUMKEY), MIN(NUMKEY) FROM UPUNIQ
[32m- 0265[0m
Calcite parsing passed, start to transform. SELECT CITY FROM PROJ1 WHERE PNUM = 'P1'
[32m- 0266[0m
Calcite parsing passed, start to transform. SELECT STR110 FROM T4 WHERE NUM6 = 100
[32m- 0267[0m
Calcite parsing passed, start to transform. SELECT STR110 FROM T4 WHERE NUM6 = 101
[32m- 0268[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM T4 WHERE STR110 LIKE '%HU%'
[32m- 0270[0m
Calcite parsing passed, start to transform. SELECT COL1, MAX(COL2 + COL3), MIN(COL3 - COL2) FROM VTABLE GROUP BY COL1 ORDER BY COL1
[32m- 0271[0m
Calcite parsing passed, start to transform. SELECT COL1,SUM(2 * COL2 * COL3) FROM VTABLE GROUP BY COL1 HAVING SUM(COL2 * COL3) > 2000 OR SUM(COL2 * COL3) < -2000 ORDER BY COL1
[32m- 0272[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.havingCondition(HiveParser_IdentifiersParser.java:1840)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.havingClause(HiveParser_IdentifiersParser.java:1748)
	at org.apache.hadoop.hive.ql.parse.HiveParser.havingClause(HiveParser.java:45863)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41597)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0273 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'GRADE' 'FROM' in function specification; line 1 pos 73[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COL1, MAX(COL2) FROM VTABLE GROUP BY COL1 HAVING EXISTS (SELECT * FROM STAFF WHERE EMPNUM = 'E1') AND MAX(COL2) BETWEEN 10 AND 90 ORDER BY COL1
calcite cannot do SELECT COL1, MAX(COL2) FROM VTABLE GROUP BY COL1 HAVING EXISTS (SELECT * FROM STAFF WHERE EMPNUM = 'E1') AND MAX(COL2) BETWEEN 10 AND 90 ORDER BY COL1
[31m- 0274 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT SUM(COL1) FROM VTABLE WHERE 10 + COL1 > COL2 HAVING MAX(COL1) > 100
[32m- 0275[0m
Calcite parsing passed, start to transform. SELECT SUM(COL1) FROM VTABLE WHERE 1000 + COL1 >= COL2 HAVING MAX(COL1) > 100
[32m- 0276[0m
Calcite parsing passed, start to transform. SELECT COL1, COL2 FROM VTABLE WHERE(2*(COL3 - COL2)) BETWEEN 5 AND 200 ORDER BY COL1
[32m- 0277[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0278 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'HOURS' 'FROM' in function specification; line 1 pos 62[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COL1, (COL3 * COL2/COL1 - COL2 + 10) FROM VTABLE WHERE COL1 > 0 ORDER BY 2
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'c1' given input columns COL1, _c1;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'c1' given input columns COL1, _c1;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:122)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)[0m
[31m  	at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:122)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['c1 ASC], true[0m
[31m   'Project [unresolvedalias('COL1),unresolvedalias((((('COL3 * 'COL2) / 'COL1) - 'COL2) + 10))][0m
[31m    'Filter ('COL1 > 0)[0m
[31m     'UnresolvedRelation [VTABLE], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  COL1: double, _c1: double[0m
[31m  'Sort ['c1 ASC], true[0m
[31m   Project [COL1#39156,((((COL3#39158 * COL2#39157) / COL1#39156) - COL2#39157) + cast(10 as double)) AS _c1#39161][0m
[31m    Filter (COL1#39156 > cast(0 as double))[0m
[31m     MetastoreRelation HU, vtable, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'c1' given input columns COL1, _c1;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'c1' given input columns COL1, _c1;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'c1' given input columns COL1, _c1;[0m
[31m  == HIVE - 12 row(s) ==[0m
[31m  1000	-3990[0m
[31m  1	11[0m
[31m  10	12.2[0m
[31m  10	12.2[0m
[31m  10	12.2[0m
[31m  10	12.2[0m
[31m  10	50[0m
[31m  100	410[0m
[31m  100	1133.32[0m
[31m  100	1133.32[0m
[31m  100	1133.32[0m
[31m  100	1133.32 (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, PNUM, HOURS FROM SUBSP
[32m- 0280[0m
Calcite parsing passed, start to transform. SELECT * FROM WORKS
[32m- 0281[0m
Calcite parsing passed, start to transform. SELECT * FROM WORKS
[32m- 0282[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, PNUM, HOURS FROM SUBSP
[32m- 0283[0m
Calcite parsing passed, start to transform. SELECT * FROM WORKS WHERE EMPNUM = 'E3'
[32m- 0284[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, PNUM, HOURS FROM SUBSP
[32m- 0285[0m
Calcite parsing passed, start to transform. SELECT * FROM WORKS WHERE EMPNUM = 'E3'
[32m- 0286[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS1 WHERE EMPNUM = 'P1' AND HOURS > 30
[32m- 0287[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE HOURS BETWEEN 80 AND 40
[32m- 0288[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE HOURS BETWEEN -40 AND -80
[32m- 0289[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE HOURS BETWEEN -80 AND -40
[32m- 0290[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE HOURS BETWEEN 11.999 AND 12 OR HOURS BETWEEN 19.999 AND 2.001E1
[32m- 0291[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS,STAFF WHERE WORKS.EMPNUM = 'E1'
[32m- 0292[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE EMPNUM = 'E7' OR HOURS = 31 OR HOURS = 17
[32m- 0293[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS),MAX(HOURS),MIN(HOURS),MIN(EMPNUM) FROM WORKS
[32m- 0294[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE HOURS IS NULL
[32m- 0295[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SULLIVAN1.AUTH_TABLE
[32m- 0297[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM AA
[32m- 0298[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF1,WORKS1,PROJ1 WHERE STAFF1.EMPNUM = 'E9' AND STAFF1.EMPNUM = WORKS1.EMPNUM AND PROJ1.PNUM = WORKS1.PNUM
[32m- 0299[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,SECOND2 FROM SULLIVAN1.MUL_SCH ORDER BY EMPNUM
[32m- 0300[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0301[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE 40 IN (SELECT HOURS FROM WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM)
calcite cannot do SELECT COUNT(*) FROM STAFF WHERE 40 IN (SELECT HOURS FROM WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM)
[31m- 0302 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE 40 NOT IN (SELECT HOURS FROM WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM)
calcite cannot do SELECT COUNT(*) FROM STAFF WHERE 40 NOT IN (SELECT HOURS FROM WORKS WHERE STAFF.EMPNUM = WORKS.EMPNUM)
[31m- 0303 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE EXISTS (SELECT * FROM WORKS WHERE HOURS = 40 AND STAFF.EMPNUM = WORKS.EMPNUM)
calcite cannot do SELECT COUNT(*) FROM STAFF WHERE EXISTS (SELECT * FROM WORKS WHERE HOURS = 40 AND STAFF.EMPNUM = WORKS.EMPNUM)
[31m- 0304 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE NOT EXISTS (SELECT * FROM WORKS WHERE HOURS = 40 AND STAFF.EMPNUM = WORKS.EMPNUM)
calcite cannot do SELECT COUNT(*) FROM STAFF WHERE NOT EXISTS (SELECT * FROM WORKS WHERE HOURS = 40 AND STAFF.EMPNUM = WORKS.EMPNUM)
[31m- 0305 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM PROJ
[32m- 0306[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM WORKS WHERE HOURS = 10
[32m- 0307[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM WORKS WHERE EMPNUM = 'E7'
[32m- 0308[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM WORKS
[32m- 0309[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF
[32m- 0310[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF WHERE GRADE = 13
[32m- 0311[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF WHERE GRADE = 15
[32m- 0312[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF WHERE GRADE IS NULL OR EMPNAME = 'Kathy'
[32m- 0313[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF WHERE GRADE = 15
[32m- 0314[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE EMPNAME = 'Ed'
[32m- 0315[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE EMPNAME = 'Ed '
[32m- 0316[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE EMPNAME = 'Ed                '
[32m- 0317[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE GRADE = 25
[32m- 0318[0m
Calcite parsing passed, start to transform. SELECT PNUM FROM WORKS WHERE EMPNUM = 'E1' AND HOURS IN (SELECT COL1 FROM CUGINI.VTABLE WHERE  COL1 > 50)
calcite cannot do SELECT PNUM FROM WORKS WHERE EMPNUM = 'E1' AND HOURS IN (SELECT COL1 FROM CUGINI.VTABLE WHERE  COL1 > 50)
[31m- 0319 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT * FROM WORKS1 WHERE EMPNUM = 'P2' ORDER BY EMPNUM, PNUM ASC
[32m- 0320[0m
Calcite parsing passed, start to transform. SELECT PNUM, WORKS.EMPNUM, EMPNAME, HOURS FROM WORKS, STAFF WHERE STAFF.EMPNUM = WORKS.EMPNUM ORDER BY 2
[32m- 0321[0m
Calcite parsing passed, start to transform. SELECT 'ZZ', EMPNUM, EMPNAME, -99 FROM STAFF WHERE NOT EXISTS (SELECT * FROM WORKS WHERE WORKS.EMPNUM = STAFF.EMPNUM) ORDER BY EMPNUM
calcite cannot do SELECT 'ZZ', EMPNUM, EMPNAME, -99 FROM STAFF WHERE NOT EXISTS (SELECT * FROM WORKS WHERE WORKS.EMPNUM = STAFF.EMPNUM) ORDER BY EMPNUM
[31m- 0322 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT W1.EMPNUM FROM WORKS W1 WHERE W1.PNUM = 'P2' AND NOT EXISTS (SELECT * FROM WORKS W2 WHERE W2.EMPNUM = W1.EMPNUM AND W2.PNUM = 'P1') ORDER BY 1 ASC
calcite cannot do SELECT W1.EMPNUM FROM WORKS W1 WHERE W1.PNUM = 'P2' AND NOT EXISTS (SELECT * FROM WORKS W2 WHERE W2.EMPNUM = W1.EMPNUM AND W2.PNUM = 'P1') ORDER BY 1 ASC
[31m- 0323 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT W1.EMPNUM FROM WORKS W1 WHERE W1.PNUM = 'P2' AND EXISTS (SELECT * FROM WORKS W2 WHERE W1.EMPNUM = W2.EMPNUM AND W2.PNUM = 'P1') ORDER BY EMPNUM ASC
calcite cannot do SELECT W1.EMPNUM FROM WORKS W1 WHERE W1.PNUM = 'P2' AND EXISTS (SELECT * FROM WORKS W2 WHERE W1.EMPNUM = W2.EMPNUM AND W2.PNUM = 'P1') ORDER BY EMPNUM ASC
[31m- 0324 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS), MAX(HOURS) FROM  STAFF, WORKS
[32m- 0325[0m
Calcite parsing passed, start to transform. SELECT AVG(HOURS), MIN(HOURS) FROM  STAFF, WORKS WHERE STAFF.EMPNUM = 'E2' AND STAFF.EMPNUM = WORKS.EMPNUM
[32m- 0326[0m
Calcite parsing passed, start to transform. SELECT STAFF.EMPNUM, SUM(HOURS), MIN(HOURS) FROM  STAFF, WORKS GROUP BY STAFF.EMPNUM ORDER BY 1
[32m- 0327[0m
Calcite parsing passed, start to transform. SELECT STAFF.EMPNUM, AVG(HOURS), MIN(HOURS) FROM  STAFF, WORKS WHERE STAFF.EMPNUM IN ('E1','E4','E3') AND STAFF.EMPNUM = WORKS.EMPNUM GROUP BY STAFF.EMPNUM HAVING COUNT(*) > 1 ORDER BY STAFF.EMPNUM
[32m- 0328[0m
Calcite parsing passed, start to transform. SELECT MAX(STAFF1.GRADE), SUM(STAFF1.GRADE) FROM STAFF1, STAFF GROUP BY STAFF1.CITY, STAFF.CITY
[32m- 0329[0m
Calcite parsing passed, start to transform. SELECT AVG(T1.COL4), AVG(T1.COL4 + T2.COL4), SUM(T2.COL4), COUNT(DISTINCT T1.COL4) FROM VTABLE T1, VTABLE T2
[32m- 0330[0m
Calcite parsing passed, start to transform. SELECT SUM(COST), MAX(COST), MIN(COST) FROM STAFF_WORKS_DESIGN
[32m- 0331[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS)
calcite cannot do SELECT COUNT (*) FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS)
[31m- 0332 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF1 WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS)
calcite cannot do SELECT COUNT (*) FROM STAFF1 WHERE EMPNUM IN (SELECT EMPNUM FROM WORKS)
[31m- 0333 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0334 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0335 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 46[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0336 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0337 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 46[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0338 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 46[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0339 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 47[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0340 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'PNUM' 'FROM' in function specification; line 1 pos 44[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0341 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'PNUM' 'FROM' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0342 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'PNUM' 'FROM' in function specification; line 1 pos 44[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0343 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'PNUM' 'FROM' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0344 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'PNUM' 'FROM' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0345 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'PNUM' 'FROM' in function specification; line 1 pos 46[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT PNUM, SUM(HOURS) FROM WORKS GROUP BY PNUM HAVING EXISTS (SELECT PNAME FROM PROJ WHERE PROJ.PNUM = WORKS.PNUM AND SUM(WORKS.HOURS) > PROJ.BUDGET / 200)
calcite cannot do SELECT PNUM, SUM(HOURS) FROM WORKS GROUP BY PNUM HAVING EXISTS (SELECT PNAME FROM PROJ WHERE PROJ.PNUM = WORKS.PNUM AND SUM(WORKS.HOURS) > PROJ.BUDGET / 200)
[31m- 0346 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT PTYPE, CITY FROM PROJ GROUP BY PTYPE, CITY HAVING AVG(BUDGET) > 21000
[32m- 0347[0m
Calcite parsing passed, start to transform. SELECT DISTINCT PTYPE, CITY FROM PROJ GROUP BY PTYPE, CITY HAVING AVG(BUDGET) > 21000
[31m- 0348 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'BUDGET' given input columns PTYPE, CITY;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'BUDGET' given input columns PTYPE, CITY;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$7.apply(TreeNode.scala:268)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:266)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Filter ('AVG('BUDGET) > 21000)[0m
[31m   'Distinct[0m
[31m    'Aggregate ['PTYPE,'CITY], [unresolvedalias('PTYPE),unresolvedalias('CITY)][0m
[31m     'UnresolvedRelation [PROJ], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  PTYPE: string, CITY: string[0m
[31m  'Filter ('AVG('BUDGET) > 21000)[0m
[31m   Distinct[0m
[31m    Aggregate [PTYPE#48294,CITY#48296], [PTYPE#48294,CITY#48296][0m
[31m     MetastoreRelation HU, proj, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'BUDGET' given input columns PTYPE, CITY;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'BUDGET' given input columns PTYPE, CITY;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'BUDGET' given input columns PTYPE, CITY;[0m
[31m  == HIVE - 1 row(s) ==[0m
[31m  Code	Vienna (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT DISTINCT SUM(BUDGET) FROM PROJ GROUP BY PTYPE, CITY HAVING AVG(BUDGET) > 21000
[31m- 0349 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'BUDGET' given input columns _c0;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'BUDGET' given input columns _c0;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$7.apply(TreeNode.scala:268)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:266)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Filter ('AVG('BUDGET) > 21000)[0m
[31m   'Distinct[0m
[31m    'Aggregate ['PTYPE,'CITY], [unresolvedalias('SUM('BUDGET))][0m
[31m     'UnresolvedRelation [PROJ], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  _c0: double[0m
[31m  'Filter ('AVG('BUDGET) > 21000)[0m
[31m   Distinct[0m
[31m    Aggregate [PTYPE#48405,CITY#48407], [sum(BUDGET#48406) AS _c0#48408][0m
[31m     MetastoreRelation HU, proj, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'BUDGET' given input columns _c0;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'BUDGET' given input columns _c0;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'BUDGET' given input columns _c0;[0m
[31m  == HIVE - 1 row(s) ==[0m
[31m  30000 (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM BB
[32m- 0350[0m
Calcite parsing passed, start to transform. SELECT INTTEST FROM EE
[32m- 0351[0m
Calcite parsing passed, start to transform. SELECT REALTEST FROM GG
[32m- 0352[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM GG WHERE REALTEST IS NULL
[32m- 0353[0m
Calcite parsing passed, start to transform. SELECT SMALLTEST FROM HH
[32m- 0354[0m
Calcite parsing passed, start to transform. SELECT DOUBLETEST FROM II
[32m- 0355[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM II WHERE DOUBLETEST IS NULL
[32m- 0356[0m
Calcite parsing passed, start to transform. SELECT FLOATTEST FROM JJ
[32m- 0357[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JJ WHERE FLOATTEST IS NULL
[32m- 0358[0m
Calcite parsing passed, start to transform. SELECT NUMTEST FROM MM
[32m- 0359[0m
Calcite parsing passed, start to transform. SELECT NUMTEST FROM SS
[32m- 0360[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF WHERE GRADE IS NULL
[32m- 0361[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_SS
[32m- 0362[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0363[0m
Calcite parsing passed, start to transform. SELECT * FROM STAFF
[32m- 0364[0m
Calcite parsing passed, start to transform. SELECT * FROM WORKS ORDER BY EMPNUM, PNUM
[32m- 0365[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF UNION SELECT EMPNAME FROM STAFF UNION ALL SELECT EMPNAME FROM STAFF
[32m- 0366[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF UNION ALL SELECT EMPNAME FROM STAFF UNION SELECT EMPNAME FROM STAFF
[32m- 0367[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ WHERE CITY IS NULL
[32m- 0368[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0369 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 44[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0370 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0371 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 44[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0372 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0373 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 45[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0374 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'CITY' 'FROM' in function specification; line 1 pos 46[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  WORKS WHERE EMPNUM = 'E9'
[32m- 0375[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  WORKS WHERE HOURS > 85
[32m- 0376[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE HOURS = 222
[32m- 0377[0m
Calcite parsing passed, start to transform. SELECT MIN(PNAME) FROM PROJ, WORKS, STAFF WHERE PROJ.PNUM = WORKS.PNUM AND WORKS.EMPNUM = STAFF.EMPNUM AND BUDGET - GRADE * HOURS * 100 IN (-4400, -1000, 4000)
[32m- 0378[0m
Calcite parsing passed, start to transform. SELECT CITY, COUNT(*) FROM PROJ GROUP BY CITY HAVING (MAX(BUDGET) - MIN(BUDGET)) / 2 IN (2, 20000, 10000) ORDER BY CITY DESC
[32m- 0379[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_OBSERV WHERE MAX_TEMP = 123.45 AND MAX_TEMP NOT BETWEEN 123.4516 AND 123.4518
[32m- 0380[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ WHERE 24 * 1000 BETWEEN BUDGET - 5000 AND 50000 / 1.7
[32m- 0381[0m
Calcite parsing passed, start to transform. SELECT PNAME FROM PROJ WHERE 'Tampa' NOT BETWEEN CITY AND 'Vienna' AND PNUM > 'P2'
[32m- 0382[0m
Calcite parsing passed, start to transform. SELECT CITY, COUNT(*) FROM PROJ GROUP BY CITY HAVING 50000 + 2 BETWEEN 33000 AND SUM(BUDGET) - 20
[32m- 0383[0m
NoViableAltException(231@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:3544)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6280)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0384 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'SELECT' 'SUM' '(' in function specification; line 1 pos 66[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.USIG
[32m- 0385[0m
Calcite parsing passed, start to transform. SELECT C1 FROM WHICH_SCHEMA1
[32m- 0386[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF1
[32m- 0387[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ1
[32m- 0388[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ1
[32m- 0389[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  WORKS1
[32m- 0390[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   WORKS1
[32m- 0391[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFFV1
[32m- 0392[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFFV2
[32m- 0393[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFFV2
[32m- 0394[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF
[32m- 0395[0m
Calcite parsing passed, start to transform. SELECT COUNT(*),SUM(COST) FROM STAFF_WORKS_DESIGN
[32m- 0396[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VTABLE
[32m- 0397[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.VTABLE
[32m- 0398[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.VTABLE
[32m- 0399[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0400[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0401[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S
[32m- 0402[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEMP_S
[32m- 0403[0m
Calcite parsing passed, start to transform. SELECT COUNT(DISTINCT EMPNUM) FROM TEMP_S
[32m- 0404[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFFV2_VIEW
[32m- 0405[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, GRADE FROM STAFFV2_VIEW WHERE EMPNUM = 'E3'
[32m- 0406[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFFV2_VIEW
[32m- 0407[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, HOURS FROM DOMAIN_VIEW WHERE PNUM = 'P3'
[32m- 0408[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM DOMAIN_VIEW
[32m- 0409[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM DOMAIN_VIEW
[32m- 0410[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS
[32m- 0411[0m
Calcite parsing passed, start to transform. SELECT X.CITY, X.MAX_C, Y.MAX_C, (X.MAX_C + Y.MAX_C) / 2 FROM CELSIUS_OBSERV X, CELSIUS_OBSERV Y WHERE X.YEAR_OBSERV = 1984 AND Y.YEAR_OBSERV = 1985 AND X.CITY = Y.CITY ORDER BY 4 DESC
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'c3' given input columns CITY, MAX_C, MAX_C, _c3;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'c3' given input columns CITY, MAX_C, MAX_C, _c3;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:122)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)[0m
[31m  	at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:122)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['c3 DESC], true[0m
[31m   'Project [unresolvedalias('X.CITY),unresolvedalias('X.MAX_C),unresolvedalias('Y.MAX_C),unresolvedalias((('X.MAX_C + 'Y.MAX_C) / 2))][0m
[31m    'Filter ((('X.YEAR_OBSERV = 1984) && ('Y.YEAR_OBSERV = 1985)) && ('X.CITY = 'Y.CITY))[0m
[31m     'Join Inner, None[0m
[31m      'UnresolvedRelation [CELSIUS_OBSERV], Some(X)[0m
[31m      'UnresolvedRelation [CELSIUS_OBSERV], Some(Y)[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  CITY: string, MAX_C: double, MAX_C: double, _c3: double[0m
[31m  'Sort ['c3 DESC], true[0m
[31m   Project [CITY#55810,MAX_C#55813,MAX_C#55817,((MAX_C#55813 + MAX_C#55817) / cast(2 as double)) AS _c3#55818][0m
[31m    Filter (((YEAR_OBSERV#55811 = cast(1984 as double)) && (YEAR_OBSERV#55815 = cast(1985 as double))) && (CITY#55810 = CITY#55814))[0m
[31m     Join Inner, None[0m
[31m      MetastoreRelation HU, celsius_observ, Some(X)[0m
[31m      MetastoreRelation HU, celsius_observ, Some(Y)[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'c3' given input columns CITY, MAX_C, MAX_C, _c3;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'c3' given input columns CITY, MAX_C, MAX_C, _c3;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'c3' given input columns CITY, MAX_C, MAX_C, _c3;[0m
[31m  == HIVE - 3 row(s) ==[0m
[31m  Sun City	43.33333333333333333333333333333333333333	40.55555555555555555555555555555555555556	41.94444444444444444444444444444444444445[0m
[31m  Abeland	38.33333333333333333333333333333333333333	36.66666666666666666666666666666666666667	37.5[0m
[31m  Iceburg	7.22222222222222222222222222222222222222	8.33333333333333333333333333333333333333	7.77777777777777777777777777777777777778 (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT CITY, YEAR_OBSERV, MIN_C, MAX_C FROM CELSIUS_OBSERV WHERE YEAR_OBSERV = 1984 AND MIN_C > 5
[32m- 0413[0m
Calcite parsing passed, start to transform. SELECT CITY, HIGH, LOW FROM MULTI_YEAR_OBSERV ORDER BY CITY ASC
[32m- 0414[0m
Calcite parsing passed, start to transform. SELECT HIGH, YEAR_OBSERV, LOW FROM EXTREME_TEMPS ORDER BY YEAR_OBSERV DESC
[32m- 0415[0m
Calcite parsing passed, start to transform. SELECT EMP1, EMP_AVG, EMP_MAX FROM SET_TEST ORDER BY EMP1
[32m- 0416[0m
Calcite parsing passed, start to transform. SELECT EMP1, HOURS, HOURS_2 FROM DUP_COL WHERE EMP1 = 'E3'
[32m- 0417[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CPBASE, CPREF
[32m- 0418[0m
Calcite parsing passed, start to transform. SELECT KC, JUNK2 FROM CPBASE, CPREF ORDER BY JUNK2, KC
[32m- 0419[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG
[32m- 0420[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG
[32m- 0421[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG WHERE PRODUCT_ID = 0
[32m- 0422[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG
[32m- 0423[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG WHERE PRODUCT_ID = 0
[32m- 0424[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG
[32m- 0425[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG WHERE PRODUCT_ID = 0
[32m- 0426[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG
[32m- 0427[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG WHERE PRODUCT_ID = 0
[32m- 0428[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG
[32m- 0429[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG
[32m- 0430[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM RET_CATALOG
[32m- 0431[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.WORKS
[32m- 0432[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.WORKS WHERE PNUM = (SELECT PNUM FROM HU.WORKS WHERE HOURS = 80)
calcite cannot do SELECT COUNT(*) FROM HU.WORKS WHERE PNUM = (SELECT PNUM FROM HU.WORKS WHERE HOURS = 80)
[31m- 0433 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT GRADE FROM HU.STAFF WHERE EMPNUM = 'xx'
[32m- 0434[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE CITY LIKE '%XX%X_%' ESCAPE 'X'
[32m- 0435[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE CITY LIKE '%XX_%' ESCAPE 'X'
[32m- 0436[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CPBASE WHERE JUNK1 LIKE 'P%X%%' ESCAPE 'X'
[32m- 0437[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF
[32m- 0438[0m
Calcite parsing passed, start to transform. SELECT AVG(SMALLTEST) FROM HU.HH
[32m- 0439[0m
Calcite parsing passed, start to transform. SELECT AVG(GRADE) FROM HU.STAFF WHERE CITY = 'Vienna'
[32m- 0440[0m
Calcite parsing passed, start to transform. SELECT SUM(DISTINCT GRADE) FROM HU.STAFF
[32m- 0441[0m
Calcite parsing passed, start to transform. SELECT CITY, COUNT(DISTINCT GRADE) FROM HU.STAFF GROUP BY CITY ORDER BY CITY DESC
[32m- 0442[0m
Calcite parsing passed, start to transform. SELECT COL2 FROM HU.UPUNIQ WHERE NUMKEY = 1
[32m- 0443[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < (SELECT MAX(HOURS) FROM HU.WORKS) OR    GRADE > (SELECT MAX(NUMKEY) FROM HU.UPUNIQ) OR    GRADE + 100 > (SELECT MIN(HOURS) FROM HU.WORKS)
calcite cannot do SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < (SELECT MAX(HOURS) FROM HU.WORKS) OR    GRADE > (SELECT MAX(NUMKEY) FROM HU.UPUNIQ) OR    GRADE + 100 > (SELECT MIN(HOURS) FROM HU.WORKS)
[31m- 0444 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COL2 FROM HU.UPUNIQ WHERE NUMKEY = 1
[32m- 0445[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < (SELECT MAX(HOURS) FROM HU.WORKS) OR    GRADE > (SELECT MAX(NUMKEY) FROM HU.UPUNIQ) OR    GRADE + 100 > (SELECT MIN(HOURS) FROM HU.WORKS)
calcite cannot do SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < (SELECT MAX(HOURS) FROM HU.WORKS) OR    GRADE > (SELECT MAX(NUMKEY) FROM HU.UPUNIQ) OR    GRADE + 100 > (SELECT MIN(HOURS) FROM HU.WORKS)
[31m- 0446 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT "A < a".CITY FROM HU.STAFF "A < a" WHERE EMPNUM = 'E5'
[32m- 0449[0m
Calcite parsing passed, start to transform. SELECT "0".CITY FROM HU.STAFF "0" WHERE EMPNUM = 'E5'
[32m- 0450[0m
Calcite parsing passed, start to transform. SELECT "%".CITY FROM HU.STAFF  "%" WHERE EMPNUM = 'E5'
[32m- 0451[0m
Calcite parsing passed, start to transform. SELECT "&".CITY FROM HU.STAFF  "&" WHERE EMPNUM = 'E5'
[32m- 0452[0m
Calcite parsing passed, start to transform. SELECT "'".CITY FROM HU.STAFF  "'" WHERE EMPNUM = 'E5'
[32m- 0453[0m
Calcite parsing passed, start to transform. SELECT "(".CITY FROM HU.STAFF  "(" WHERE EMPNUM = 'E5'
[32m- 0454[0m
Calcite parsing passed, start to transform. SELECT ")".CITY FROM HU.STAFF  ")" WHERE EMPNUM = 'E5'
[32m- 0455[0m
Calcite parsing passed, start to transform. SELECT "*".CITY FROM HU.STAFF  "*" WHERE EMPNUM = 'E5'
[32m- 0456[0m
Calcite parsing passed, start to transform. SELECT "+".CITY FROM HU.STAFF  "+" WHERE EMPNUM = 'E5'
[32m- 0457[0m
Calcite parsing passed, start to transform. SELECT ",".CITY FROM HU.STAFF  "," WHERE EMPNUM = 'E5'
[32m- 0458[0m
Calcite parsing passed, start to transform. SELECT "-".CITY FROM HU.STAFF  "-" WHERE EMPNUM = 'E5'
[32m- 0459[0m
Calcite parsing passed, start to transform. SELECT ".".CITY FROM HU.STAFF  "." WHERE EMPNUM = 'E5'
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve '..CITY' given input columns empnum, empname, grade, city;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve '..CITY' given input columns empnum, empname, grade, city;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:122)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:122)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias('..CITY)][0m
[31m   'Filter ('EMPNUM = E5)[0m
[31m    'UnresolvedRelation [HU,STAFF], Some(.)[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias('..CITY)[0m
[31m  'Project [unresolvedalias('..CITY)][0m
[31m   Filter (EMPNUM#62907 = E5)[0m
[31m    MetastoreRelation hu, staff, Some(.)[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve '..CITY' given input columns empnum, empname, grade, city;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve '..CITY' given input columns empnum, empname, grade, city;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve '..CITY' given input columns empnum, empname, grade, city;[0m
[31m  == HIVE - 0 row(s) == (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT "/".CITY FROM HU.STAFF  "/" WHERE EMPNUM = 'E5'
[32m- 0461[0m
Calcite parsing passed, start to transform. SELECT ":".CITY FROM HU.STAFF  ":" WHERE EMPNUM = 'E5'
[32m- 0462[0m
Calcite parsing passed, start to transform. SELECT "<".CITY FROM HU.STAFF  "<" WHERE EMPNUM = 'E5'
[32m- 0463[0m
Calcite parsing passed, start to transform. SELECT "=".CITY FROM HU.STAFF  "=" WHERE EMPNUM = 'E5'
[32m- 0464[0m
Calcite parsing passed, start to transform. SELECT ">".CITY FROM HU.STAFF  ">" WHERE EMPNUM = 'E5'
[32m- 0465[0m
Calcite parsing passed, start to transform. SELECT "_".CITY FROM HU.STAFF  "_" WHERE EMPNUM = 'E5'
[32m- 0466[0m
Calcite parsing passed, start to transform. SELECT "|".CITY FROM HU.STAFF  "|" WHERE EMPNUM = 'E5'
[32m- 0467[0m
Calcite parsing passed, start to transform. SELECT GRADE AS PROVOLONE, EMPNAME AS EDAM FROM HU.STAFF ORDER BY PROVOLONE, EDAM DESC
[32m- 0468[0m
Calcite parsing passed, start to transform. SELECT HU.PROJ.CITY AS PCITY, HU.STAFF.CITY SCITY, BUDGET + GRADE * HOURS * 100  REAL_BUDGET FROM HU.STAFF, HU.PROJ, HU.WORKS WHERE HU.WORKS.EMPNUM = HU.STAFF.EMPNUM AND HU.WORKS.PNUM = HU.PROJ.PNUM AND EMPNAME = 'Alice' AND HU.PROJ.PNUM = 'P3'
[31m- 0469 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'HU.WORKS.EMPNUM' given input columns grade, empnum, empnum, pnum, hours, ptype, pname, budget, city, city, empname, pnum;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.WORKS.EMPNUM' given input columns grade, empnum, empnum, pnum, hours, ptype, pname, budget, city, city, empname, pnum;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias('HU.PROJ.CITY AS PCITY#65131),unresolvedalias('HU.STAFF.CITY AS SCITY#65132),unresolvedalias(('BUDGET + (('GRADE * 'HOURS) * 100)) AS REAL_BUDGET#65133)][0m
[31m   'Filter (((('HU.WORKS.EMPNUM = 'HU.STAFF.EMPNUM) && ('HU.WORKS.PNUM = 'HU.PROJ.PNUM)) && ('EMPNAME = Alice)) && ('HU.PROJ.PNUM = P3))[0m
[31m    'Join Inner, None[0m
[31m     'Join Inner, None[0m
[31m      'UnresolvedRelation [HU,STAFF], None[0m
[31m      'UnresolvedRelation [HU,PROJ], None[0m
[31m     'UnresolvedRelation [HU,WORKS], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias('HU.PROJ.CITY)[0m
[31m  'Project [unresolvedalias('HU.PROJ.CITY),unresolvedalias('HU.STAFF.CITY),unresolvedalias(('BUDGET + (('GRADE * 'HOURS) * 100)))][0m
[31m   'Filter (((('HU.WORKS.EMPNUM = 'HU.STAFF.EMPNUM) && ('HU.WORKS.PNUM = 'HU.PROJ.PNUM)) && (EMPNAME#65135 = Alice)) && ('HU.PROJ.PNUM = P3))[0m
[31m    Join Inner, None[0m
[31m     Join Inner, None[0m
[31m      MetastoreRelation hu, staff, None[0m
[31m      MetastoreRelation hu, proj, None[0m
[31m     MetastoreRelation hu, works, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.WORKS.EMPNUM' given input columns grade, empnum, empnum, pnum, hours, ptype, pname, budget, city, city, empname, pnum;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.WORKS.EMPNUM' given input columns grade, empnum, empnum, pnum, hours, ptype, pname, budget, city, city, empname, pnum;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'HU.WORKS.EMPNUM' given input columns grade, empnum, empnum, pnum, hours, ptype, pname, budget, city, city, empname, pnum;[0m
[31m  == HIVE - 0 row(s) == (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT T_DECIMAL / .000000001 FROM FOUR_TYPES WHERE T_CHAR = 'X'
[32m- 0470[0m
Calcite parsing passed, start to transform. SELECT SUM(T_REAL) FROM FOUR_TYPES
[32m- 0471[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,EMPNAME FROM HU.STAFF WHERE EMPNUM='E8'
[32m- 0473[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF4
[32m- 0475[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CUGINI.AA
[32m- 0476[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.AA
[32m- 0477[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.AA WHERE CHARTEST <> 'Twenty Characters...'
[32m- 0478[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.AA
[32m- 0479[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CUGINI.BB
[32m- 0480[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CUGINI.CC
[32m- 0481[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CUGINI.DD
[32m- 0482[0m
Calcite parsing passed, start to transform. SELECT INTTEST FROM CUGINI.EE
[32m- 0483[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.EE
[32m- 0484[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.EE WHERE INTTEST = 1
[32m- 0485[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.EE
[32m- 0486[0m
Calcite parsing passed, start to transform. SELECT INTTEST FROM CUGINI.FF
[32m- 0487[0m
Calcite parsing passed, start to transform. SELECT C1 FROM CUGINI.GG
[32m- 0488[0m
Calcite parsing passed, start to transform. SELECT SMALLTEST FROM CUGINI.HH
[32m- 0489[0m
Calcite parsing passed, start to transform. SELECT COL2 FROM CUGINI.VTABLE WHERE COL1 = 0
[32m- 0490[0m
Calcite parsing passed, start to transform. SELECT C1 FROM CUGINI.II
[32m- 0491[0m
Calcite parsing passed, start to transform. SELECT C1 FROM CUGINI.JJ
[32m- 0492[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.VS1
[32m- 0493[0m
Calcite parsing passed, start to transform. SELECT C1, C2 FROM FLATER.VS1 WHERE C2 = 0
[32m- 0494[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.VS1
[32m- 0495[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.VS1
[32m- 0496[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.VS1 WHERE C2 = 1
[32m- 0497[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.BASE_VS1
[32m- 0498[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.BADG1
[32m- 0499[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.BADG2
[32m- 0500[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CUGINI.VAA
[32m- 0501[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.VAA
[32m- 0502[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.VAA WHERE CHARTEST <> 'Twenty Characters...'
[32m- 0503[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CUGINI.VAA
[32m- 0504[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CUGINI.VBB
[32m- 0505[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CUGINI.VCC
[32m- 0506[0m
Calcite parsing passed, start to transform. SELECT CHARTEST FROM CUGINI.VDD
[32m- 0507[0m
Calcite parsing passed, start to transform. SELECT COL2 FROM CUGINI.VVTABLE WHERE COL1 = 0
[32m- 0508[0m
Calcite parsing passed, start to transform. SELECT C1 FROM CUGINI.VII
[32m- 0509[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS_P WHERE EMPNUM = 'E9'
[32m- 0510[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS_P WHERE EMPNUM = 'E2'
[32m- 0511[0m
Calcite parsing passed, start to transform. SELECT 'USER',PNAME FROM HU.PROJ
[32m- 0512[0m
Calcite parsing passed, start to transform. SELECT PNUM,'BUDGET IN GRAMS IS ',BUDGET * 5 FROM HU.PROJ WHERE PNUM = 'P1'
[32m- 0513[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,10 FROM HU.STAFF WHERE GRADE = 10
[32m- 0514[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, 10 FROM HU.STAFF
[32m- 0515[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,EMPNAME FROM HU.STAFF WHERE EMPNUM='E7'
[32m- 0517[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.PROJ
[32m- 0519[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,PNUM FROM HU.WORKS WHERE EMPNUM='E8'
[32m- 0521[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,EMPNAME FROM HU.STAFF WHERE EMPNUM='E8'
[32m- 0523[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,EMPNAME,GRADE FROM HU.STAFF3 WHERE EMPNUM = 'E3'
[32m- 0524[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,EMPNAME,GRADE FROM HU.STAFF3 WHERE EMPNUM = 'E8'
[32m- 0525[0m
Calcite parsing passed, start to transform. SELECT EMPNUM,EMPNAME FROM HU.STAFF4 WHERE EMPNUM = 'E6'
[32m- 0528[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF4
[32m- 0529[0m
Calcite parsing passed, start to transform. SELECT PNUM,PNAME,BUDGET FROM HU.PROJ WHERE PNUM = 'P3'
[32m- 0530[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM HU.PROJ
[32m- 0531[0m
Calcite parsing passed, start to transform. SELECT * FROM HU.UPUNIQ WHERE NUMKEY = 3
[32m- 0532[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF4 WHERE GRADE=0
[32m- 0534[0m
Calcite parsing passed, start to transform. SELECT GRADE FROM STAFF4 WHERE (EMPNAME IS NULL) AND CITY = '               '
[32m- 0535[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF14 WHERE EMPNAME = 'SUN'
[32m- 0536[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF16 WHERE EMPNAME IS NULL
[32m- 0537[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF16 WHERE EMPNUM = 'E2'
[32m- 0538[0m
Calcite parsing passed, start to transform. SELECT GRADE  FROM STAFF16 WHERE EMPNUM = 'E3'
[32m- 0539[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF5
[32m- 0540[0m
Calcite parsing passed, start to transform. SELECT GRADE FROM STAFF6 WHERE GRADE > 10
[32m- 0541[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF7
[32m- 0542[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF8
[32m- 0543[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF8
[32m- 0544[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF13
[32m- 0545[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF13
[32m- 0546[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF9
[32m- 0547[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF10
[32m- 0548[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF5
[32m- 0549[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF5 WHERE GRADE = 15
[32m- 0550[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF11
[32m- 0551[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF12
[32m- 0552[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF15
[32m- 0553[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF15
[32m- 0554[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF15
[32m- 0555[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF15
[32m- 0556[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF5 WHERE GRADE = 14
[32m- 0557[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF6 WHERE GRADE = 14
[32m- 0558[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF7 WHERE GRADE = 14
[32m- 0559[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF8 WHERE EMPNAME = 'Alice'
[32m- 0560[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF13 WHERE EMPNAME = 'Alice'
[32m- 0561[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF9 WHERE EMPNAME = 'Susan'
[32m- 0562[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF10 WHERE GRADE = 11
[32m- 0563[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF11 WHERE EMPNAME = 'Susan' AND GRADE = 11
[32m- 0564[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF12 WHERE GRADE = 11
[32m- 0565[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF15 WHERE EMPNAME = 'Alice'
[32m- 0566[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS3
[32m- 0567[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS3
[32m- 0568[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS3 WHERE PNUM = 'P2'
[32m- 0569[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF3 WHERE EMPNUM = 'E1'
[32m- 0570[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF3 WHERE EMPNUM = 'E1'
[32m- 0571[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF3 WHERE EMPNUM = 'E1'
[32m- 0572[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF3
[32m- 0573[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF3
[32m- 0574[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF3 WHERE EMPNUM = 'E2'
[32m- 0575[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF3
[32m- 0576[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF3 WHERE EMPNUM = 'E2'
[32m- 0577[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS3 WHERE EMPNUM = 'E2'
[32m- 0578[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS WHERE EMPNUM = 'E1'
[32m- 0579[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKS3 WHERE EMPNUM = 'E1'
[32m- 0580[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_C WHERE MGR = 'E1'
[32m- 0581[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF_C WHERE EMPNUM = 'E1'
[32m- 0582[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF_C WHERE EMPNUM = 'E9'
[32m- 0583[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_C WHERE MGR = 'E9'
[32m- 0584[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF_C WHERE EMPNUM = 'E1'
[32m- 0585[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_C WHERE EMPNUM ='E6'
[32m- 0586[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_C WHERE MGR = 'E1'
[32m- 0587[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_C WHERE MGR = 'E5'
[32m- 0588[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ_M WHERE MGR = 'E5'
[32m- 0589[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF_M WHERE EMPNUM = 'E2'
[32m- 0590[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF_M WHERE EMPNUM = 'E9'
[32m- 0591[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_M WHERE EMPNUM = 'E2'
[32m- 0592[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ_M WHERE MGR = 'E3'
[32m- 0593[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PROJ_M WHERE MGR = 'E5'
[32m- 0594[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_M WHERE PRI_WK = 'P9'
[32m- 0595[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F10
[32m- 0596[0m
Calcite parsing passed, start to transform. SELECT P1 FROM SIZ2_P WHERE P1 = '  A'
[32m- 0597[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_P WHERE P1 = '  A'
[32m- 0598[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_P WHERE P1 = '  A'
[32m- 0599[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_P WHERE P1 = '  A'
[32m- 0600[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F1 WHERE F1 = '  A'
[32m- 0601[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P5 WHERE F1 = 11
[32m- 0602[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P5 WHERE F1 = 11
[32m- 0603[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_F,SIZ3_P1,SIZ3_P2,SIZ3_P3,SIZ3_P4, SIZ3_P5,SIZ3_P6 WHERE P1 = SIZ3_P1.F1 AND P2 = SIZ3_P2.F1 AND P3 = SIZ3_P3.F1 AND P4 = SIZ3_P4.F1 AND P5 = SIZ3_P5.F1 AND P6 = SIZ3_P6.F1 AND SIZ3_P3.F1 BETWEEN 1 AND 2
[32m- 0604[0m
Calcite parsing passed, start to transform. SELECT COUNT(*)  FROM EMP WHERE ENO = 41
[32m- 0605[0m
Calcite parsing passed, start to transform. SELECT COUNT(*)  FROM EMP WHERE ENO = 21
[32m- 0606[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM EMP WHERE DNO = 12 AND DNAME = 'Education' AND ENO = 21 AND ENAME = 'Tom'
[32m- 0607[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM EMP WHERE DNO = 12 AND ENO = 21 AND ENAME = 'Tom'
[32m- 0608[0m
Calcite parsing passed, start to transform. SELECT MAX(F1),MIN(F1) FROM SIZ3_P3
[32m- 0609[0m
Calcite parsing passed, start to transform. SELECT MAX(F1),MIN(F1) FROM SIZ2_F3
[32m- 0610[0m
Calcite parsing passed, start to transform. SELECT MAX(F_KEY),MIN(F_KEY),MAX(P_KEY),MIN(P_KEY) FROM MID1
[32m- 0611[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_C
[32m- 0612[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM STAFF_P WHERE EMPNUM = 'E1'
[32m- 0613[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_P WHERE EMPNUM = 'E2'
[32m- 0614[0m
Calcite parsing passed, start to transform. SELECT NICKNAME, INSURANCE1 FROM CHAR_DEFAULT WHERE SEX_CODE = 'M'
[32m- 0615[0m
Calcite parsing passed, start to transform. SELECT SEX_CODE FROM CHAR_DEFAULT WHERE INSURANCE1 = 'Kaise'
[32m- 0616[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM EXACT_DEF WHERE BODY_TEMP = 99.0 AND MAX_NUM = -55555 AND MIN_NUM = .000001 OR BODY_TEMP = 98.6 AND MAX_NUM = 100 AND MIN_NUM = .2
[32m- 0617[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM APPROX_DEF WHERE (Y_COUNT BETWEEN -9.991E10 AND -9.989E10) AND (Z_COUNT BETWEEN 3.44E-11 AND 3.46E-11) AND (ZZ_COUNT BETWEEN -7.6778E-7 AND -7.6776E-7) OR (X_COUNT BETWEEN 1.77E12 AND 1.79E12)
[32m- 0618[0m
Calcite parsing passed, start to transform. SELECT COUNT(*)  FROM SIZE_TAB WHERE COL4 BETWEEN -1.46E22 AND -1.048575E22 GROUP BY COL1, COL2, COL3
[32m- 0619[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM C_TRANSACTION WHERE COMMOD_NO = 17
[32m- 0620[0m
Calcite parsing passed, start to transform. SELECT UNIT_PRICE, FROM_DATE, TO_DATE, COMMODITY FROM DOLLARS_PER_POUND ORDER BY COMMODITY DESC
[32m- 0621[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM COST_PER_UNIT
[32m- 0622[0m
Calcite parsing passed, start to transform. SELECT CURRENCY, MEASURE, UNIT_PRICE, COMMODITY FROM COST_PER_UNIT
[32m- 0623[0m
Calcite parsing passed, start to transform. SELECT (100 + 7) * UNIT_PRICE * 700 / 100, COMMODITY FROM DOLLARS_PER_POUND ORDER BY COMMODITY
[32m- 0624[0m
Calcite parsing passed, start to transform. SELECT EXP_NAME, BTH_DATE FROM EXPERIENCE WHERE EXP_NAME IS NOT NULL AND BTH_DATE IS NOT NULL AND DESCR = 'Car Mechanic'
[32m- 0625[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM EXPERIENCE WHERE DESCR = 'Car Mechanic'
[32m- 0626[0m
Calcite parsing passed, start to transform. SELECT EXP_NAME, DESCR, BTH_DATE FROM EXPERIENCE ORDER BY EXP_NAME, BTH_DATE
[32m- 0627[0m
Calcite parsing passed, start to transform. SELECT ENAME FROM EMP WHERE DNAME = 'Education'
[32m- 0628[0m
Calcite parsing passed, start to transform. SELECT DNAME FROM EMP WHERE ENAME = 'Joan'
[32m- 0629[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF5
[32m- 0630[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF5 WHERE GRADE IS NULL
[32m- 0631[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF6_WITH_GRADES
[32m- 0632[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF6
[32m- 0633[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF6 WHERE GRADE IS NULL
[32m- 0634[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF9
[32m- 0635[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF9
[32m- 0636[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM DEPT WHERE DNO = 10
[32m- 0637[0m
Calcite parsing passed, start to transform. SELECT * FROM DEPT ORDER BY DNO
[32m- 0638[0m
Calcite parsing passed, start to transform. SELECT ENO, ENAME, DNO, DNAME FROM EMP ORDER BY ENO
[32m- 0639[0m
Calcite parsing passed, start to transform. SELECT C1, C2, C3 FROM ICAST2
[32m- 0640[0m
Calcite parsing passed, start to transform. SELECT C1, C2, C3 FROM ICAST2
[32m- 0641[0m
Calcite parsing passed, start to transform. SELECT C1, C2, C3 FROM ICAST2
[32m- 0642[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.ECCO
[32m- 0643[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM NAMGRP1 WHERE NAME = 'KERI' AND GRP = 10
[32m- 0644[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM NAMGRP1 WHERE NAME = 'MARY' AND GRP = 20
[32m- 0645[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP1 WHERE (NAME <> 'MARY' AND NAME <> 'KERI') OR GRP <> 20 AND GRP <> 10 OR EMPNUM <> 0 AND EMPNUM <> 1 OR NAME IS NULL OR GRP IS NULL OR EMPNUM IS NULL
[32m- 0646[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM NAMGRP2 WHERE NAME = 'KERI' AND GRP = 10
[32m- 0647[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM NAMGRP2 WHERE NAME = 'MARY' AND GRP = 20
[32m- 0648[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP2 WHERE NAME <> 'MARY' AND NAME <> 'KERI' OR GRP <> 20 AND GRP <> 10 OR EMPNUM <> 0 AND EMPNUM <> 1 OR NAME IS NULL OR GRP IS NULL OR EMPNUM IS NULL
[32m- 0649[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM NMGRP2 WHERE NAME = 'MARY' AND GRP = 20
[32m- 0650[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NMGRP2 WHERE NAME <> 'MARY' OR GRP <> 20 OR EMPNUM <> 1 OR NAME IS NULL OR GRP IS NULL OR EMPNUM IS NULL
[32m- 0651[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP3 WHERE EMPNUM = 0 AND NAME = 'KERI' AND GRP = 10
[32m- 0652[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP3 WHERE EMPNUM = 1 AND NAME = 'MARY' AND GRP = 20
[32m- 0653[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP3 WHERE EMPNUM = 5 AND NAME = 'HARRY' AND GRP IS NULL
[32m- 0654[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP3 WHERE EMPNUM = 7 AND NAME = 'LARRY' AND GRP IS NULL
[32m- 0655[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP3 WHERE EMPNUM = 9 AND NAME = 'BARRY' AND GRP IS NULL
[32m- 0656[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP3
[32m- 0657[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NMGRP3 WHERE NAME = 'HARRY' AND GRP IS NULL
[32m- 0658[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NMGRP3 WHERE NAME = 'MARY' AND GRP = 30
[32m- 0659[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NMGRP3 WHERE NAME = 'MARY' AND GRP = 40
[32m- 0660[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NMGRP3 WHERE NAME = 'BARRY' AND GRP IS NULL
[32m- 0661[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NMGRP3 WHERE NAME = 'LARRY' AND GRP IS NULL
[32m- 0662[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NMGRP3
[32m- 0663[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP4 WHERE EMPNUM = 0 AND NAME = 'KERI' AND GRP = 10
[32m- 0664[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP4 WHERE EMPNUM = 1 AND NAME = 'MARY' AND GRP = 20
[32m- 0665[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP4 WHERE EMPNUM = 2 AND NAME IS NULL AND GRP = 30
[32m- 0666[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP4 WHERE EMPNUM = 3 AND NAME IS NULL AND GRP = 40
[32m- 0667[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NAMGRP4
[32m- 0668[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM UUSIG
[32m- 0669[0m
Calcite parsing passed, start to transform. SELECT COUNT(DISTINCT U1) FROM UUSIG
[32m- 0670[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM UUSIG WHERE U1 < 0 OR U1 > 3 OR U1 IS NULL
[32m- 0671[0m
Calcite parsing passed, start to transform. SELECT * FROM ABOVE_AVERAGE ORDER BY COLUMN_1
[32m- 0672[0m
Calcite parsing passed, start to transform. SELECT * FROM STAFF_DUP ORDER BY CITY
[32m- 0673[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_DUP
[32m- 0674[0m
Calcite parsing passed, start to transform. SELECT C2, C1, C3 FROM FOUR_CITIES ORDER BY C3, C2
[32m- 0675[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM FOUR_CITIES
[32m- 0676[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FOUR_CITIES WHERE C3 > 0
[32m- 0677[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FOUR_CITIES WHERE C2 = 'Vienna'
[32m- 0678[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL3
[32m- 0679[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL3 WHERE D2 IS NOT NULL OR D1 IS NOT NULL
[32m- 0680[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL5
[32m- 0681[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL6 WHERE C2 IS NOT NULL
[32m- 0682[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL3
[32m- 0683[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL3 WHERE C1 IS NULL
[32m- 0684[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL3 WHERE D1 IS NULL
[32m- 0685[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL3 WHERE D2 IS NULL
[32m- 0686[0m
Calcite parsing passed, start to transform. SELECT AVG(D1) * 10 FROM JNULL3
[32m- 0687[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL6 WHERE C2 = 1
[32m- 0688[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL6 WHERE C2 IS NULL
[32m- 0689[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JNULL6 WHERE C2 = C1 AND D1 IS NULL
[32m- 0690[0m
Calcite parsing passed, start to transform. SELECT MAX(AGE) FROM CHANGGVIEW
[32m- 0691[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CHANGG
[32m- 0692[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CHANGG WHERE AGE > 30
[32m- 0693[0m
Calcite parsing passed, start to transform. SELECT NAAM FROM CHANGG WHERE NUMBRR LIKE '%000%'
[32m- 0694[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM OBITUARIES WHERE BORN <> DATE '1880-01-01' OR BORN IS NULL OR DIED <> TESTING1 OR DIED IS NULL OR ENTERED <> TESTING2 OR ENTERED IS NULL
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM OBITUARIES WHERE BORN <> DATE '1880-01-01' OR BORN IS NULL OR DIED <> TESTING1 OR DIED IS NULL OR ENTERED <> TESTING2 OR ENTERED IS NULL
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM OBITUARIES WHERE BORN <> DATE '1880-01-01' OR BORN IS NULL OR DIED <> TESTING1 OR DIED IS NULL OR ENTERED <> TESTING2 OR ENTERED IS NULL
[31m  Results do not match for 0695:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias(count(1))][0m
[31m   'Filter (((((NOT ('BORN = -32872) || isnull('BORN)) || NOT ('DIED = 'TESTING1)) || isnull('DIED)) || NOT ('ENTERED = 'TESTING2)) || isnull('ENTERED))[0m
[31m    'UnresolvedRelation [OBITUARIES], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  _c0: bigint[0m
[31m  Aggregate [count(1) AS _c0#88652L][0m
[31m   Filter (((((NOT (cast(BORN#88647 as string) = cast(-32872 as string)) || isnull(BORN#88647)) || NOT (DIED#88648 = TESTING1#88650)) || isnull(DIED#88648)) || NOT (ENTERED#88649 = TESTING2#88651)) || isnull(ENTERED#88649))[0m
[31m    MetastoreRelation FLATER, obituaries, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Aggregate [count(1) AS _c0#88652L][0m
[31m   Project[0m
[31m    Filter (((((NOT (cast(BORN#88647 as string) = 1880-01-01) || isnull(BORN#88647)) || NOT (DIED#88648 = TESTING1#88650)) || isnull(DIED#88648)) || NOT (ENTERED#88649 = TESTING2#88651)) || isnull(ENTERED#88649))[0m
[31m     MetastoreRelation FLATER, obituaries, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#88652L])[0m
[31m   TungstenExchange SinglePartition[0m
[31m    TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[currentCount#88655L])[0m
[31m     Project[0m
[31m      Filter (((((NOT (cast(BORN#88647 as string) = 1880-01-01) || isnull(BORN#88647)) || NOT (DIED#88648 = TESTING1#88650)) || isnull(DIED#88648)) || NOT (ENTERED#88649 = TESTING2#88651)) || isnull(ENTERED#88649))[0m
[31m       HiveTableScan [TESTING2#88651,BORN#88647,DIED#88648,TESTING1#88650,ENTERED#88649], (MetastoreRelation FLATER, obituaries, None)[0m
  
[31m  Code Generation: true[0m
[31m  _c0[0m
[31m  !== HIVE - 1 row(s) ==   == CATALYST - 1 row(s) ==[0m
[31m  !0                       1 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT TRIM ('X' FROM SPONSOR) FROM WEIRDPAD WHERE TRIM (NAAM) = 'KEITH'
calcite cannot do SELECT TRIM ('X' FROM SPONSOR) FROM WEIRDPAD WHERE TRIM (NAAM) = 'KEITH'
[31m- 0696 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT TRIM (LEADING 'X' FROM SPONSOR) FROM WEIRDPAD WHERE TRIM (TRAILING FROM NAAM) = '    KEITH'
calcite cannot do SELECT TRIM (LEADING 'X' FROM SPONSOR) FROM WEIRDPAD WHERE TRIM (TRAILING FROM NAAM) = '    KEITH'
[31m- 0697 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT TRIM (LEADING 'X' FROM SPONSOR) FROM WEIRDPAD WHERE TRIM (TRAILING 'X' FROM SPONSOR) = 'XXXXKATE'
calcite cannot do SELECT TRIM (LEADING 'X' FROM SPONSOR) FROM WEIRDPAD WHERE TRIM (TRAILING 'X' FROM SPONSOR) = 'XXXXKATE'
[31m- 0698 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT TRIM (LEADING FROM B.NAAM)  FROM WEIRDPAD A, WEIRDPAD B WHERE TRIM (BOTH 'B' FROM A.NAAM) = TRIM (BOTH 'X' FROM B.SPONSOR)
calcite cannot do SELECT TRIM (LEADING FROM B.NAAM)  FROM WEIRDPAD A, WEIRDPAD B WHERE TRIM (BOTH 'B' FROM A.NAAM) = TRIM (BOTH 'X' FROM B.SPONSOR)
[31m- 0699 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WEIRDPAD A, WEIRDPAD B WHERE TRIM (LEADING '0' FROM A.SPONSOR) = TRIM (' ' FROM B.NAAM)
calcite cannot do SELECT COUNT(*) FROM WEIRDPAD A, WEIRDPAD B WHERE TRIM (LEADING '0' FROM A.SPONSOR) = TRIM (' ' FROM B.NAAM)
[31m- 0700 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WEIRDPAD
[32m- 0701[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WEIRDPAD WHERE NAAM = 'KATE' OR SPONSOR = 'KATE'
[32m- 0702[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WEIRDPAD
[32m- 0703[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WEIRDPAD WHERE TRIM (PADCHAR FROM SPONSOR) IS NULL
calcite cannot do SELECT COUNT(*) FROM WEIRDPAD WHERE TRIM (PADCHAR FROM SPONSOR) IS NULL
[31m- 0704 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WEIRDPAD WHERE TRIM (PADCHAR FROM SPONSOR) = 'KEITH'
calcite cannot do SELECT COUNT(*) FROM WEIRDPAD WHERE TRIM (PADCHAR FROM SPONSOR) = 'KEITH'
[31m- 0705 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, SUM (HOURS) FROM WORKWEEK WHERE HOURS > 20 GROUP BY EMPNUM HAVING EMPNUM = 'E1'
[32m- 0706[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM WORKWEEK WHERE HOURS > 40
[32m- 0707[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM HU.STAFF, WORKWEEK WHERE HU.STAFF.EMPNUM = WORKWEEK.EMPNUM AND HOURS = 12
[31m- 0708 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'HU.STAFF.EMPNUM' given input columns empnum, grade, empname, hours, empnum, city;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.EMPNUM' given input columns empnum, grade, empname, hours, empnum, city;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias('EMPNAME)][0m
[31m   'Filter (('HU.STAFF.EMPNUM = 'WORKWEEK.EMPNUM) && ('HOURS = 12))[0m
[31m    'Join Inner, None[0m
[31m     'UnresolvedRelation [HU,STAFF], None[0m
[31m     'UnresolvedRelation [WORKWEEK], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias('EMPNAME)[0m
[31m  'Project [unresolvedalias('EMPNAME)][0m
[31m   'Filter (('HU.STAFF.EMPNUM = EMPNUM#89875) && (HOURS#89876 = cast(12 as double)))[0m
[31m    Join Inner, None[0m
[31m     MetastoreRelation hu, staff, None[0m
[31m     MetastoreRelation FLATER, workweek, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.EMPNUM' given input columns empnum, grade, empname, hours, empnum, city;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.EMPNUM' given input columns empnum, grade, empname, hours, empnum, city;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.EMPNUM' given input columns empnum, grade, empname, hours, empnum, city;[0m
[31m  == HIVE - 0 row(s) == (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT COUNT(*), MAX(EMPNUM), MIN(EMPNUM), AVG(HOURS) FROM WORKWEEK
[32m- 0709[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM HU.STAFF WHERE EMPNUM = (SELECT EMPNUM FROM WORKWEEK WHERE HOURS = 12)
calcite cannot do SELECT EMPNAME FROM HU.STAFF WHERE EMPNUM = (SELECT EMPNUM FROM WORKWEEK WHERE HOURS = 12)
[31m- 0710 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM HU.STAFF WHERE EMPNUM = (SELECT EMPNUM FROM HU.WORKS GROUP BY EMPNUM, HOURS HAVING HOURS = 12)
calcite cannot do SELECT EMPNAME FROM HU.STAFF WHERE EMPNUM = (SELECT EMPNUM FROM HU.WORKS GROUP BY EMPNUM, HOURS HAVING HOURS = 12)
[31m- 0711 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM QUALSTAR
[32m- 0712[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SUBQ2
[32m- 0713[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, GRADE, CITY, HOURS FROM QUALSTAR WHERE EMPNAME = 'Carmen'
[32m- 0714[0m
Calcite parsing passed, start to transform. SELECT HU.STAFF.*, HOURS FROM HU.STAFF, HU.WORKS WHERE HU.STAFF.EMPNUM = HU.WORKS.EMPNUM AND EMPNAME = 'Carmen'
calcite cannot do SELECT HU.STAFF.*, HOURS FROM HU.STAFF, HU.WORKS WHERE HU.STAFF.EMPNUM = HU.WORKS.EMPNUM AND EMPNAME = 'Carmen'
[31m- 0715 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CORRQUALSTAR
[32m- 0716[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CORRSUBQ2
[32m- 0717[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, GRADE, CITY, HOURS FROM CORRQUALSTAR WHERE EMPNAME = 'Carmen'
[32m- 0718[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM LUSER_DATA
[32m- 0719[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM LUSERS
[32m- 0720[0m
Calcite parsing passed, start to transform. SELECT CAST (AVG (CAST (USER_TYPED AS INT)) AS INT) FROM USER_INPUT
Calcite parsing passed, start to transform. SELECT CAST (AVG (CAST (USER_TYPED AS INT)) AS INT) FROM USER_INPUT
Calcite parsing passed, start to transform. SELECT CAST (AVG (CAST (USER_TYPED AS INT)) AS INT) FROM USER_INPUT
[31m- 0721 *** FAILED ***[0m
[31m  Results do not match for 0721:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias(cast('AVG(cast('USER_TYPED as int)) as int))][0m
[31m   'UnresolvedRelation [USER_INPUT], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  _c0: int[0m
[31m  Aggregate [cast(avg(cast(cast(cast(USER_TYPED#91019 as decimal(20,0)) as int) as bigint)) as int) AS _c0#91021][0m
[31m   MetastoreRelation FLATER, user_input, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Aggregate [cast(avg(cast(cast(cast(USER_TYPED#91019 as decimal(20,0)) as int) as bigint)) as int) AS _c0#91021][0m
[31m   Project [USER_TYPED#91019][0m
[31m    MetastoreRelation FLATER, user_input, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenAggregate(key=[], functions=[(average(cast(cast(cast(USER_TYPED#91019 as decimal(20,0)) as int) as bigint)),mode=Final,isDistinct=false)], output=[_c0#91021])[0m
[31m   TungstenExchange SinglePartition[0m
[31m    TungstenAggregate(key=[], functions=[(average(cast(cast(cast(USER_TYPED#91019 as decimal(20,0)) as int) as bigint)),mode=Partial,isDistinct=false)], output=[currentSum#91026,currentCount#91027L])[0m
[31m     HiveTableScan [USER_TYPED#91019], (MetastoreRelation FLATER, user_input, None)[0m
  
[31m  Code Generation: true[0m
[31m  _c0[0m
[31m  !== HIVE - 1 row(s) ==   == CATALYST - 1 row(s) ==[0m
[31m  !6                       5 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT AVG (USER_INPUT) FROM STANDARD_INPUT
[32m- 0722[0m
Calcite parsing passed, start to transform. SELECT SUM (USER_INPUT) * 100, SUM (RECEIVABLE) FROM STANDARD_INPUT
[32m- 0723[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SSSLOG WHERE ENTERED_BY = 'FLATER' AND SEVERITY = 1 AND PROBLEM IS NULL
[32m- 0724[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SSSLOG WHERE ENTERED_BY = 'FLATER' AND SEVERITY = 1 AND PROBLEM = 'Cross-linked inode'
[32m- 0725[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SSSLOG WHERE ENTERED_BY = 'system' AND SEVERITY = 1 AND PROBLEM = 'Freed a free frag'
[32m- 0726[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SSSLOG WHERE ENTERED_BY = 'nobody' AND SEVERITY = 6 AND PROBLEM IS NULL
[32m- 0727[0m
Calcite parsing passed, start to transform. SELECT CH1A, CH1B, CH1C FROM CH1
[32m- 0728[0m
Calcite parsing passed, start to transform. SELECT CH1A, CH1B, CH1C FROM CH1
[32m- 0729[0m
Calcite parsing passed, start to transform. SELECT NUM1C1 * 100, NUM1C2, NUM1C3 FROM NUM1
[32m- 0730[0m
Calcite parsing passed, start to transform. SELECT NUM1C1 * 100, NUM1C2, NUM1C3 FROM NUM1
[32m- 0731[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VERBOSE_INV
[32m- 0732[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM INCOMPLETES
[32m- 0733[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM COMPLETES
[32m- 0734[0m
NoViableAltException(-1@[323:1: atomExpression : ( ( KW_NULL )=> KW_NULL -> TOK_NULL | ( constant )=> constant | castExpression | caseExpression | whenExpression | ( functionName LPAREN )=> function | tableOrColumn | LPAREN ! expression RPAREN !);])
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser$DFA32.specialStateTransition(HiveParser_IdentifiersParser.java)
	at org.antlr.runtime.DFA.predict(DFA.java:80)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6383)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7195)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8290)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9327)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45846)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.searchCondition(HiveParser_FromClauseParser.java:6637)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.whereClause(HiveParser_FromClauseParser.java:6545)
	at org.apache.hadoop.hive.ql.parse.HiveParser.whereClause(HiveParser.java:45849)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41543)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in expression specification; line 1 pos 136[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VERBOSE_INV WHERE ITEMTEXT = 'Lousy excuse for a tape deck' AND CONDTEXT = 'Visibly damaged (no returns)' AND COSTTEXT = 'Expensive'
[32m- 0736[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VERBOSE_INV WHERE ITEMTEXT = 'Self-destruct VGA monitor w/ critical need detect' AND CONDTEXT = 'Slightly used' AND COSTTEXT = 'Robbery\; a complete and total rip-off'
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VERBOSE_INV WHERE ITEMTEXT = 'Self-destruct VGA monitor w/ critical need detect' AND CONDTEXT = 'Slightly used' AND COSTTEXT = 'Robbery\; a complete and total rip-off'
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VERBOSE_INV WHERE ITEMTEXT = 'Self-destruct VGA monitor w/ critical need detect' AND CONDTEXT = 'Slightly used' AND COSTTEXT = 'Robbery\; a complete and total rip-off'
[31m  Results do not match for 0737:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias(count(1))][0m
[31m   'Filter ((('ITEMTEXT = Self-destruct VGA monitor w/ critical need detect) && ('CONDTEXT = Slightly used)) && ('COSTTEXT = Robbery\; a complete and total rip-off))[0m
[31m    'UnresolvedRelation [VERBOSE_INV], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  _c0: bigint[0m
[31m  Aggregate [count(1) AS _c0#92593L][0m
[31m   Filter (((ITEMTEXT#92590 = Self-destruct VGA monitor w/ critical need detect) && (CONDTEXT#92591 = Slightly used)) && (COSTTEXT#92592 = Robbery\; a complete and total rip-off))[0m
[31m    MetastoreRelation FLATER, verbose_inv, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Aggregate [count(1) AS _c0#92593L][0m
[31m   Project[0m
[31m    Filter (((ITEMTEXT#92590 = Self-destruct VGA monitor w/ critical need detect) && (CONDTEXT#92591 = Slightly used)) && (COSTTEXT#92592 = Robbery\; a complete and total rip-off))[0m
[31m     MetastoreRelation FLATER, verbose_inv, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#92593L])[0m
[31m   TungstenExchange SinglePartition[0m
[31m    TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[currentCount#92596L])[0m
[31m     Project[0m
[31m      Filter (((ITEMTEXT#92590 = Self-destruct VGA monitor w/ critical need detect) && (CONDTEXT#92591 = Slightly used)) && (COSTTEXT#92592 = Robbery\; a complete and total rip-off))[0m
[31m       HiveTableScan [ITEMTEXT#92590,CONDTEXT#92591,COSTTEXT#92592], (MetastoreRelation FLATER, verbose_inv, None)[0m
  
[31m  Code Generation: true[0m
[31m  _c0[0m
[31m  !== HIVE - 1 row(s) ==   == CATALYST - 1 row(s) ==[0m
[31m  !1                       0 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VERBOSE_INV WHERE ITEMTEXT = 'Self-destruct VGA monitor w/ critical need detect' AND CONDTEXT = 'Visibly damaged (no returns)' AND COSTTEXT = 'Outrageously expensive'
[32m- 0738[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM VERBOSE_INV WHERE ITEMTEXT = 'World''s worst VCR' AND CONDTEXT = 'Returned as defective' AND COSTTEXT = 'Absurdly expensive'
[32m- 0739[0m
Calcite parsing passed, start to transform. SELECT * FROM PTYPES ORDER BY NUM
[32m- 0740[0m
NoViableAltException(313@[192:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4747)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:45918)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:5029)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2640)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1650)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0741 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near '"PStaff"' '(' 'PNUM' in table name; line 1 pos 13[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT * FROM PTYPES ORDER BY NUM
[32m- 0742[0m
NoViableAltException(313@[192:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4747)
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:45918)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:5029)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2640)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1650)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0743 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near '"PStaff"' '(' 'PNUM' in table name; line 1 pos 13[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EXTRACT (HOUR FROM HOURS), EXTRACT (MINUTE FROM HOURS) FROM PROJ_HOURS ORDER BY PNUM
[32m- 0744[0m
Calcite parsing passed, start to transform. SELECT EXTRACT (SECOND FROM RUN_SECONDS) FROM TYPE_TIMES ORDER BY JOB_TYPE
[32m- 0745[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM NOT_HERE
[32m- 0746[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM U_SIG
[32m- 0747[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM USIG
[32m- 0748[0m
Calcite parsing passed, start to transform. SELECT EMPNUM FROM HU.STAFF3 NATURAL LEFT JOIN HU.STAFF NATURAL INNER JOIN HU.STAFF4 ORDER BY EMPNUM DESC
[31m- 0749 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#93923, EMPNUM#93927, EMPNUM#93931.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#93923, EMPNUM#93927, EMPNUM#93931.;[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:278)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:162)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$19.apply(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$19.apply(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:383)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:122)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:122)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:383)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:277)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:277)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:276)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)[0m
[31m  	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)[0m
[31m  	at scala.collection.immutable.List.foldLeft(List.scala:84)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed$lzycompute(TestHive.scala:195)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed(TestHive.scala:180)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM DESC], true[0m
[31m   'Project [unresolvedalias('EMPNUM)][0m
[31m    'Join Inner, None[0m
[31m     'Join LeftOuter, None[0m
[31m      'UnresolvedRelation [HU,STAFF3], None[0m
[31m      'UnresolvedRelation [HU,STAFF], None[0m
[31m     'UnresolvedRelation [HU,STAFF4], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#93935, EMPNUM#93939, EMPNUM#93943.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#93947, EMPNUM#93951, EMPNUM#93955.;[0m
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#93959, EMPNUM#93963, EMPNUM#93967.;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#93971, EMPNUM#93975, EMPNUM#93979.;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#93983, EMPNUM#93987, EMPNUM#93991.;[0m
[31m  == HIVE - 0 row(s) == (HiveCompSuite.scala:3091)[0m
NoViableAltException(26@[150:5: ( ( Identifier LPAREN )=> partitionedTableFunction | tableSource | subQuerySource | virtualTableSource )])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:3711)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:1873)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1518)
	at org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:45861)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41516)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0750 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near '(' 'HU' '.' in from source; line 1 pos 20[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
NoViableAltException(26@[150:5: ( ( Identifier LPAREN )=> partitionedTableFunction | tableSource | subQuerySource | virtualTableSource )])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:3711)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:1910)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1518)
	at org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:45861)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41516)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0751 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near '(' 'HU' '.' in from source; line 1 pos 48[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, CITY, SALARY FROM HU.STAFF3 LEFT JOIN STAFF66 USING (EMPNUM) UNION SELECT EMPNUM, CITY, SALARY FROM HU.STAFF3 RIGHT JOIN STAFF66 USING (EMPNUM) ORDER BY EMPNUM
calcite cannot do SELECT EMPNUM, CITY, SALARY FROM HU.STAFF3 LEFT JOIN STAFF66 USING (EMPNUM) UNION SELECT EMPNUM, CITY, SALARY FROM HU.STAFF3 RIGHT JOIN STAFF66 USING (EMPNUM) ORDER BY EMPNUM
[31m- 0752 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT * FROM STAFF66 NATURAL INNER JOIN HU.STAFF3
[32m- 0753[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, EMPNAME, SALARY FROM HU.STAFF3 NATURAL LEFT OUTER JOIN STAFF66 WHERE EMPNUM > 'E1' ORDER BY EMPNUM ASC
[31m- 0754 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94421, EMPNUM#94428.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94421, EMPNUM#94428.;[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:278)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:162)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$19.apply(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$19.apply(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:383)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:383)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:277)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:277)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:276)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)[0m
[31m  	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)[0m
[31m  	at scala.collection.immutable.List.foldLeft(List.scala:84)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed$lzycompute(TestHive.scala:195)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed(TestHive.scala:180)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM ASC], true[0m
[31m   'Project [unresolvedalias('EMPNUM),unresolvedalias('EMPNAME),unresolvedalias('SALARY)][0m
[31m    'Filter ('EMPNUM > E1)[0m
[31m     'Join LeftOuter, None[0m
[31m      'UnresolvedRelation [HU,STAFF3], None[0m
[31m      'UnresolvedRelation [STAFF66], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94429, EMPNUM#94436.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94437, EMPNUM#94444.;[0m
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94445, EMPNUM#94452.;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94453, EMPNUM#94460.;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94461, EMPNUM#94468.;[0m
[31m  == HIVE - 0 row(s) == (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, EMPNAME, SALARY FROM STAFF66 NATURAL RIGHT OUTER JOIN HU.STAFF WHERE EMPNUM > 'E1' ORDER BY EMPNUM DESC
[31m- 0755 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94746, EMPNUM#94747.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94746, EMPNUM#94747.;[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:278)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:162)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$19.apply(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$19.apply(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:387)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:383)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:383)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:277)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:277)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:276)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)[0m
[31m  	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)[0m
[31m  	at scala.collection.immutable.List.foldLeft(List.scala:84)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed$lzycompute(TestHive.scala:195)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed(TestHive.scala:180)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM DESC], true[0m
[31m   'Project [unresolvedalias('EMPNUM),unresolvedalias('EMPNAME),unresolvedalias('SALARY)][0m
[31m    'Filter ('EMPNUM > E1)[0m
[31m     'Join RightOuter, None[0m
[31m      'UnresolvedRelation [STAFF66], None[0m
[31m      'UnresolvedRelation [HU,STAFF], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94754, EMPNUM#94755.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94762, EMPNUM#94763.;[0m
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94770, EMPNUM#94771.;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94778, EMPNUM#94779.;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#94786, EMPNUM#94787.;[0m
[31m  == HIVE - 3 row(s) ==[0m
[31m  E7	SULLIVAN	NULL[0m
[31m  E6	Fidel	NULL[0m
[31m  E13	ff	NULL (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT * FROM STAFF66 RIGHT JOIN HU.STAFF USING ( GRADE, EMPNUM, EMPNAME) WHERE EMPNUM > 'E1' ORDER BY EMPNUM
(List(STAFF66, GRADE),List(HU, STAFF, GRADE))
(List(STAFF66, EMPNUM),List(HU, STAFF, EMPNUM))
(List(STAFF66, EMPNAME),List(HU, STAFF, EMPNAME))
ListBuffer(('STAFF66.EMPNUM = 'HU.STAFF.EMPNUM), ('STAFF66.EMPNAME = 'HU.STAFF.EMPNAME))
[31m- 0756 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'HU.STAFF.GRADE' given input columns empname, empnum, empname, grade, city, grade, empnum, salary;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.GRADE' given input columns empname, empnum, empname, grade, city, grade, empnum, salary;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:119)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM ASC], true[0m
[31m   'Project [unresolvedalias(*)][0m
[31m    'Filter ('EMPNUM > E1)[0m
[31m     'Join RightOuter, Some((('STAFF66.GRADE = 'HU.STAFF.GRADE) && (('STAFF66.EMPNUM = 'HU.STAFF.EMPNUM) && ('STAFF66.EMPNAME = 'HU.STAFF.EMPNAME))))[0m
[31m      'UnresolvedRelation [STAFF66], None[0m
[31m      'UnresolvedRelation [HU,STAFF], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias(*)[0m
[31m  'Sort ['EMPNUM ASC], true[0m
[31m   'Project [unresolvedalias(*)][0m
[31m    'Filter ('EMPNUM > E1)[0m
[31m     'Join RightOuter, Some(((GRADE#95067 = 'HU.STAFF.GRADE) && ((EMPNUM#95068 = 'HU.STAFF.EMPNUM) && (EMPNAME#95066 = 'HU.STAFF.EMPNAME))))[0m
[31m      MetastoreRelation FLATER, staff66, None[0m
[31m      MetastoreRelation hu, staff, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.GRADE' given input columns empname, empnum, empname, grade, city, grade, empnum, salary;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.GRADE' given input columns empname, empnum, empname, grade, city, grade, empnum, salary;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.GRADE' given input columns empname, empnum, empname, grade, city, grade, empnum, salary;[0m
[31m  == HIVE - 3 row(s) ==[0m
[31m  82	E13	ff	NULL	gg[0m
[31m  0	E6	Fidel	NULL	Havana[0m
[31m  15	E7	SULLIVAN	NULL	Gaithersburg (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT * FROM HU.STAFF3 LEFT JOIN STAFF66 USING (GRADE, EMPNUM) WHERE EMPNUM > 'E1' ORDER BY EMPNUM ASC
(List(HU, STAFF3, GRADE),List(STAFF66, GRADE))
(List(HU, STAFF3, EMPNUM),List(STAFF66, EMPNUM))
[31m- 0757 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'HU.STAFF3.GRADE' given input columns city, grade, empname, empname, empnum, empnum, salary, grade;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.GRADE' given input columns city, grade, empname, empname, empnum, empnum, salary, grade;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:119)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM ASC], true[0m
[31m   'Project [unresolvedalias(*)][0m
[31m    'Filter ('EMPNUM > E1)[0m
[31m     'Join LeftOuter, Some((('HU.STAFF3.GRADE = 'STAFF66.GRADE) && ('HU.STAFF3.EMPNUM = 'STAFF66.EMPNUM)))[0m
[31m      'UnresolvedRelation [HU,STAFF3], None[0m
[31m      'UnresolvedRelation [STAFF66], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias(*)[0m
[31m  'Sort ['EMPNUM ASC], true[0m
[31m   'Project [unresolvedalias(*)][0m
[31m    'Filter ('EMPNUM > E1)[0m
[31m     'Join LeftOuter, Some((('HU.STAFF3.GRADE = GRADE#95213) && ('HU.STAFF3.EMPNUM = EMPNUM#95214)))[0m
[31m      MetastoreRelation hu, staff3, None[0m
[31m      MetastoreRelation FLATER, staff66, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.GRADE' given input columns city, grade, empname, empname, empnum, empnum, salary, grade;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.GRADE' given input columns city, grade, empname, empname, empnum, empnum, salary, grade;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.GRADE' given input columns city, grade, empname, empname, empnum, empnum, salary, grade;[0m
[31m  == HIVE - 0 row(s) == (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, GRADE, HU.STAFF3.EMPNAME, CITY, SALARY, STAFF66.EMPNAME FROM HU.STAFF3 LEFT JOIN STAFF66 USING (GRADE, EMPNUM) WHERE EMPNUM = 'E3'
(List(HU, STAFF3, GRADE),List(STAFF66, GRADE))
(List(HU, STAFF3, EMPNUM),List(STAFF66, EMPNUM))
[31m- 0758 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'HU.STAFF3.GRADE' given input columns empname, empname, city, salary, grade, empnum, grade, empnum;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.GRADE' given input columns empname, empname, city, salary, grade, empnum, grade, empnum;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:119)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias('EMPNUM),unresolvedalias('GRADE),unresolvedalias('HU.STAFF3.EMPNAME),unresolvedalias('CITY),unresolvedalias('SALARY),unresolvedalias('STAFF66.EMPNAME)][0m
[31m   'Filter ('EMPNUM = E3)[0m
[31m    'Join LeftOuter, Some((('HU.STAFF3.GRADE = 'STAFF66.GRADE) && ('HU.STAFF3.EMPNUM = 'STAFF66.EMPNUM)))[0m
[31m     'UnresolvedRelation [HU,STAFF3], None[0m
[31m     'UnresolvedRelation [STAFF66], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias('EMPNUM)[0m
[31m  'Project [unresolvedalias('EMPNUM),unresolvedalias('GRADE),unresolvedalias('HU.STAFF3.EMPNAME),unresolvedalias('CITY),unresolvedalias('SALARY),unresolvedalias('STAFF66.EMPNAME)][0m
[31m   'Filter ('EMPNUM = E3)[0m
[31m    'Join LeftOuter, Some((('HU.STAFF3.GRADE = GRADE#95355) && ('HU.STAFF3.EMPNUM = EMPNUM#95356)))[0m
[31m     MetastoreRelation hu, staff3, None[0m
[31m     MetastoreRelation FLATER, staff66, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.GRADE' given input columns empname, empname, city, salary, grade, empnum, grade, empnum;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.GRADE' given input columns empname, empname, city, salary, grade, empnum, grade, empnum;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.GRADE' given input columns empname, empname, city, salary, grade, empnum, grade, empnum;[0m
[31m  == HIVE - 0 row(s) == (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF66 NATURAL RIGHT JOIN HU.PROJ
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF66 NATURAL RIGHT JOIN HU.PROJ
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF66 NATURAL RIGHT JOIN HU.PROJ
[31m- 0759 *** FAILED ***[0m
[31m  Results do not match for 0759:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias(count(1))][0m
[31m   'Join RightOuter, None[0m
[31m    'UnresolvedRelation [STAFF66], None[0m
[31m    'UnresolvedRelation [HU,PROJ], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  _c0: bigint[0m
[31m  Aggregate [count(1) AS _c0#95521L][0m
[31m   Join RightOuter, None[0m
[31m    MetastoreRelation FLATER, staff66, None[0m
[31m    MetastoreRelation hu, proj, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Aggregate [count(1) AS _c0#95521L][0m
[31m   Project[0m
[31m    Join RightOuter, None[0m
[31m     Project[0m
[31m      MetastoreRelation FLATER, staff66, None[0m
[31m     Project[0m
[31m      MetastoreRelation hu, proj, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#95521L])[0m
[31m   TungstenExchange SinglePartition[0m
[31m    TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[currentCount#95524L])[0m
[31m     TungstenProject[0m
[31m      CartesianProduct[0m
[31m       HiveTableScan (MetastoreRelation FLATER, staff66, None)[0m
[31m       HiveTableScan (MetastoreRelation hu, proj, None)[0m
  
[31m  Code Generation: true[0m
[31m  _c0[0m
[31m  !== HIVE - 1 row(s) ==   == CATALYST - 1 row(s) ==[0m
[31m  !3                       0 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT * FROM HU.WORKS NATURAL LEFT JOIN HU.PROJ ORDER BY EMPNUM DESC, PNUM
[31m- 0760 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: Reference 'PNUM' is ambiguous, could be: PNUM#95957, PNUM#95959.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'PNUM' is ambiguous, could be: PNUM#95957, PNUM#95959.;[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:278)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:172)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders$1$$anonfun$4.applyOrElse(Analyzer.scala:422)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders$1$$anonfun$4.applyOrElse(Analyzer.scala:420)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders$1.apply(Analyzer.scala:420)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders$1.apply(Analyzer.scala:414)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)[0m
[31m  	at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders(Analyzer.scala:414)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$.resolveAndFindMissing(Analyzer.scala:466)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:443)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:440)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$.apply(Analyzer.scala:440)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$.apply(Analyzer.scala:439)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)[0m
[31m  	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)[0m
[31m  	at scala.collection.immutable.List.foldLeft(List.scala:84)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed$lzycompute(TestHive.scala:195)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed(TestHive.scala:180)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM DESC,'PNUM ASC], true[0m
[31m   'Project [unresolvedalias(*)][0m
[31m    'Join LeftOuter, None[0m
[31m     'UnresolvedRelation [HU,WORKS], None[0m
[31m     'UnresolvedRelation [HU,PROJ], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'PNUM' is ambiguous, could be: PNUM#95965, PNUM#95967.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'PNUM' is ambiguous, could be: PNUM#95973, PNUM#95975.;[0m
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'PNUM' is ambiguous, could be: PNUM#95981, PNUM#95983.;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'PNUM' is ambiguous, could be: PNUM#95989, PNUM#95991.;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: Reference 'PNUM' is ambiguous, could be: PNUM#95997, PNUM#95999.;[0m
[31m  == HIVE - 27 row(s) ==[0m
[31m  P2	e1	NULL	CALM	Code	30000	Vienna[0m
[31m  P5	e1	NULL	IRM	Test	10000	NULL[0m
[31m  p2	e1	NULL	NULL	NULL	NULL	NULL[0m
[31m  low	UPP	NULL	NULL	NULL	NULL	NULL[0m
[31m  P7	E9	10	PROGRAM	RISC	15000	Gaithersburg[0m
[31m  P9	E9	NULL	NULL	NULL	NULL	NULL[0m
[31m  P8	E8	NULL	NULL	NULL	NULL	NULL[0m
[31m  P4	E7	NULL	NULL	NULL	NULL	NULL[0m
[31m  P2	E6	55	CALM	Code	30000	Vienna[0m
[31m  P6	E6	NULL	NULL	NULL	NULL	NULL[0m
[31m  P5	E5	NULL	IRM	Test	10000	NULL[0m
[31m  P4	E4	NULL	NULL	NULL	NULL	NULL[0m
[31m  p4	E4	NULL	NULL	NULL	NULL	NULL[0m
[31m  P2	E3	100	CALM	Code	30000	Vienna[0m
[31m  P5	E3	100	IRM	Test	10000	NULL[0m
[31m  P6	E3	NULL	NULL	NULL	NULL	NULL[0m
[31m  P22	E22	NULL	NULL	NULL	NULL	NULL[0m
[31m  P1	E2	NULL	NULL	NULL	NULL	NULL[0m
[31m  P2	E2	NULL	CALM	Code	30000	Vienna[0m
[31m  P18	E18	NULL	NULL	NULL	NULL	NULL[0m
[31m  P1	E1	NULL	NULL	NULL	NULL	NULL[0m
[31m  P2	E1	NULL	CALM	Code	30000	Vienna[0m
[31m  P4	E1	NULL	NULL	NULL	NULL	NULL[0m
[31m  P5	E1	NULL	IRM	Test	10000	NULL[0m
[31m  P6	E1	NULL	NULL	NULL	NULL	NULL[0m
[31m  P7	E1	NULL	PROGRAM	RISC	15000	Gaithersburg[0m
[31m  p2	E1	NULL	NULL	NULL	NULL	NULL (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT * FROM HU.WORKS JOIN HU.PROJ USING (PNUM) ORDER BY EMPNUM DESC, PNUM
(List(HU, WORKS, PNUM),List(HU, PROJ, PNUM))
[31m- 0761 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'HU.WORKS.PNUM' given input columns pnum, city, empnum, budget, hours, ptype, pnum, pname;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.WORKS.PNUM' given input columns pnum, city, empnum, budget, hours, ptype, pnum, pname;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:119)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM DESC,'PNUM ASC], true[0m
[31m   'Project [unresolvedalias(*)][0m
[31m    'Join Inner, Some(('HU.WORKS.PNUM = 'HU.PROJ.PNUM))[0m
[31m     'UnresolvedRelation [HU,WORKS], None[0m
[31m     'UnresolvedRelation [HU,PROJ], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias(*)[0m
[31m  'Sort ['EMPNUM DESC,'PNUM ASC], true[0m
[31m   'Project [unresolvedalias(*)][0m
[31m    'Join Inner, Some(('HU.WORKS.PNUM = 'HU.PROJ.PNUM))[0m
[31m     MetastoreRelation hu, works, None[0m
[31m     MetastoreRelation hu, proj, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.WORKS.PNUM' given input columns pnum, city, empnum, budget, hours, ptype, pnum, pname;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.WORKS.PNUM' given input columns pnum, city, empnum, budget, hours, ptype, pnum, pname;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'HU.WORKS.PNUM' given input columns pnum, city, empnum, budget, hours, ptype, pnum, pname;[0m
[31m  == HIVE - 11 row(s) ==[0m
[31m  P2	e1	NULL	CALM	Code	30000	Vienna[0m
[31m  P5	e1	NULL	IRM	Test	10000	NULL[0m
[31m  P7	E9	10	PROGRAM	RISC	15000	Gaithersburg[0m
[31m  P2	E6	55	CALM	Code	30000	Vienna[0m
[31m  P5	E5	NULL	IRM	Test	10000	NULL[0m
[31m  P2	E3	100	CALM	Code	30000	Vienna[0m
[31m  P5	E3	100	IRM	Test	10000	NULL[0m
[31m  P2	E2	NULL	CALM	Code	30000	Vienna[0m
[31m  P2	E1	NULL	CALM	Code	30000	Vienna[0m
[31m  P5	E1	NULL	IRM	Test	10000	NULL[0m
[31m  P7	E1	NULL	PROGRAM	RISC	15000	Gaithersburg (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT * FROM HU.WORKS RIGHT JOIN HU.PROJ ON HU.WORKS.PNUM = HU.PROJ.PNUM ORDER BY 1 DESC, 2
calcite cannot do SELECT * FROM HU.WORKS RIGHT JOIN HU.PROJ ON HU.WORKS.PNUM = HU.PROJ.PNUM ORDER BY 1 DESC, 2
[31m- 0762 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT EMPNAME, CITY, T_DECIMAL FROM HU.STAFF LEFT OUTER JOIN SEVEN_TYPES ON -GRADE / 11 BETWEEN T_REAL AND T_DECIMAL ORDER BY EMPNAME
[32m- 0763[0m
Calcite parsing passed, start to transform. SELECT T_INT, T_CHAR, EMPNAME, EMPNUM, GRADE FROM SEVEN_TYPES RIGHT JOIN HU.STAFF ON GRADE IN (10, 11, 13) AND EMPNUM = T_CHAR ORDER BY EMPNAME, T_INT
[32m- 0764[0m
Calcite parsing passed, start to transform. SELECT HU.STAFF.CITY,EMPNAME,PNAME,BUDGET FROM HU.STAFF LEFT JOIN HU.PROJ ON HU.STAFF.CITY = HU.PROJ.CITY AND HU.STAFF.CITY <> 'Vienna' AND EMPNAME <> 'Don' WHERE BUDGET > 15000 OR BUDGET IS NULL ORDER BY HU.STAFF.CITY, EMPNAME, BUDGET
[31m- 0765 *** FAILED ***[0m
[31m  Failed to generate golden answer for query:[0m
[31m  Error: FAILED: SemanticException Line 1:23 Invalid path ''/home/cherry/sotc_cloud-panthera-nist-test/plusd/0765/PROJ.csv'': No files matching path file:/home/cherry/sotc_cloud-panthera-nist-test/plusd/0765/PROJ.csv[0m
[31m  org.apache.spark.sql.execution.QueryExecutionException: FAILED: SemanticException Line 1:23 Invalid path ''/home/cherry/sotc_cloud-panthera-nist-test/plusd/0765/PROJ.csv'': No files matching path file:/home/cherry/sotc_cloud-panthera-nist-test/plusd/0765/PROJ.csv[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:433)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:418)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:211)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:248)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper.runHive(ClientWrapper.scala:418)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper.runSqlHive(ClientWrapper.scala:408)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:558)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:111)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$15.apply(HiveCompSuite.scala:3048)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$15.apply(HiveCompSuite.scala:3033)[0m
[31m  	at scala.runtime.Tuple3Zipped$$anonfun$map$extension$1.apply(Tuple3Zipped.scala:37)[0m
[31m  	at scala.runtime.Tuple3Zipped$$anonfun$map$extension$1.apply(Tuple3Zipped.scala:35)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.runtime.Tuple3Zipped$.map$extension(Tuple3Zipped.scala:35)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3033)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  LOAD DATA LOCAL INPATH '/home/cherry/sotc_cloud-panthera-nist-test/plusd/0765/PROJ.csv' OVERWRITE INTO TABLE PROJ (HiveCompSuite.scala:3068)[0m
Calcite parsing passed, start to transform. SELECT HU.STAFF.CITY,EMPNAME,PNAME,BUDGET FROM HU.STAFF LEFT JOIN HU.PROJ ON HU.STAFF.CITY = HU.PROJ.CITY AND HU.STAFF.CITY <> 'Vienna' WHERE (BUDGET > 15000 OR BUDGET IS NULL) AND EMPNAME <> 'Don' ORDER BY HU.STAFF.CITY, EMPNAME, BUDGET
[31m- 0766 *** FAILED ***[0m
[31m  Failed to generate golden answer for query:[0m
[31m  Error: FAILED: SemanticException Line 1:23 Invalid path ''/home/cherry/sotc_cloud-panthera-nist-test/plusd/0766/PROJ.csv'': No files matching path file:/home/cherry/sotc_cloud-panthera-nist-test/plusd/0766/PROJ.csv[0m
[31m  org.apache.spark.sql.execution.QueryExecutionException: FAILED: SemanticException Line 1:23 Invalid path ''/home/cherry/sotc_cloud-panthera-nist-test/plusd/0766/PROJ.csv'': No files matching path file:/home/cherry/sotc_cloud-panthera-nist-test/plusd/0766/PROJ.csv[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:433)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:418)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:211)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:248)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper.runHive(ClientWrapper.scala:418)[0m
[31m  	at org.apache.spark.sql.hive.client.ClientWrapper.runSqlHive(ClientWrapper.scala:408)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:558)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:111)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$15.apply(HiveCompSuite.scala:3048)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$15.apply(HiveCompSuite.scala:3033)[0m
[31m  	at scala.runtime.Tuple3Zipped$$anonfun$map$extension$1.apply(Tuple3Zipped.scala:37)[0m
[31m  	at scala.runtime.Tuple3Zipped$$anonfun$map$extension$1.apply(Tuple3Zipped.scala:35)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.runtime.Tuple3Zipped$.map$extension(Tuple3Zipped.scala:35)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3033)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  LOAD DATA LOCAL INPATH '/home/cherry/sotc_cloud-panthera-nist-test/plusd/0766/PROJ.csv' OVERWRITE INTO TABLE PROJ (HiveCompSuite.scala:3068)[0m
Calcite parsing passed, start to transform. SELECT XX.T_INT, YY.T_INT FROM SEVEN_TYPES XX RIGHT OUTER JOIN SEVEN_TYPES YY ON XX.T_INT = YY.T_INT +1 ORDER BY YY.T_INT
[32m- 0767[0m
Calcite parsing passed, start to transform. SELECT GRADE, T_FLOAT, T_DOUBLE FROM HU.STAFF LEFT JOIN SEVEN_TYPES T7 ON GRADE * -40 > T7.T_FLOAT OR (T_DOUBLE -542.5 < GRADE AND T_DOUBLE -541.5 > GRADE) ORDER BY GRADE
[32m- 0768[0m
Calcite parsing passed, start to transform. SELECT CAST (100.5 AS DECIMAL (3)) FROM HU.ECCO
[32m- 0769[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NO_DUCK WHERE GOOSE = 23.23
[32m- 0770[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NO_DUCK
[32m- 0771[0m
Calcite parsing passed, start to transform. SELECT CAST (CAST (3 AS DEC (5, 3)) AS CHAR (5)) FROM HU.ECCO
[32m- 0772[0m
Calcite parsing passed, start to transform. SELECT OSPREY FROM NO_DUCK
[32m- 0773[0m
Calcite parsing passed, start to transform. SELECT OSPREY FROM NO_DUCK
[32m- 0774[0m
Calcite parsing passed, start to transform. SELECT CAST (-GOOSE AS CHAR (5)) FROM NO_DUCK
[32m- 0775[0m
Calcite parsing passed, start to transform. SELECT CAST (-ALBATROSS AS CHAR (5)) FROM NO_DUCK
[32m- 0776[0m
Calcite parsing passed, start to transform. SELECT CAST (0230E-1 AS CHAR (10)) FROM HU.ECCO
[32m- 0777[0m
Calcite parsing passed, start to transform. SELECT CAST (0230E+1 AS CHAR (10)) FROM HU.ECCO
[32m- 0778[0m
Calcite parsing passed, start to transform. SELECT OSPREY FROM NO_DUCK
[32m- 0779[0m
Calcite parsing passed, start to transform. SELECT OSPREY FROM NO_DUCK
[32m- 0780[0m
Calcite parsing passed, start to transform. SELECT OSPREY FROM NO_DUCK
[32m- 0781[0m
Calcite parsing passed, start to transform. SELECT CAST (ALBATROSS AS CHAR (4)) FROM NO_DUCK
[32m- 0782[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NO_DUCK WHERE GOOSE IS NULL
[32m- 0784[0m
Calcite parsing passed, start to transform. SELECT CAST (GOOSE AS INT) FROM NO_DUCK
[32m- 0785[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF
[32m- 0786[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE EMPNAME LIKE 'H%'
[32m- 0787[0m
Calcite parsing passed, start to transform. SELECT * FROM CTRANS
[32m- 0788[0m
Calcite parsing passed, start to transform. SELECT * FROM TRANSIENT ORDER BY WINDOW_ID
[32m- 0789[0m
Calcite parsing passed, start to transform. SELECT * FROM HU.STAFF LEFT OUTER JOIN HU.WORKS USING (EMPNUM)
(List(HU, STAFF, EMPNUM),List(HU, WORKS, EMPNUM))
[31m- 0790 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'HU.STAFF.EMPNUM' given input columns pnum, empnum, hours, empnum, city, empname, grade;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.EMPNUM' given input columns pnum, empnum, hours, empnum, city, empname, grade;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:119)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias(*)][0m
[31m   'Join LeftOuter, Some(('HU.STAFF.EMPNUM = 'HU.WORKS.EMPNUM))[0m
[31m    'UnresolvedRelation [HU,STAFF], None[0m
[31m    'UnresolvedRelation [HU,WORKS], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias(*)[0m
[31m  'Project [unresolvedalias(*)][0m
[31m   'Join LeftOuter, Some(('HU.STAFF.EMPNUM = 'HU.WORKS.EMPNUM))[0m
[31m    MetastoreRelation hu, staff, None[0m
[31m    MetastoreRelation hu, works, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.EMPNUM' given input columns pnum, empnum, hours, empnum, city, empname, grade;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.EMPNUM' given input columns pnum, empnum, hours, empnum, city, empname, grade;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF.EMPNUM' given input columns pnum, empnum, hours, empnum, city, empname, grade;[0m
[31m  == HIVE - 4 row(s) ==[0m
[31m  E7	SULLIVAN	15	Gaithersburg	P4	NULL[0m
[31m  E13	ff	82	gg	NULL	NULL[0m
[31m  E6	Fidel	0	Havana	P2	55[0m
[31m  E6	Fidel	0	Havana	P6	NULL (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM XX WHERE C4 = 'Timestamp' AND C5 BETWEEN TIMESTAMP '1993-11-10 00:01:00' AND TIMESTAMP '1993-11-10 00:03:00'
[32m- 0791[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.GRANT010
[32m- 0792[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.GRANT010
[32m- 0793[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.GRANT010
[32m- 0794[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.GRANT010
[32m- 0795[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.GRANT010 WHERE C1 = 0
[32m- 0796[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.GRANT010
[32m- 0797[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.GRANT010
[32m- 0798[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.GRANT010
[32m- 0799[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FLATER.X WHERE A = 0 AND B = 2 AND C = 0 AND D = 0
[32m- 0800[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, PNUM FROM WORKS ORDER BY EMPNUM, PNUM
[32m- 0801[0m
Calcite parsing passed, start to transform. SELECT SALARY, EMPNAME, HOURS, CITY FROM CTS1.STAFFb WHERE NULLIF(SALARY,HOURS) IS NULL ORDER BY EMPNAME
[32m- 0802[0m
Calcite parsing passed, start to transform. SELECT SALARY,PNUM,HOURS,NULLIF(EMPNAME,CITY) FROM CTS1.STAFFb WHERE EMPNAME = CITY OR EMPNAME IS NULL ORDER BY PNUM
[32m- 0803[0m
Calcite parsing passed, start to transform. SELECT SUM(NULLIF(NULLIF(SALARY,10000),20000)) FROM STAFFb
[32m- 0804[0m
SELECT NUM6 FROM   T4 WHERE  NUM6 = 2 AND STR110 <= 'Second character '--Comments here 'literal.'failed.
[32m- 0805[0m
SELECT NUM6 FROM   T4 WHERE  STR110 = 'Third character literal.'--Comments here 'second fragment' 'third fragment.'failed.
[32m- 0806[0m
SELECT COUNT(*) FROM   T4 WHERE  STR110 < 'An indifferent'--Comments ' charac' 'ter literal.'failed.
[32m- 0807[0m
SELECT COUNT(*) FROM   T4 WHERE  STR110 >= 'An indifferent'--Comments ' charac' 'ter literal.'failed.
[32m- 0808[0m
Calcite parsing passed, start to transform. SELECT STR110, COL4 FROM T4 WHERE NUM6 = 11
[32m- 0809[0m
Calcite parsing passed, start to transform. SELECT STR110 FROM T4 WHERE  NUM6 = 12
[32m- 0810[0m
Calcite parsing passed, start to transform. SELECT STR110 FROM   T4 WHERE  NUM6 = 13
[32m- 0811[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  'Alice' LIKE 'Alice'
[32m- 0812[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  'Equal_literal' NOT LIKE 'Eq_alS_literal%' ESCAPE 'S'
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  'Equal_literal' NOT LIKE 'Eq_alS_literal%' ESCAPE 'S'
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  'Equal_literal' NOT LIKE 'Eq_alS_literal%' ESCAPE 'S'
[31m- 0813 *** FAILED ***[0m
[31m  Results do not match for 0813:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias(count(1))][0m
[31m   'Filter NOT Equal_literal LIKE Eq_alS_literal%[0m
[31m    'UnresolvedRelation [STAFF], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  _c0: bigint[0m
[31m  Aggregate [count(1) AS _c0#104459L][0m
[31m   Filter NOT Equal_literal LIKE Eq_alS_literal%[0m
[31m    MetastoreRelation CTS1, staff, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Aggregate [count(1) AS _c0#104459L][0m
[31m   Project[0m
[31m    MetastoreRelation CTS1, staff, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#104459L])[0m
[31m   TungstenExchange SinglePartition[0m
[31m    TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[currentCount#104462L])[0m
[31m     HiveTableScan (MetastoreRelation CTS1, staff, None)[0m
  
[31m  Code Generation: true[0m
[31m  _c0[0m
[31m  !== HIVE - 1 row(s) ==   == CATALYST - 1 row(s) ==[0m
[31m  !0                       5 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  EMPNAME LIKE EMPNAME
[32m- 0815[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  EMPNAME LIKE CITY ESCAPE 'S'
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  EMPNAME LIKE CITY ESCAPE 'S'
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM   STAFF WHERE  EMPNAME LIKE CITY ESCAPE 'S'
[31m- 0816 *** FAILED ***[0m
[31m  Results do not match for 0816:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias(count(1))][0m
[31m   'Filter 'EMPNAME LIKE 'CITY[0m
[31m    'UnresolvedRelation [STAFF], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  _c0: bigint[0m
[31m  Aggregate [count(1) AS _c0#105239L][0m
[31m   Filter EMPNAME#105236 LIKE CITY#105238[0m
[31m    MetastoreRelation CTS1, staff, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Aggregate [count(1) AS _c0#105239L][0m
[31m   Project[0m
[31m    Filter EMPNAME#105236 LIKE CITY#105238[0m
[31m     MetastoreRelation CTS1, staff, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#105239L])[0m
[31m   TungstenExchange SinglePartition[0m
[31m    TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[currentCount#105242L])[0m
[31m     Project[0m
[31m      Filter EMPNAME#105236 LIKE CITY#105238[0m
[31m       HiveTableScan [EMPNAME#105236,CITY#105238], (MetastoreRelation CTS1, staff, None)[0m
  
[31m  Code Generation: true[0m
[31m  _c0[0m
[31m  !== HIVE - 1 row(s) ==   == CATALYST - 1 row(s) ==[0m
[31m  !1                       0 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CTS1.STAFF
[32m- 0817[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CTS1.STAFF
[32m- 0818[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CTS1.STAFF
[32m- 0819[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CTS1.STAFF
[32m- 0820[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM CTS1.STAFF
[32m- 0821[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CTS1.STAFF
[32m- 0822[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  CTS2.PROJ_MAN
[32m- 0823[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  CTS1.STAFF
[32m- 0824[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CTS1.STAFF
[32m- 0825[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEST6840C
[32m- 0826[0m
Calcite parsing passed, start to transform. SELECT NUM_C1,CH_C1,NUM_C2,CH_C2 FROM TEST6840C WHERE NUM_C1 = 1000
[32m- 0827[0m
Calcite parsing passed, start to transform. SELECT NUM_C1,CH_C1,NUM_C2,CH_C2 FROM TEST6840C WHERE NUM_C1 = 1001
[32m- 0828[0m
Calcite parsing passed, start to transform. SELECT NUM_C1,CH_C1,NUM_C2,CH_C2 FROM TEST6840C WHERE NUM_C1 = 1002
[32m- 0829[0m
Calcite parsing passed, start to transform. SELECT NUM_C1,CH_C1,NUM_C2,CH_C2 FROM TEST6840C WHERE NUM_C1 = 1003
[32m- 0830[0m
Calcite parsing passed, start to transform. SELECT NUM_C1,CH_C1,NUM_C2,CH_C2 FROM TEST6840C WHERE NUM_C1 = 1004
[32m- 0831[0m
Calcite parsing passed, start to transform. SELECT COL_1,COL_2 FROM CTS1.TABLE728b WHERE COL_1 = 'NICKOS' AND COL_2 = 'GEORGE'
[32m- 0832[0m
Calcite parsing passed, start to transform. SELECT COL_1,COL_2 FROM CTS1.TABLE728b WHERE COL_1 = 'HARRY' AND COL_2 = 'TANIA'
[32m- 0833[0m
Calcite parsing passed, start to transform. SELECT COLUMNOFCHARACTERSA, columnofcharactersb, cOlUmNoFNUMERICss_0, cOlUmNoFNUMERICss_1 FROM CTS1.TESTA6439
[32m- 0834[0m
Calcite parsing passed, start to transform. SELECT  COUNT(*) FROM TEST6740A NATURAL FULL OUTER JOIN TEST6740B
[32m- 0835[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEST6740A NATURAL FULL JOIN TEST6740B
[32m- 0836[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEST6740A NATURAL FULL OUTER JOIN TEST6740B
[32m- 0837[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TEST6740B NATURAL FULL JOIN TEST6740A
[32m- 0838[0m
Calcite parsing passed, start to transform. SELECT * FROM TEST6840A FULL OUTER JOIN TEST6840B ON NUM_A = NUM_B ORDER BY NUM_A
[32m- 0839[0m
Calcite parsing passed, start to transform. SELECT * FROM TEST6840A FULL JOIN TEST6840B ON CH_A = CH_B ORDER BY NUM_A
[32m- 0840[0m
Calcite parsing passed, start to transform. SELECT  COUNT(*) FROM TEST6840C
[32m- 0841[0m
Calcite parsing passed, start to transform. SELECT  COUNT(*) FROM TEST6840C WHERE NUM_C1 = 1 AND CH_C1 = 'A' AND NUM_C2 IS NULL AND CH_C2 IS NULL
[32m- 0842[0m
Calcite parsing passed, start to transform. SELECT  COUNT(*) FROM TEST6840C WHERE NUM_C1 = 2 AND CH_C1 = 'B' AND NUM_C2 = 2 AND CH_C2 = 'C'
[32m- 0843[0m
Calcite parsing passed, start to transform. SELECT  COUNT(*) FROM TEST6840C WHERE NUM_C1 = 2 AND CH_C1 = 'B' AND NUM_C2 = 3  AND CH_C2 = 'A'
[32m- 0844[0m
NoViableAltException(26@[150:5: ( ( Identifier LPAREN )=> partitionedTableFunction | tableSource | subQuerySource | virtualTableSource )])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:144)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:3711)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:1873)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1518)
	at org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:45861)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41516)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0845 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near '(' 'TEST6840B' 'FULL' in from source; line 1 pos 15[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM TAB734 WHERE CSTR1 = CSTR2
[32m- 0846[0m
Calcite parsing passed, start to transform. SELECT COUNT(CSTR1) FROM TAB734 WHERE CSTR1 <> N' * '
[32m- 0847[0m
Calcite parsing passed, start to transform. SELECT COUNT(CSTR2) FROM TAB734 WHERE N'++++' <> CSTR2
[32m- 0848[0m
Calcite parsing passed, start to transform. SELECT C1,C2 FROM CTS1.TAB735 ORDER BY C1
[32m- 0849[0m
Calcite parsing passed, start to transform. SELECT COUNT(ALL 115.5), COUNT(ALL 'ATHINA'), COUNT(ALL 255), COUNT(*) FROM CL_DATA_TYPE
[32m- 0850[0m
Calcite parsing passed, start to transform. SELECT COUNT(*),COUNT(ALL 119), COUNT(ALL 'GIORGOS') , COUNT(CL_CHAR), COUNT(CL_REAL) FROM CL_DATA_TYPE
[32m- 0851[0m
Calcite parsing passed, start to transform. SELECT COUNT(*), COUNT(ALL 1000), COUNT(ALL 'STEFOS'), COUNT(CL_CHAR), COUNT(CL_REAL) FROM CL_DATA_TYPE
[32m- 0852[0m
Calcite parsing passed, start to transform. SELECT MAX(AVSAL) FROM V000V
[32m- 0853[0m
Calcite parsing passed, start to transform. SELECT EMPNAME, NULLIF (SALARY,HOURS) FROM CTS1.STAFFb WHERE SEX = 'M' AND PNUM NOT IN ('P1','P2','P3','P6','P8') AND (SALARY <> HOURS OR SALARY IS NULL OR HOURS IS NULL) ORDER BY PNUM
[32m- 0854[0m
Calcite parsing passed, start to transform. SELECT NULLIF (EMPNAME,CITY), SALARY FROM CTS1.STAFFb WHERE SEX = 'M' AND PNUM NOT IN ('P1','P2','P3','P5','P7') AND (EMPNAME <> CITY OR EMPNAME IS NULL OR CITY IS NULL) ORDER BY PNUM
[32m- 0855[0m
Calcite parsing passed, start to transform. SELECT * FROM CTS1.TEMP1426 ORDER BY EMPNAME
[32m- 0856[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, COALESCE(SALARY,GRADE,HOURS), COALESCE(EMPNAME,LOC,DEPTNO) FROM CTS1.CL_EMPLOYEE ORDER BY EMPNUM
[32m- 0857[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, COALESCE('NICKOS',DEPTNO,LOC), COALESCE(SALARY,GRADE,47000) FROM CL_EMPLOYEE WHERE EMPNUM = 7000
[32m- 0858[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.STAFFc WHERE GRADE IS NULL
[32m- 0859[0m
Calcite parsing passed, start to transform. SELECT GRADE FROM CTS1.STAFFc WHERE EMPNUM = 'E3'
[32m- 0860[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.STAFFc WHERE MGR = 'E8'
[32m- 0861[0m
Calcite parsing passed, start to transform. SELECT MGR FROM CTS1.STAFFc WHERE EMPNUM = 'E6'
[32m- 0862[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFFd WHERE GRADE = 22
[32m- 0863[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFFd WHERE GRADE = 23
[32m- 0864[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFFd WHERE GRADE = 24
[32m- 0865[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFFd WHERE GRADE IS NULL
[32m- 0866[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFFd WHERE MGR = 'E7'
[32m- 0867[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFFd WHERE MGR = 'E6'
[32m- 0868[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFFd WHERE MGR = 'E4'
[32m- 0869[0m
Calcite parsing passed, start to transform. SELECT COUNT (DISTINCT COL1) FROM CTS1.ET
[32m- 0871[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.ET
[32m- 0872[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.ET
[32m- 0873[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.ET
[32m- 0874[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF_CTS2
[32m- 0875[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM STAFF_CTS
[32m- 0876[0m
Calcite parsing passed, start to transform. SELECT col2, col3, col4 FROM CTS1.ET ORDER BY col3, col4
[32m- 0877[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.STAFFb
[32m- 0878[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.STAFFa
[32m- 0879[0m
Calcite parsing passed, start to transform. SELECT COUNT (*), col5, col6 FROM CTS1.ET GROUP BY col6, col5 ORDER BY 2,3
[32m- 0880[0m
Calcite parsing passed, start to transform. SELECT EMPNAME FROM STAFF WHERE (SELECT EMPNUM FROM WORKS WHERE PNUM = 'P3') = EMPNUM
calcite cannot do SELECT EMPNAME FROM STAFF WHERE (SELECT EMPNUM FROM WORKS WHERE PNUM = 'P3') = EMPNUM
[31m- 0881 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT B FROM CTS1.TV WHERE A = 1
[32m- 0882[0m
Calcite parsing passed, start to transform. SELECT B FROM CTS1.TV WHERE A = 2
[32m- 0883[0m
Calcite parsing passed, start to transform. SELECT B FROM CTS1.TV WHERE A = 3
[32m- 0884[0m
Calcite parsing passed, start to transform. SELECT B FROM CTS1.TV WHERE A = 4
[32m- 0885[0m
Calcite parsing passed, start to transform. SELECT B FROM CTS1.TV WHERE A = 5
[32m- 0886[0m
Calcite parsing passed, start to transform. SELECT DISTINCT A, (SELECT D FROM TW WHERE E = X.A) FROM TV X, TW Y WHERE A = 1
calcite cannot do SELECT DISTINCT A, (SELECT D FROM TW WHERE E = X.A) FROM TV X, TW Y WHERE A = 1
[31m- 0887 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT DISTINCT A, (SELECT D FROM TW WHERE E = X.A) FROM TV X, TW Y WHERE A = 3
calcite cannot do SELECT DISTINCT A, (SELECT D FROM TW WHERE E = X.A) FROM TV X, TW Y WHERE A = 3
[31m- 0888 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT TTA, TTB, TTC FROM CTS1.TT WHERE (SELECT TUD FROM TU WHERE TU.TUE = TT.TTA) IS NULL ORDER BY TTA DESC
calcite cannot do SELECT TTA, TTB, TTC FROM CTS1.TT WHERE (SELECT TUD FROM TU WHERE TU.TUE = TT.TTA) IS NULL ORDER BY TTA DESC
[31m- 0889 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT TTA, TTB, TTC FROM CTS1.TT WHERE (SELECT TUD FROM TU WHERE TU.TUE = TT.TTA) IS NOT NULL ORDER BY TTA
calcite cannot do SELECT TTA, TTB, TTC FROM CTS1.TT WHERE (SELECT TUD FROM TU WHERE TU.TUE = TT.TTA) IS NOT NULL ORDER BY TTA
[31m- 0890 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.TT WHERE TTB IS NULL OR TTC IS NULL
[32m- 0891[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.TT WHERE TTB IS NOT NULL AND TTC IS NOT NULL
[32m- 0892[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM CTS1.TT WHERE NOT (TTB IS NULL AND TTC IS NULL)
[32m- 0893[0m
Calcite parsing passed, start to transform. SELECT * FROM STAFF1 NATURAL FULL OUTER JOIN STAFFA ORDER BY EMPNUM, EMPNAME, GRADE, PNUM
[31m- 0894 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#113971, EMPNUM#113977.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#113971, EMPNUM#113977.;[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:278)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:172)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders$1$$anonfun$4.applyOrElse(Analyzer.scala:422)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders$1$$anonfun$4.applyOrElse(Analyzer.scala:420)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders$1.apply(Analyzer.scala:420)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders$1.apply(Analyzer.scala:414)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)[0m
[31m  	at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$resolveSortOrders(Analyzer.scala:414)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$.resolveAndFindMissing(Analyzer.scala:466)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:443)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:440)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$.apply(Analyzer.scala:440)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSortReferences$.apply(Analyzer.scala:439)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)[0m
[31m  	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)[0m
[31m  	at scala.collection.immutable.List.foldLeft(List.scala:84)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed$lzycompute(TestHive.scala:195)[0m
[31m  	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.analyzed(TestHive.scala:180)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM ASC,'EMPNAME ASC,'GRADE ASC,'PNUM ASC], true[0m
[31m   'Project [unresolvedalias(*)][0m
[31m    'Join FullOuter, None[0m
[31m     'UnresolvedRelation [STAFF1], None[0m
[31m     'UnresolvedRelation [STAFFA], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#113980, EMPNUM#113986.;[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#113989, EMPNUM#113995.;[0m
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#113998, EMPNUM#114004.;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#114007, EMPNUM#114013.;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: Reference 'EMPNUM' is ambiguous, could be: EMPNUM#114016, EMPNUM#114022.;[0m
[31m  == HIVE - 12 row(s) ==[0m
[31m  E1	Alice	4	Lyon	20	40000	11[0m
[31m  E1	Alice	8	Lyon	20	40000	11[0m
[31m  E1	Alice	12	Geneva	20	40000	11[0m
[31m  E2	Betty	16	Strasbourg	15	20000	12[0m
[31m  E2	Betty	16	Strasbourg	15	20000	13[0m
[31m  E2	Betty	20	Munich	15	20000	12[0m
[31m  E2	Betty	20	Munich	15	20000	13[0m
[31m  E3	Colin	24	Leuven	10	15000	14[0m
[31m  E3	Colin	24	Leuven	10	8000	15[0m
[31m  E3	Colin	24	Leuven	10	8000	16[0m
[31m  E4	Daniel	28	Cologne	NULL	NULL	NULL[0m
[31m  E5	Edward	NULL	NULL	30	50000	17 (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT COUNT (DISTINCT COL5) FROM CTS1.ET
[32m- 0895[0m
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.type(HiveParser.java:38610)
	at org.apache.hadoop.hive.ql.parse.HiveParser.colType(HiveParser.java:38367)
	at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameType(HiveParser.java:38051)
	at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameTypeList(HiveParser.java:36203)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:5214)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2640)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1650)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:258)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:283)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:62)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.getLogicalPlan(CalciteDialect.scala:54)
	at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:166)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:42)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:189)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:280)
	at org.apache.spark.sql.hive.test.TestHiveContext$QueryExecution.<init>(TestHive.scala:179)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$13.apply(HiveCompSuite.scala:3028)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3028)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
	at org.scalatest.Suite$class.run(Suite.scala:1421)
	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
[31m- 0896 *** FAILED ***[0m
[31m  org.apache.spark.sql.AnalysisException: cannot recognize input near 'NCHAR' ')' 'ROW' in column type; line 1 pos 33[0m
[31m  at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:296)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)[0m
[31m  at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)[0m
[31m  at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT VT1, VT2, VT3 FROM V_DATA_TYPE WHERE NOT VT1 = 0 ORDER BY VT2, VT3
[31m- 0897 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'NOT VT1' due to data type mismatch: argument 1 requires boolean type, however, 'VT1' is of double type.;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'NOT VT1' due to data type mismatch: argument 1 requires boolean type, however, 'VT1' is of double type.;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:61)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['VT2 ASC,'VT3 ASC], true[0m
[31m   'Project [unresolvedalias('VT1),unresolvedalias('VT2),unresolvedalias('VT3)][0m
[31m    'Filter (NOT 'VT1 = 0)[0m
[31m     'UnresolvedRelation [V_DATA_TYPE], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias('VT1)[0m
[31m  'Sort ['VT2 ASC,'VT3 ASC], true[0m
[31m   'Project [unresolvedalias('VT1),unresolvedalias('VT2),unresolvedalias('VT3)][0m
[31m    'Filter (NOT VT1#114277 = 0)[0m
[31m     MetastoreRelation CTS1, v_data_type, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'NOT VT1' due to data type mismatch: argument 1 requires boolean type, however, 'VT1' is of double type.;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'NOT VT1' due to data type mismatch: argument 1 requires boolean type, however, 'VT1' is of double type.;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'NOT VT1' due to data type mismatch: argument 1 requires boolean type, however, 'VT1' is of double type.;[0m
[31m  == HIVE - 3 row(s) ==[0m
[31m  56	2	3[0m
[31m  70	4	3[0m
[31m  42	4	5 (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT VT1, VT2, VT3 FROM V_DATA_TYPE WHERE VT2 = 1
[32m- 0898[0m
Calcite parsing passed, start to transform. SELECT SUM(NUM) FROM DATA_TYPE WHERE NUM IS NOT NULL
[32m- 0899[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM TX WHERE TX2 || TX3 IS NOT NULL
[32m- 0900[0m
Calcite parsing passed, start to transform. SELECT TX1 FROM TX WHERE TX3 || TX2 IS NULL
[32m- 0901[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE CITY IS NULL
[32m- 0902[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE GRADE = 15
[32m- 0903[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE CASE GRADE WHEN 10 THEN 12 WHEN 13 THEN 12 END = 12
[32m- 0904[0m
Calcite parsing passed, start to transform. SELECT SUM(HOURS) FROM V0866
[32m- 0905[0m
Calcite parsing passed, start to transform. SELECT COALESCE (CITY, EMPNUM) FROM HU.STAFF ORDER BY 1
[32m- 0906[0m
Calcite parsing passed, start to transform. SELECT * FROM APPLES UNION ALL SELECT * FROM ORANGES ORDER BY 1
calcite cannot do SELECT * FROM APPLES UNION ALL SELECT * FROM ORANGES ORDER BY 1
[31m- 0912 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT * FROM T0878
[32m- 0913[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM T0879
[32m- 0914[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE 1 IS NOT NULL
[32m- 0915[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, CITY, SALARY FROM HU.STAFF3 FULL OUTER JOIN STAFF66 USING (EMPNUM) ORDER BY EMPNUM
(List(HU, STAFF3, EMPNUM),List(STAFF66, EMPNUM))
[31m- 0916 *** FAILED ***[0m
[31m  Failed to execute query using catalyst:[0m
[31m  Error: cannot resolve 'HU.STAFF3.EMPNUM' given input columns empname, empnum, grade, empnum, grade, city, salary, empname;[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.EMPNUM' given input columns empname, empnum, grade, empnum, grade, city, salary, empname;[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:119)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)[0m
[31m  	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)[0m
[31m  	at scala.collection.Iterator$class.foreach(Iterator.scala:727)[0m
[31m  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)[0m
[31m  	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)[0m
[31m  	at scala.collection.AbstractIterator.to(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)[0m
[31m  	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)[0m
[31m  	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)[0m
[31m  	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)[0m
[31m  	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)[0m
[31m  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:912)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:911)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:915)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:920)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:918)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:924)[0m
[31m  	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:573)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3079)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1$$anonfun$16.apply(HiveCompSuite.scala:3077)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)[0m
[31m  	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[0m
[31m  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)[0m
[31m  	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)[0m
[31m  	at scala.collection.AbstractTraversable.map(Traversable.scala:105)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply$mcV$sp(HiveCompSuite.scala:3077)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompSuite$$anonfun$createQueryTest$1.apply(HiveCompSuite.scala:2965)[0m
[31m  	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:22)[0m
[31m  	at org.scalatest.Transformer.apply(Transformer.scala:20)[0m
[31m  	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)[0m
[31m  	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)[0m
[31m  	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)[0m
[31m  	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)[0m
[31m  	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m
[31m  	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)[0m
[31m  	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)[0m
[31m  	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)[0m
[31m  	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1424)[0m
[31m  	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)[0m
[31m  	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)[0m
[31m  	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:44)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)[0m
[31m  	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)[0m
[31m  	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)[0m
[31m  	at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)[0m
[31m  	at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)[0m
[31m  	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)[0m
[31m  	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)[0m
[31m  	at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.Suite$class.run(Suite.scala:1421)[0m
[31m  	at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)[0m
[31m  	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)[0m
[31m  	at scala.collection.immutable.List.foreach(List.scala:318)[0m
[31m  	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)[0m
[31m  	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)[0m
[31m  	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)[0m
[31m  	at org.scalatest.tools.Runner$.main(Runner.scala:860)[0m
[31m  	at org.scalatest.tools.Runner.main(Runner.scala)[0m
  
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['EMPNUM ASC], true[0m
[31m   'Project [unresolvedalias('EMPNUM),unresolvedalias('CITY),unresolvedalias('SALARY)][0m
[31m    'Join FullOuter, Some(('HU.STAFF3.EMPNUM = 'STAFF66.EMPNUM))[0m
[31m     'UnresolvedRelation [HU,STAFF3], None[0m
[31m     'UnresolvedRelation [STAFF66], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias('EMPNUM)[0m
[31m  'Sort ['EMPNUM ASC], true[0m
[31m   'Project [unresolvedalias('EMPNUM),unresolvedalias('CITY),unresolvedalias('SALARY)][0m
[31m    'Join FullOuter, Some(('HU.STAFF3.EMPNUM = EMPNUM#116596))[0m
[31m     MetastoreRelation hu, staff3, None[0m
[31m     MetastoreRelation FLATER, staff66, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.EMPNUM' given input columns empname, empnum, grade, empnum, grade, city, salary, empname;[0m
[31m  == Physical Plan ==[0m
[31m  org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.EMPNUM' given input columns empname, empnum, grade, empnum, grade, city, salary, empname;[0m
[31m  Code Generation: org.apache.spark.sql.AnalysisException: cannot resolve 'HU.STAFF3.EMPNUM' given input columns empname, empnum, grade, empnum, grade, city, salary, empname;[0m
[31m  == HIVE - 0 row(s) == (HiveCompSuite.scala:3091)[0m
Calcite parsing passed, start to transform. SELECT NUMTEST - 999999999999990, NUMTEST / 9999999 FROM HU.P15 WHERE NUMTEST > 0
[32m- 0917[0m
Calcite parsing passed, start to transform. SELECT NUMTEST + 999999999999990, NUMTEST / 9999999 FROM HU.P15 WHERE NUMTEST < 0
[32m- 0918[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.P15 WHERE NUMTEST = 562949953421312
[32m- 0919[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.P15 WHERE NUMTEST = 562949953421313
[32m- 0920[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.P15 WHERE NUMTEST = 562949953421314
[32m- 0921[0m
Calcite parsing passed, start to transform. SELECT C1, C2, C3, C4, C5, C6, C7, C8, C9, C10, C11, C12, C13, C14, C15, C16, C17, C18, C19, C20, C21, C22, C23, C24, C25, C26, C27, C28, C29, C30, C31, C32, C33, C34, C35, C36, C37, C38, C39, C40, C41, C42, C43, C44, C45, C46, C47, C48, C49, C50, C51, C52, C53, C54, C55, C56, C57, C58, C59, C60, C61, C62, C63, C64, C65, C66, C67, C68, C69, C70, C71, C72, C73, C74, C75, C76, C77, C78, C79, C80, C81, C82, C83, C84, C85, C86, C87, C88, C89, C90, C91, C92, C93, C94, C95, C96, C97, C98, C99, C100 FROM HU.T100
[32m- 0922[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.WORKS T01, HU.PROJ T02, HU.STAFF T03, USIG T04, U_SIG T05, BASE_VS1 T06, VS1 T07, VS2 T08, HU.VSTAFF3 T09, BASE_WCOV T10 WHERE T03.EMPNUM > 'E1'
[32m- 0923[0m
Calcite parsing passed, start to transform. SELECT LONG_INT, LONG_INT /1000000, LONG_INT - 123456789000000. FROM LONGINT
[32m- 0924[0m
Calcite parsing passed, start to transform. SELECT C1,C21,C41,C61,C81,C100 FROM T100
[32m- 0925[0m
Calcite parsing passed, start to transform. SELECT STR110,STR180,STR216 FROM T2000
[32m- 0926[0m
Calcite parsing passed, start to transform. SELECT COL1,COL2,COL3,COL4,COL5,COL6,COL7,COL8 FROM T8
[32m- 0927[0m
Calcite parsing passed, start to transform. SELECT STR110 FROM T4
[32m- 0928[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  T12
[32m- 0929[0m
Calcite parsing passed, start to transform. SELECT COL6,SUM(COL11),MAX(COL12) FROM T12 GROUP BY COL1,COL5,COL3,COL6,COL2,COL4 ORDER BY COL6 DESC
Calcite parsing passed, start to transform. SELECT COL6,SUM(COL11),MAX(COL12) FROM T12 GROUP BY COL1,COL5,COL3,COL6,COL2,COL4 ORDER BY COL6 DESC
Calcite parsing passed, start to transform. SELECT COL6,SUM(COL11),MAX(COL12) FROM T12 GROUP BY COL1,COL5,COL3,COL6,COL2,COL4 ORDER BY COL6 DESC
[31m  Results do not match for 0930:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['COL6 DESC], true[0m
[31m   'Aggregate ['COL1,'COL5,'COL3,'COL6,'COL2,'COL4], [unresolvedalias('COL6),unresolvedalias('SUM('COL11)),unresolvedalias('MAX('COL12))][0m
[31m    'UnresolvedRelation [T12], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  COL6: string, _c1: double, _c2: double[0m
[31m  Sort [COL6#118800 DESC], true[0m
[31m   Aggregate [COL1#118795,COL5#118799,COL3#118797,COL6#118800,COL2#118796,COL4#118798], [COL6#118800,sum(COL11#118805) AS _c1#118807,max(COL12#118806) AS _c2#118808][0m
[31m    MetastoreRelation HU, t12, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Sort [COL6#118800 DESC], true[0m
[31m   Aggregate [COL1#118795,COL5#118799,COL3#118797,COL6#118800,COL2#118796,COL4#118798], [COL6#118800,sum(COL11#118805) AS _c1#118807,max(COL12#118806) AS _c2#118808][0m
[31m    Project [COL12#118806,COL4#118798,COL5#118799,COL2#118796,COL3#118797,COL11#118805,COL6#118800,COL1#118795][0m
[31m     MetastoreRelation HU, t12, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenSort [COL6#118800 DESC], true, 0[0m
[31m   ConvertToUnsafe[0m
[31m    Exchange rangepartitioning(COL6#118800 DESC)[0m
[31m     ConvertToSafe[0m
[31m      TungstenAggregate(key=[COL1#118795,COL5#118799,COL3#118797,COL6#118800,COL2#118796,COL4#118798], functions=[(sum(COL11#118805),mode=Final,isDistinct=false),(max(COL12#118806),mode=Final,isDistinct=false)], output=[COL6#118800,_c1#118807,_c2#118808])[0m
[31m       TungstenExchange hashpartitioning(COL1#118795,COL5#118799,COL3#118797,COL6#118800,COL2#118796,COL4#118798)[0m
[31m        TungstenAggregate(key=[COL1#118795,COL5#118799,COL3#118797,COL6#118800,COL2#118796,COL4#118798], functions=[(sum(COL11#118805),mode=Partial,isDistinct=false),(max(COL12#118806),mode=Partial,isDistinct=false)], output=[COL1#118795,COL5#118799,COL3#118797,COL6#118800,COL2#118796,COL4#118798,currentSum#118813,max#118815])[0m
[31m         HiveTableScan [COL12#118806,COL4#118798,COL5#118799,COL2#118796,COL3#118797,COL11#118805,COL6#118800,COL1#118795], (MetastoreRelation HU, t12, None)[0m
  
[31m  Code Generation: true[0m
[31m  COL6	_c1	_c2[0m
[31m  !== HIVE - 10 row(s) ==   == CATALYST - 10 row(s) ==[0m
[31m  !1010101010	286	24        1010101010	1443.0	112.0[0m
[31m  !1010101010	143	12        1010101010	10563.0	448.0[0m
[31m  !1010101010	4329	336      1010101010	286.0	24.0[0m
[31m  !1010101010	1443	112      1010101010	10101.0	448.0[0m
[31m  !1010101010	2886	224      1010101010	143.0	12.0[0m
[31m  !1010101010	10563	448     1010101010	4329.0	336.0[0m
[31m  !1010101010	10101	448     1010101010	2886.0	224.0[0m
[31m  !0101010101	572	48        0101010101	572.0	48.0[0m
[31m  !0101010101	429	36        0101010101	429.0	36.0[0m
[31m  !0101010101	1078	48       0101010101	1078.0	48.0 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  T12
[32m- 0931[0m
Calcite parsing passed, start to transform. SELECT COL5,SUM(COL11),MAX(COL12) FROM T12 GROUP BY COL9,COL5,COL7,COL4,COL3,COL8 ORDER BY COL5 DESC
Calcite parsing passed, start to transform. SELECT COL5,SUM(COL11),MAX(COL12) FROM T12 GROUP BY COL9,COL5,COL7,COL4,COL3,COL8 ORDER BY COL5 DESC
Calcite parsing passed, start to transform. SELECT COL5,SUM(COL11),MAX(COL12) FROM T12 GROUP BY COL9,COL5,COL7,COL4,COL3,COL8 ORDER BY COL5 DESC
[31m  Results do not match for 0932:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['COL5 DESC], true[0m
[31m   'Aggregate ['COL9,'COL5,'COL7,'COL4,'COL3,'COL8], [unresolvedalias('COL5),unresolvedalias('SUM('COL11)),unresolvedalias('MAX('COL12))][0m
[31m    'UnresolvedRelation [T12], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  COL5: string, _c1: double, _c2: double[0m
[31m  Sort [COL5#119145 DESC], true[0m
[31m   Aggregate [COL9#119149,COL5#119145,COL7#119147,COL4#119144,COL3#119143,COL8#119148], [COL5#119145,sum(COL11#119151) AS _c1#119153,max(COL12#119152) AS _c2#119154][0m
[31m    MetastoreRelation HU, t12, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Sort [COL5#119145 DESC], true[0m
[31m   Aggregate [COL9#119149,COL5#119145,COL7#119147,COL4#119144,COL3#119143,COL8#119148], [COL5#119145,sum(COL11#119151) AS _c1#119153,max(COL12#119152) AS _c2#119154][0m
[31m    Project [COL4#119144,COL9#119149,COL8#119148,COL3#119143,COL11#119151,COL7#119147,COL12#119152,COL5#119145][0m
[31m     MetastoreRelation HU, t12, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenSort [COL5#119145 DESC], true, 0[0m
[31m   ConvertToUnsafe[0m
[31m    Exchange rangepartitioning(COL5#119145 DESC)[0m
[31m     ConvertToSafe[0m
[31m      TungstenAggregate(key=[COL9#119149,COL5#119145,COL7#119147,COL4#119144,COL3#119143,COL8#119148], functions=[(sum(COL11#119151),mode=Final,isDistinct=false),(max(COL12#119152),mode=Final,isDistinct=false)], output=[COL5#119145,_c1#119153,_c2#119154])[0m
[31m       TungstenExchange hashpartitioning(COL9#119149,COL5#119145,COL7#119147,COL4#119144,COL3#119143,COL8#119148)[0m
[31m        TungstenAggregate(key=[COL9#119149,COL5#119145,COL7#119147,COL4#119144,COL3#119143,COL8#119148], functions=[(sum(COL11#119151),mode=Partial,isDistinct=false),(max(COL12#119152),mode=Partial,isDistinct=false)], output=[COL9#119149,COL5#119145,COL7#119147,COL4#119144,COL3#119143,COL8#119148,currentSum#119159,max#119161])[0m
[31m         HiveTableScan [COL4#119144,COL9#119149,COL8#119148,COL3#119143,COL11#119151,COL7#119147,COL12#119152,COL5#119145], (MetastoreRelation HU, t12, None)[0m
  
[31m  Code Generation: true[0m
[31m  COL5	_c1	_c2[0m
[31m  !== HIVE - 10 row(s) ==   == CATALYST - 10 row(s) ==[0m
[31m  !88888889	10878	448       88888889	10878.0	448.0[0m
[31m  !88888888	1540	48         88888888	4329.0	336.0[0m
[31m  !88888888	4329	336        88888888	10434.0	448.0[0m
[31m  !88888888	1443	112        88888888	1540.0	48.0[0m
[31m  !88888888	2886	224        88888888	1443.0	112.0[0m
[31m  !88888888	10434	448       88888888	2886.0	224.0[0m
[31m  !88888884	143	12          88888884	143.0	12.0[0m
[31m  !88888883	286	24          88888883	286.0	24.0[0m
[31m  !88888882	429	36          88888882	429.0	36.0[0m
[31m  !88888881	572	48          88888881	572.0	48.0 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  T12
[32m- 0933[0m
Calcite parsing passed, start to transform. SELECT COL5,COL6,COL11,COL3,COL4,COL7,COL8 FROM T12 ORDER BY COL7,COL8,COL3,COL4,COL6,COL5 DESC
Calcite parsing passed, start to transform. SELECT COL5,COL6,COL11,COL3,COL4,COL7,COL8 FROM T12 ORDER BY COL7,COL8,COL3,COL4,COL6,COL5 DESC
Calcite parsing passed, start to transform. SELECT COL5,COL6,COL11,COL3,COL4,COL7,COL8 FROM T12 ORDER BY COL7,COL8,COL3,COL4,COL6,COL5 DESC
[31m  Results do not match for 0934:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['COL7 ASC,'COL8 ASC,'COL3 ASC,'COL4 ASC,'COL6 ASC,'COL5 DESC], true[0m
[31m   'Project [unresolvedalias('COL5),unresolvedalias('COL6),unresolvedalias('COL11),unresolvedalias('COL3),unresolvedalias('COL4),unresolvedalias('COL7),unresolvedalias('COL8)][0m
[31m    'UnresolvedRelation [T12], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  COL5: string, COL6: string, COL11: double, COL3: string, COL4: string, COL7: string, COL8: string[0m
[31m  Sort [COL7#119493 ASC,COL8#119494 ASC,COL3#119489 ASC,COL4#119490 ASC,COL6#119492 ASC,COL5#119491 DESC], true[0m
[31m   Project [COL5#119491,COL6#119492,COL11#119497,COL3#119489,COL4#119490,COL7#119493,COL8#119494][0m
[31m    MetastoreRelation HU, t12, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Sort [COL7#119493 ASC,COL8#119494 ASC,COL3#119489 ASC,COL4#119490 ASC,COL6#119492 ASC,COL5#119491 DESC], true[0m
[31m   Project [COL5#119491,COL6#119492,COL11#119497,COL3#119489,COL4#119490,COL7#119493,COL8#119494][0m
[31m    MetastoreRelation HU, t12, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenSort [COL7#119493 ASC,COL8#119494 ASC,COL3#119489 ASC,COL4#119490 ASC,COL6#119492 ASC,COL5#119491 DESC], true, 0[0m
[31m   ConvertToUnsafe[0m
[31m    Exchange rangepartitioning(COL7#119493 ASC,COL8#119494 ASC,COL3#119489 ASC,COL4#119490 ASC,COL6#119492 ASC,COL5#119491 DESC)[0m
[31m     HiveTableScan [COL5#119491,COL6#119492,COL11#119497,COL3#119489,COL4#119490,COL7#119493,COL8#119494], (MetastoreRelation HU, t12, None)[0m
  
[31m  Code Generation: true[0m
[31m  COL5	COL6	COL11	COL3	COL4	COL7	COL8[0m
[31m  !== HIVE - 220 row(s) ==                                                                   == CATALYST - 220 row(s) ==[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	44	4444	666666	2020...20	3030...30                                    88888888	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	0101010101	33	4444	666666	2020...20	3030...30                                    88888888	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888882	0101010101	33	4444	666666	2020...20	3030...30                                    88888882	0101010101	33.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888881	0101010101	44	4444	666666	2020...20	3030...30                                    88888881	0101010101	44.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	22	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	11	4444	666666	2020...20	3030...30                                    88888888	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888884	1010101010	11	4444	666666	2020...20	3030...30                                    88888884	1010101010	11.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888883	1010101010	22	4444	666666	2020...20	3030...30                                    88888883	1010101010	22.0	4444	666666	2020...20	3030...30[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	333	4441	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	333.0	4441	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4442	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4442	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4443	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4443	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	333	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	333.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888889	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888889	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	111	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	444.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	444	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	111.0	4444	666666	20202020202020202020	303030303030303030303030303030[0m
[31m  !88888888	1010101010	222	4444	666666	20202020202020202020	303030303030303030303030303030   88888888	1010101010	222.0	4444	666666	20202020202020202020	303030303030303030303030303030 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM  T12
[32m- 0935[0m
Calcite parsing passed, start to transform. SELECT COL3,COL11,COL9,COL8,COL7,COL5,COL4 FROM T12 ORDER BY COL9,COL8,COL7,COL5,COL4,COL3
Calcite parsing passed, start to transform. SELECT COL3,COL11,COL9,COL8,COL7,COL5,COL4 FROM T12 ORDER BY COL9,COL8,COL7,COL5,COL4,COL3
Calcite parsing passed, start to transform. SELECT COL3,COL11,COL9,COL8,COL7,COL5,COL4 FROM T12 ORDER BY COL9,COL8,COL7,COL5,COL4,COL3
[31m  Results do not match for 0936:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Sort ['COL9 ASC,'COL8 ASC,'COL7 ASC,'COL5 ASC,'COL4 ASC,'COL3 ASC], true[0m
[31m   'Project [unresolvedalias('COL3),unresolvedalias('COL11),unresolvedalias('COL9),unresolvedalias('COL8),unresolvedalias('COL7),unresolvedalias('COL5),unresolvedalias('COL4)][0m
[31m    'UnresolvedRelation [T12], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  COL3: string, COL11: double, COL9: string, COL8: string, COL7: string, COL5: string, COL4: string[0m
[31m  Sort [COL9#119808 ASC,COL8#119807 ASC,COL7#119806 ASC,COL5#119804 ASC,COL4#119803 ASC,COL3#119802 ASC], true[0m
[31m   Project [COL3#119802,COL11#119810,COL9#119808,COL8#119807,COL7#119806,COL5#119804,COL4#119803][0m
[31m    MetastoreRelation HU, t12, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Sort [COL9#119808 ASC,COL8#119807 ASC,COL7#119806 ASC,COL5#119804 ASC,COL4#119803 ASC,COL3#119802 ASC], true[0m
[31m   Project [COL3#119802,COL11#119810,COL9#119808,COL8#119807,COL7#119806,COL5#119804,COL4#119803][0m
[31m    MetastoreRelation HU, t12, None[0m
  
[31m  == Physical Plan ==[0m
[31m  TungstenSort [COL9#119808 ASC,COL8#119807 ASC,COL7#119806 ASC,COL5#119804 ASC,COL4#119803 ASC,COL3#119802 ASC], true, 0[0m
[31m   ConvertToUnsafe[0m
[31m    Exchange rangepartitioning(COL9#119808 ASC,COL8#119807 ASC,COL7#119806 ASC,COL5#119804 ASC,COL4#119803 ASC,COL3#119802 ASC)[0m
[31m     HiveTableScan [COL3#119802,COL11#119810,COL9#119808,COL8#119807,COL7#119806,COL5#119804,COL4#119803], (MetastoreRelation HU, t12, None)[0m
  
[31m  Code Generation: true[0m
[31m  COL3	COL11	COL9	COL8	COL7	COL5	COL4[0m
[31m  !== HIVE - 224 row(s) ==                                                                                                 == CATALYST - 224 row(s) ==[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888881	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888881	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888882	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888882	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888883	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888883	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888884	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888884	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	44	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	44.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	11	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	11.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	22	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	22.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4444	33	4040...40	3030...30	2020...20	88888888	666666                                                                   4444	33.0	4040...40	3030...30	2020...20	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4441	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4441	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4442	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4442	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4443	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4443	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	111.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	222	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	222.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	111	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888888	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	444	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	333.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666[0m
[31m  !4444	333	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666   4444	444.0	4040404040404040404040404040404040404040	303030303030303030303030303030	20202020202020202020	88888889	666666 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT * FROM T240
[32m- 0937[0m
Calcite parsing passed, start to transform. SELECT STAFF.EMPNUM,PROJ.PNUM,WORKS.HOURS, STAFF3.GRADE,STAFF4.CITY,WORKS1.HOURS, TEMP_S.GRADE,PROJ1.PNUM,STAFF1.GRADE, UPUNIQ.COL2 FROM   STAFF,PROJ,WORKS,STAFF3,STAFF4,WORKS1, TEMP_S,PROJ1,STAFF1,UPUNIQ WHERE  STAFF.EMPNUM = WORKS.EMPNUM    AND PROJ.PNUM = WORKS.PNUM         AND STAFF3.EMPNUM = WORKS.EMPNUM   AND STAFF4.EMPNUM = WORKS.EMPNUM   AND WORKS1.EMPNUM = WORKS.EMPNUM   AND WORKS1.PNUM = WORKS.PNUM       AND TEMP_S.EMPNUM = WORKS.EMPNUM   AND PROJ1.PNUM = WORKS.PNUM        AND STAFF1.EMPNUM = WORKS.EMPNUM   AND UPUNIQ.COL2 = 'A' ORDER BY 1, 2
[32m- 0938[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, EMPNAME FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM  FROM WORKS WHERE PNUM IN (SELECT PNUM  FROM PROJ WHERE PTYPE IN (SELECT PTYPE  FROM PROJ WHERE PNUM IN (SELECT PNUM  FROM WORKS WHERE EMPNUM IN (SELECT EMPNUM  FROM WORKS WHERE PNUM IN (SELECT PNUM   FROM PROJ WHERE PTYPE IN (SELECT PTYPE  FROM PROJ WHERE CITY IN (SELECT CITY  FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM  FROM WORKS WHERE HOURS = 20 AND PNUM = 'P2' )))))))))
calcite cannot do SELECT EMPNUM, EMPNAME FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM  FROM WORKS WHERE PNUM IN (SELECT PNUM  FROM PROJ WHERE PTYPE IN (SELECT PTYPE  FROM PROJ WHERE PNUM IN (SELECT PNUM  FROM WORKS WHERE EMPNUM IN (SELECT EMPNUM  FROM WORKS WHERE PNUM IN (SELECT PNUM   FROM PROJ WHERE PTYPE IN (SELECT PTYPE  FROM PROJ WHERE CITY IN (SELECT CITY  FROM STAFF WHERE EMPNUM IN (SELECT EMPNUM  FROM WORKS WHERE HOURS = 20 AND PNUM = 'P2' )))))))))
[31m- 0939 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HH WHERE SMALLTEST = 9999
[32m- 0940[0m
Calcite parsing passed, start to transform. SELECT SMALLTEST FROM HH WHERE SMALLTEST = -9999
[32m- 0941[0m
Calcite parsing passed, start to transform. SELECT INTTEST FROM EE WHERE INTTEST = 999999999
[32m- 0942[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM EE WHERE INTTEST = -999999999
[32m- 0943[0m
Calcite parsing passed, start to transform. SELECT NUMTEST FROM PP_15
[32m- 0944[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PP_15 WHERE NUMTEST = 0.123456789012345
[32m- 0945[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM PP_15 WHERE NUMTEST = -0.912345678901234
[32m- 0946[0m
Calcite parsing passed, start to transform. SELECT C6, C16, C26, C36, C46, C56, C66, C76, C86, C96, C100 FROM T100 WHERE C1 = 'ZA' AND C2 = 'ZB'
[32m- 0947[0m
Calcite parsing passed, start to transform. SELECT C5, C20, C35, C40, C55, C60, C75, C80, C90, C95, C100 FROM T100 WHERE C1 = 'ZA' AND C2 = 'ZB'
[32m- 0948[0m
Calcite parsing passed, start to transform. SELECT FLOATTEST FROM JJ
[32m- 0949[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JJ WHERE FLOATTEST > 0.1048574 AND FLOATTEST < 0.1048576
[32m- 0950[0m
Calcite parsing passed, start to transform. SELECT FLOATTEST FROM JJ
[32m- 0951[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM JJ WHERE FLOATTEST > -0.1048576 AND FLOATTEST < -0.1048574
[32m- 0952[0m
Calcite parsing passed, start to transform. SELECT REALTEST FROM GG
[32m- 0953[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM GG WHERE REALTEST > 0.1048574 AND REALTEST < 0.1048576
[32m- 0954[0m
Calcite parsing passed, start to transform. SELECT REALTEST FROM GG
[32m- 0955[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM GG WHERE REALTEST > -0.1048576 AND REALTEST < -0.1048574
[32m- 0956[0m
Calcite parsing passed, start to transform. SELECT DOUBLETEST FROM II
[32m- 0957[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM II WHERE DOUBLETEST > 0.1073741822 AND DOUBLETEST < 0.1073741824
[32m- 0958[0m
Calcite parsing passed, start to transform. SELECT DOUBLETEST FROM II
[32m- 0959[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM II WHERE DOUBLETEST > -0.1073741824 AND DOUBLETEST < -0.1073741822
[32m- 0960[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_P WHERE S1 = 'E1'
[32m- 0961[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_F WHERE F6 = 'RRS'
[32m- 0962[0m
Calcite parsing passed, start to transform. SELECT S1 FROM SIZ1_P WHERE S1 = 'E1' AND S2 = 'TTT'
[32m- 0963[0m
Calcite parsing passed, start to transform. SELECT S1 FROM SIZ1_P WHERE S1 = 'E9'
[32m- 0964[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_P WHERE S1 = 'E1' AND S2 = 'TTS' AND S3 =1
[32m- 0965[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_P WHERE S1 = 'E1' AND S2 = 'TTS' AND S3 = 1
[32m- 0966[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_P WHERE S1 = 'E1' AND S2 = 'TTT' AND S3 = 1
[32m- 0967[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_F WHERE F1 = 'E2'
[32m- 0968[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ1_F WHERE F1 = 'E1'
[32m- 0969[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F10
[32m- 0970[0m
Calcite parsing passed, start to transform. SELECT P1 FROM SIZ2_P WHERE P1 = '  A'
[32m- 0971[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_P WHERE P1 = '  A'
[32m- 0972[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_P WHERE P1 = '  A'
[32m- 0973[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_P WHERE P1 = '  A'
[32m- 0974[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ2_F1 WHERE F1 = '  A'
[32m- 0975[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P5 WHERE F1 = 11
[32m- 0976[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P5 WHERE F1 = 11
[32m- 0977[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_F,SIZ3_P1,SIZ3_P2,SIZ3_P3,SIZ3_P4, SIZ3_P5,SIZ3_P6 WHERE P1 = SIZ3_P1.F1 AND P2 = SIZ3_P2.F1 AND P3 = SIZ3_P3.F1 AND P4 = SIZ3_P4.F1 AND P5 = SIZ3_P5.F1 AND P6 = SIZ3_P6.F1 AND SIZ3_P3.F1 BETWEEN 1 AND 2
[32m- 0978[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_F
[32m- 0979[0m
Calcite parsing passed, start to transform. SELECT F1 FROM SIZ3_P1 WHERE F1 = '  A'
[32m- 0980[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P1 WHERE F1 = '  A'
[32m- 0981[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P1 WHERE F1 = '  A'
[32m- 0982[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_P1 WHERE F1 = '  A'
[32m- 0983[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM SIZ3_F WHERE P1 = '  A'
[32m- 0984[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM UUSIG
[32m- 0985[0m
Calcite parsing passed, start to transform. SELECT COUNT(DISTINCT U1) FROM UUSIG
[32m- 0986[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM UUSIG WHERE U1 < 0 OR U1 > 3 OR U1 IS NULL
[32m- 0987[0m
Calcite parsing passed, start to transform. SELECT * FROM ABOVE_AVERAGE ORDER BY COLUMN_1
[32m- 0988[0m
Calcite parsing passed, start to transform. SELECT * FROM STAFF_DUP ORDER BY CITY
[32m- 0989[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM STAFF_DUP
[32m- 0990[0m
Calcite parsing passed, start to transform. SELECT C2, C1, C3 FROM FOUR_CITIES ORDER BY C3, C2
[32m- 0991[0m
Calcite parsing passed, start to transform. SELECT COUNT (*) FROM FOUR_CITIES
[32m- 0992[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FOUR_CITIES WHERE C3 > 0
[32m- 0993[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM FOUR_CITIES WHERE C2 = 'Vienna'
[32m- 0994[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CONTACTS WHERE DESCRIPTION = 'Harry works in the Redundancy Automation Division of the ' || 'Materials ' || 'Blasting Laboratory in the National Cattle Acceleration ' || 'Project of ' || 'lower Michigan.  His job is to document the trajectory of ' || 'cattle and ' || 'correlate the loft and acceleration versus the quality of ' || 'materials ' || 'used in the trebuchet.  He served ten years as the ' || 'vice-president in ' || 'charge of marketing in the now defunct milk trust of the ' || 'Pennsylvania ' || 'Coalition of All Things Bovine.  Prior to that he ' || 'established himself ' || 'as a world-class graffiti artist and source of all good ' || 'bits related ' || 'to channel dredging in poor weather.  He is author of over ' || 'ten thousand ' || 'paperback novels, including such titles as "How Many ' || 'Pumpkins will Fit ' || 'on the Head of a Pin," "A Whole Bunch of Useless Things ' || 'that you Don''t ' || 'Want to Know," and "How to Lift Heavy Things Over your ' || 'Head without ' || 'Hurting Yourself or Dropping Them."  He attends ANSI and ' || 'ISO standards ' || 'meetings in his copious free time and funds the development ' || 'of test ' || 'suites with his pocket change.' AND KEYWORDS = 'aardvark albatross nutmeg redundancy ' || 'automation materials blasting ' || 'cattle acceleration trebuchet catapult ' || 'loft coffee java sendmail SMTP ' || 'FTP HTTP censorship expletive senility ' || 'extortion distortion conformity ' || 'conformance nachos chicks goslings ' || 'ducklings honk quack melatonin tie ' || 'noose circulation column default ' || 'ionic doric chlorine guanine Guam ' || 'invasions rubicon helmet plastics ' || 'recycle HDPE nylon ceramics plumbing ' || 'parachute zeppelin carbon hydrogen ' || 'vinegar sludge asphalt adhesives ' || 'tensile magnetic Ellesmere Greenland ' || 'Knud Rasmussen precession ' || 'navigation positioning orbit altitude ' || 'resistance radiation levitation ' || 'yoga demiurge election violence ' || 'collapsed fusion cryogenics gravity ' || 'sincerity idiocy budget accounting ' || 'auditing titanium torque pressure ' || 'fragile hernia muffler cartilage ' || 'graphics deblurring headache eyestrain ' || 'interlace bandwidth resolution ' || 'determination steroids barrel oak wine ' || 'ferment yeast brewing bock siphon ' || 'clarity impurities SQL RBAC data ' || 'warehouse security integrity feedback'
[32m- 0995[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CONTACTS WHERE DESCRIPTION LIKE '%change.' AND KEYWORDS LIKE '%feedback'
[32m- 0996[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CONTACTS WHERE DESCRIPTION = 'Harry works in the Redundancy Automation Division of the ' || 'Materials ' || 'Blasting Laboratory in the National Cattle Acceleration ' || 'Project of ' || 'lower Michigan.  His job is to document the trajectory of ' || 'cattle and ' || 'correlate the loft and acceleration versus the quality of ' || 'materials ' || 'used in the trebuchet.  He served ten years as the ' || 'vice-president in ' || 'charge of marketing in the now defunct milk trust of the ' || 'Pennsylvania ' || 'Coalition of All Things Bovine.  Prior to that he ' || 'established himself ' || 'as a world-class graffiti artist and source of all good ' || 'bits related ' || 'to channel dredging in poor weather.  He is author of over ' || 'ten thousand ' || 'paperback novels, including such titles as "How Many ' || 'Pumpkins will Fit ' || 'on the Head of a Pin," "A Whole Bunch of Useless Things ' || 'that you Don''t ' || 'Want to Know," and "How to Lift Heavy Things Over your ' || 'Head without ' || 'Hurting Yourself or Dropping Them."  He attends ANSI and ' || 'ISO standards ' || 'meetings in his copious free time and funds the development ' || 'of test ' || 'suites with his pocket change.' AND KEYWORDS = 'aardvark albatross nutmeg redundancy ' || 'automation materials blasting ' || 'cattle acceleration trebuchet catapult ' || 'loft coffee java sendmail SMTP ' || 'FTP HTTP censorship expletive senility ' || 'extortion distortion conformity ' || 'conformance nachos chicks goslings ' || 'ducklings honk quack melatonin tie ' || 'noose circulation column default ' || 'ionic doric chlorine guanine Guam ' || 'invasions rubicon helmet plastics ' || 'recycle HDPE nylon ceramics plumbing ' || 'parachute zeppelin carbon hydrogen ' || 'vinegar sludge asphalt adhesives ' || 'tensile magnetic Ellesmere Greenland ' || 'Knud Rasmussen precession ' || 'navigation positioning orbit altitude ' || 'resistance radiation levitation ' || 'yoga demiurge election violence ' || 'collapsed fusion cryogenics gravity ' || 'sincerity idiocy budget accounting ' || 'auditing titanium torque pressure ' || 'fragile hernia muffler cartilage ' || 'graphics deblurring headache eyestrain ' || 'interlace bandwidth resolution ' || 'determination steroids barrel oak wine ' || 'ferment yeast brewing bock siphon ' || 'clarity impurities SQL RBAC data ' || 'warehouse security integrity feedback'
[32m- 0997[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CONTACTS WHERE DESCRIPTION LIKE '%change.' AND KEYWORDS LIKE '%feedback'
[32m- 0998[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CONTACTS WHERE DESCRIPTION = N'Harry works in the Redundancy Automation Division of the ' || 'Materials ' || 'Blasting Laboratory in the National Cattle Acceleration ' || 'Project of ' || 'lower Michigan.  His job is to document the trajectory of ' || 'cattle and ' || 'correlate the loft and acceleration versus the quality of ' || 'materials ' || 'used in the trebuchet.  He served ten years as the ' || 'vice-president in ' || 'charge of marketing in the now defunct milk trust of the ' || 'Pennsylvania ' || 'Coalition of All Things Bovine.  Prior to that he ' || 'established himself ' || 'as a world-class gra' AND KEYWORDS = N'aardvark albatross nutmeg redundancy ' || 'automation materials blasting ' || 'cattle acceleration trebuchet catapult ' || 'loft coffee java sendmail SMTP ' || 'FTP HTTP censorship expletive senility ' || 'extortion distortion conformity ' || 'conformance nachos chicks goslings ' || 'ducklings honk quack melatonin tie ' || 'noose circulation column default ' || 'ionic doric chlorine guanine Guam ' || 'invasions rubicon helmet plastics ' || 'recycle HDPE nylon ceramics plumbing ' || 'parachute zeppelin carbon hydrogen ' || 'vinegar sludge asphalt adhesives ' || 'tensile magnetic'
[32m- 0999[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM CONTACTS WHERE DESCRIPTION = N'Harry works in the Redundancy Automation Division of the ' || 'Materials ' || 'Blasting Laboratory in the National Cattle Acceleration ' || 'Project of ' || 'lower Michigan.  His job is to document the trajectory of ' || 'cattle and ' || 'correlate the loft and acceleration versus the quality of ' || 'materials ' || 'used in the trebuchet.  He served ten years as the ' || 'vice-president in ' || 'charge of marketing in the now defunct milk trust of the ' || 'Pennsylvania ' || 'Coalition of All Things Bovine.  Prior to that he ' || 'established himself ' || 'as a world-class gra' AND KEYWORDS = N'aardvark albatross nutmeg redundancy ' || 'automation materials blasting ' || 'cattle acceleration trebuchet catapult ' || 'loft coffee java sendmail SMTP ' || 'FTP HTTP censorship expletive senility ' || 'extortion distortion conformity ' || 'conformance nachos chicks goslings ' || 'ducklings honk quack melatonin tie ' || 'noose circulation column default ' || 'ionic doric chlorine guanine Guam ' || 'invasions rubicon helmet plastics ' || 'recycle HDPE nylon ceramics plumbing ' || 'parachute zeppelin carbon hydrogen ' || 'vinegar sludge asphalt adhesives ' || 'tensile magnetic'
[32m- 1001[0m
Calcite parsing passed, start to transform. SELECT C1 FROM NOMAIL WHERE C1 > 0
Calcite parsing passed, start to transform. SELECT C1 FROM NOMAIL WHERE C1 > 0
Calcite parsing passed, start to transform. SELECT C1 FROM NOMAIL WHERE C1 > 0
[31m  Results do not match for 1003:[0m
[31m  == Parsed Logical Plan ==[0m
[31m  'Project [unresolvedalias('C1)][0m
[31m   'Filter ('C1 > 0)[0m
[31m    'UnresolvedRelation [NOMAIL], None[0m
  
[31m  == Analyzed Logical Plan ==[0m
[31m  C1: double[0m
[31m  Project [C1#126664][0m
[31m   Filter (C1#126664 > cast(0 as double))[0m
[31m    MetastoreRelation FLATER, nomail, None[0m
  
[31m  == Optimized Logical Plan ==[0m
[31m  Project [C1#126664][0m
[31m   Filter (C1#126664 > 0.0)[0m
[31m    MetastoreRelation FLATER, nomail, None[0m
  
[31m  == Physical Plan ==[0m
[31m  Filter (C1#126664 > 0.0)[0m
[31m   HiveTableScan [C1#126664], (MetastoreRelation FLATER, nomail, None)[0m
  
[31m  Code Generation: true[0m
[31m  C1[0m
[31m  !== HIVE - 22 row(s) ==   == CATALYST - 22 row(s) ==[0m
[31m  !2147483634               2.147483634E9[0m
[31m  !2147483635               2.147483635E9[0m
[31m  !2147483636               2.147483636E9[0m
[31m  !2147483636               2.147483636E9[0m
[31m  !2147483637               2.147483637E9[0m
[31m  !2147483637               2.147483637E9[0m
[31m  !2147483638               2.147483638E9[0m
[31m  !2147483638               2.147483638E9[0m
[31m  !2147483639               2.147483639E9[0m
[31m  !2147483639               2.147483639E9[0m
[31m  !2147483640               2.147483641E9[0m
[31m  !2147483640               2.147483641E9[0m
[31m  !2147483641               2.147483642E9[0m
[31m  !2147483641               2.147483642E9[0m
[31m  !2147483642               2.147483643E9[0m
[31m  !2147483642               2.147483643E9[0m
[31m  !2147483643               2.147483644E9[0m
[31m  !2147483643               2.147483645E9[0m
[31m  !2147483644               2.147483646E9[0m
[31m  !2147483645               2.147483647E9[0m
[31m  !2147483646               2.14748364E9[0m
[31m  !2147483647               2.14748364E9 (HiveCompSuite.scala:3159)[0m
Calcite parsing passed, start to transform. SELECT C1 FROM NOMAIL WHERE C1 < 0
[32m- 1004[0m
Calcite parsing passed, start to transform. SELECT C1 - 2147483646 FROM NOMAIL WHERE C1 > 0
[32m- 1005[0m
Calcite parsing passed, start to transform. SELECT C1 + 2147483646 FROM NOMAIL WHERE C1 < 0
[32m- 1006[0m
Calcite parsing passed, start to transform. SELECT C1 FROM NOMAIL WHERE C1 < 0
[32m- 1007[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NOMAIL WHERE C1 = 2147483645
[32m- 1008[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NOMAIL WHERE C1 = 2147483646
[32m- 1009[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM NOMAIL WHERE C1 = 2147483647
[32m- 1010[0m
Calcite parsing passed, start to transform. SELECT C1 FROM YESMAIL WHERE C1 > 0
[32m- 1011[0m
Calcite parsing passed, start to transform. SELECT C1 FROM YESMAIL WHERE C1 < 0
[32m- 1012[0m
Calcite parsing passed, start to transform. SELECT C1 - 32766 FROM YESMAIL WHERE C1 > 0
[32m- 1013[0m
Calcite parsing passed, start to transform. SELECT C1 + 32766 FROM YESMAIL WHERE C1 < 0
[32m- 1014[0m
Calcite parsing passed, start to transform. SELECT C1 FROM YESMAIL WHERE C1 < 0
[32m- 1015[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM YESMAIL WHERE C1 = 32765
[32m- 1016[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM YESMAIL WHERE C1 = 32766
[32m- 1017[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM YESMAIL WHERE C1 = 32767
[32m- 1018[0m
Calcite parsing passed, start to transform. SELECT C1, C2, C3, C4, C5, C6, C7, C8, C9, C10, C11, C12, C13, C14, C15, C16, C17, C18, C19, C20, C21, C22, C23, C24, C25, C26, C27, C28, C29, C30, C31, C32, C33, C34, C35, C36, C37, C38, C39, C40, C41, C42, C43, C44, C45, C46, C47, C48, C49, C50, C51, C52, C53, C54, C55, C56, C57, C58, C59, C60, C61, C62, C63, C64, C65, C66, C67, C68, C69, C70, C71, C72, C73, C74, C75, C76, C77, C78, C79, C80, C81, C82, C83, C84, C85, C86, C87, C88, C89, C90, C91, C92, C93, C94, C95, C96, C97, C98, C99, C100, C101, C102, C103, C104, C105, C106, C107, C108, C109, C110, C111, C112, C113, C114, C115, C116, C117, C118, C119, C120, C121, C122, C123, C124, C125, C126, C127, C128, C129, C130, C131, C132, C133, C134, C135, C136, C137, C138, C139, C140, C141, C142, C143, C144, C145, C146, C147, C148, C149, C150, C151, C152, C153, C154, C155, C156, C157, C158, C159, C160, C161, C162, C163, C164, C165, C166, C167, C168, C169, C170, C171, C172, C173, C174, C175, C176, C177, C178, C179, C180, C181, C182, C183, C184, C185, C186, C187, C188, C189, C190, C191, C192, C193, C194, C195, C196, C197, C198, C199, C200, C201, C202, C203, C204, C205, C206, C207, C208, C209, C210, C211, C212, C213, C214, C215, C216, C217, C218, C219, C220, C221, C222, C223, C224, C225, C226, C227, C228, C229, C230, C231, C232, C233, C234, C235, C236, C237, C238, C239, C240, C241, C242, C243, C244, C245, C246, C247, C248, C249, C250 FROM L1
[32m- 1019[0m
Calcite parsing passed, start to transform. SELECT FIRSTNAME, LASTNAME, AVG(COST) FROM PEOPLE_ORDERS GROUP BY LASTNAME, FIRSTNAME ORDER BY LASTNAME, FIRSTNAME
[32m- 1020[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, CASE GRADE WHEN 0 THEN 1000 WHEN 1 THEN 997 WHEN 2 THEN 994 WHEN 3 THEN 991 WHEN 4 THEN 988 WHEN 5 THEN 985 WHEN 6 THEN 982 WHEN 7 THEN 979 WHEN 8 THEN 976 WHEN 9 THEN 973 WHEN 10 THEN 970 WHEN 11 THEN 967 WHEN 12 THEN 964 WHEN 13 THEN 961 WHEN 14 THEN 958 WHEN 15 THEN 955 WHEN 16 THEN 952 WHEN 17 THEN 949 WHEN 18 THEN 946 WHEN 19 THEN 943 WHEN 20 THEN 940 WHEN 21 THEN 937 WHEN 22 THEN 934 WHEN 23 THEN 931 WHEN 24 THEN 928 WHEN 25 THEN 925 WHEN 26 THEN 922 WHEN 27 THEN 919 WHEN 28 THEN 916 WHEN 29 THEN 913 WHEN 30 THEN 910 WHEN 31 THEN 907 WHEN 32 THEN 904 WHEN 33 THEN 901 WHEN 34 THEN 898 WHEN 35 THEN 895 WHEN 36 THEN 892 WHEN 37 THEN 889 WHEN 38 THEN 886 WHEN 39 THEN 883 WHEN 40 THEN 880 WHEN 41 THEN 877 WHEN 42 THEN 874 WHEN 43 THEN 871 WHEN 44 THEN 868 WHEN 45 THEN 865 WHEN 46 THEN 862 WHEN 47 THEN 859 WHEN 48 THEN 856 WHEN 49 THEN 853 END FROM HU.STAFF WHERE EMPNAME = 'Betty'
[32m- 1021[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, CASE WHEN GRADE = 0 THEN 1000 WHEN GRADE = 1 THEN 997 WHEN GRADE = 2 THEN 994 WHEN GRADE = 3 THEN 991 WHEN GRADE = 4 THEN 988 WHEN GRADE = 5 THEN 985 WHEN GRADE = 6 THEN 982 WHEN GRADE = 7 THEN 979 WHEN GRADE = 8 THEN 976 WHEN GRADE = 9 THEN 973 WHEN GRADE = 11 THEN 967 WHEN GRADE = 12 THEN 964 WHEN GRADE = 13 THEN 961 WHEN GRADE = 14 THEN 958 WHEN GRADE = 15 THEN 955 WHEN GRADE = 16 THEN 952 WHEN GRADE = 17 THEN 949 WHEN GRADE = 18 THEN 946 WHEN GRADE = 19 THEN 943 WHEN GRADE = 20 THEN 940 WHEN GRADE = 21 THEN 937 WHEN GRADE = 22 THEN 934 WHEN GRADE = 23 THEN 931 WHEN GRADE = 24 THEN 928 WHEN GRADE = 25 THEN 925 WHEN GRADE = 26 THEN 922 WHEN GRADE = 27 THEN 919 WHEN GRADE = 28 THEN 916 WHEN GRADE = 29 THEN 913 WHEN GRADE = 30 THEN 910 WHEN GRADE = 31 THEN 907 WHEN GRADE = 32 THEN 904 WHEN GRADE = 33 THEN 901 WHEN GRADE = 34 THEN 898 WHEN GRADE = 35 THEN 895 WHEN GRADE = 36 THEN 892 WHEN GRADE = 37 THEN 889 WHEN GRADE = 38 THEN 886 WHEN GRADE = 39 THEN 883 WHEN GRADE = 40 THEN 880 WHEN GRADE = 41 THEN 877 WHEN GRADE = 42 THEN 874 WHEN GRADE = 43 THEN 871 WHEN GRADE = 44 THEN 868 WHEN GRADE = 45 THEN 865 WHEN GRADE = 46 THEN 862 WHEN GRADE = 47 THEN 859 WHEN GRADE = 48 THEN 856 WHEN GRADE = 49 THEN 853 WHEN GRADE = 10 THEN 970 END FROM HU.STAFF WHERE EMPNAME = 'Betty'
[32m- 1022[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, CITY FROM STAFF UNION SELECT PTYPE, CITY FROM PROJ
[32m- 1023[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, CITY FROM STAFF UNION SELECT 'e1', CITY FROM PROJ
[32m- 1024[0m
Calcite parsing passed, start to transform. SELECT EMPNUM, GRADE FROM STAFF UNION SELECT EMPNUM, HOURS FROM WORKS
[32m- 1025[0m
Calcite parsing passed, start to transform. SELECT COL2                 FROM  TABLEFGHIJKLMNOPQ19
[32m- 1026[0m
Calcite parsing passed, start to transform. SELECT COLUMN123456789IS19  FROM  SHORTTABLE
[32m- 1027[0m
Calcite parsing passed, start to transform. SELECT COL3                 FROM  VIEWABCDEFGHIKLMN19
[32m- 1028[0m
Calcite parsing passed, start to transform. SELECT * FROM AA ORDER BY CHARTEST
[32m- 1029[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM AA
[32m- 1030[0m
Calcite parsing passed, start to transform. SELECT COL2 FROM HU.UPUNIQ WHERE NUMKEY = 1
[32m- 1031[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < (SELECT MAX(HOURS) FROM HU.WORKS) OR    GRADE > (SELECT MAX(NUMKEY) FROM HU.UPUNIQ) OR    GRADE + 100 > (SELECT MIN(HOURS) FROM HU.WORKS)
calcite cannot do SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < (SELECT MAX(HOURS) FROM HU.WORKS) OR    GRADE > (SELECT MAX(NUMKEY) FROM HU.UPUNIQ) OR    GRADE + 100 > (SELECT MIN(HOURS) FROM HU.WORKS)
[31m- 1032 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COL2 FROM HU.UPUNIQ WHERE NUMKEY = 1
[32m- 1033[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < (SELECT MAX(HOURS) FROM HU.WORKS) OR    GRADE > (SELECT MAX(NUMKEY) FROM HU.UPUNIQ) OR    GRADE + 100 > (SELECT MIN(HOURS) FROM HU.WORKS)
calcite cannot do SELECT COUNT(*) FROM HU.STAFF WHERE GRADE < (SELECT MAX(HOURS) FROM HU.WORKS) OR    GRADE > (SELECT MAX(NUMKEY) FROM HU.UPUNIQ) OR    GRADE + 100 > (SELECT MIN(HOURS) FROM HU.WORKS)
[31m- 1034 *** FAILED ***[0m
[31m  java.lang.RuntimeException: Parse failed.[0m
[31m  at scala.sys.package$.error(package.scala:27)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:33)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect$$anonfun$parse$1.apply(CalciteDialect.scala:32)[0m
[31m  at scala.Option.getOrElse(Option.scala:120)[0m
[31m  at com.intel.ssg.bdt.spark.sql.CalciteDialect.parse(CalciteDialect.scala:31)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:169)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:115)[0m
[31m  at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)[0m
[31m  at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)[0m
[31m  ...[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM C_TRANSACTION WHERE COMMOD_NO = 17
[32m- 1035[0m
Calcite parsing passed, start to transform. SELECT UNIT_PRICE, FROM_DATE, TO_DATE, COMMODITY FROM DOLLARS_PER_POUND ORDER BY COMMODITY DESC
[32m- 1036[0m
Calcite parsing passed, start to transform. SELECT COUNT(*) FROM COST_PER_UNIT
[32m- 1037[0m
Calcite parsing passed, start to transform. SELECT CURRENCY, MEASURE, UNIT_PRICE, COMMODITY FROM COST_PER_UNIT
[32m- 1038[0m
Calcite parsing passed, start to transform. SELECT (100 + 7) * UNIT_PRICE * 700 / 100, COMMODITY FROM DOLLARS_PER_POUND ORDER BY COMMODITY
[32m- 1039[0m
[33m- 1051 !!! IGNORED !!![0m
[33m- 1052 !!! IGNORED !!![0m
[33m- 1053 !!! IGNORED !!![0m
[33m- 1054 !!! IGNORED !!![0m
[33m- 1055 !!! IGNORED !!![0m
[33m- 1056 !!! IGNORED !!![0m
[33m- 1057 !!! IGNORED !!![0m
[33m- 1058 !!! IGNORED !!![0m
[33m- 1059 !!! IGNORED !!![0m
[33m- 1060 !!! IGNORED !!![0m
[33m- 1061 !!! IGNORED !!![0m
[33m- 1062 !!! IGNORED !!![0m
[33m- 1063 !!! IGNORED !!![0m
[33m- 1064 !!! IGNORED !!![0m
[33m- 1065 !!! IGNORED !!![0m
[33m- 1066 !!! IGNORED !!![0m
[33m- 1067 !!! IGNORED !!![0m
[33m- 1068 !!! IGNORED !!![0m
[33m- 1069 !!! IGNORED !!![0m
[33m- 1070 !!! IGNORED !!![0m
[33m- 1071 !!! IGNORED !!![0m
[33m- 1072 !!! IGNORED !!![0m
[33m- 2030 !!! IGNORED !!![0m
[33m- 2031 !!! IGNORED !!![0m
[33m- 2032 !!! IGNORED !!![0m
[33m- 2033 !!! IGNORED !!![0m
[33m- 2034 !!! IGNORED !!![0m
[33m- 2035 !!! IGNORED !!![0m
[33m- 2036 !!! IGNORED !!![0m
[33m- 2037 !!! IGNORED !!![0m
[33m- 2038 !!! IGNORED !!![0m
[33m- 2039 !!! IGNORED !!![0m
[33m- 5000 !!! IGNORED !!![0m
[33m- 5001 !!! IGNORED !!![0m
[33m- 5002 !!! IGNORED !!![0m
[33m- 5003 !!! IGNORED !!![0m
[33m- 5004 !!! IGNORED !!![0m
[33m- 5005 !!! IGNORED !!![0m
[33m- 5006 !!! IGNORED !!![0m
[33m- 5007 !!! IGNORED !!![0m
[33m- 5008 !!! IGNORED !!![0m
[33m- 5009 !!! IGNORED !!![0m
[33m- 5010 !!! IGNORED !!![0m
[33m- 5011 !!! IGNORED !!![0m
[33m- 5012 !!! IGNORED !!![0m
[33m- 5013 !!! IGNORED !!![0m
[33m- 5014 !!! IGNORED !!![0m
[33m- 5015 !!! IGNORED !!![0m
[33m- 5016 !!! IGNORED !!![0m
[33m- 5017 !!! IGNORED !!![0m
[33m- 5018 !!! IGNORED !!![0m
[33m- 5019 !!! IGNORED !!![0m
[33m- 5020 !!! IGNORED !!![0m
[33m- 5021 !!! IGNORED !!![0m
[33m- 5022 !!! IGNORED !!![0m
[33m- 5023 !!! IGNORED !!![0m
[33m- 5024 !!! IGNORED !!![0m
[33m- 5025 !!! IGNORED !!![0m
[33m- 5026 !!! IGNORED !!![0m
[33m- 5027 !!! IGNORED !!![0m
[33m- 5028 !!! IGNORED !!![0m
[33m- 5029 !!! IGNORED !!![0m
[33m- 5030 !!! IGNORED !!![0m
[33m- 5031 !!! IGNORED !!![0m
[33m- 5032 !!! IGNORED !!![0m
[33m- 5033 !!! IGNORED !!![0m
[33m- 5034 !!! IGNORED !!![0m
[33m- 5035 !!! IGNORED !!![0m
[33m- 5036 !!! IGNORED !!![0m
[33m- 5037 !!! IGNORED !!![0m
[33m- 5038 !!! IGNORED !!![0m
[33m- 5039 !!! IGNORED !!![0m
[33m- 5040 !!! IGNORED !!![0m
[33m- 5041 !!! IGNORED !!![0m
[33m- 5042 !!! IGNORED !!![0m
[33m- 5043 !!! IGNORED !!![0m
[33m- 5044 !!! IGNORED !!![0m
[33m- 5045 !!! IGNORED !!![0m
[33m- 5046 !!! IGNORED !!![0m
[33m- 5047 !!! IGNORED !!![0m
[33m- 5048 !!! IGNORED !!![0m
[33m- 5049 !!! IGNORED !!![0m
[33m- 5050 !!! IGNORED !!![0m
[33m- 5051 !!! IGNORED !!![0m
[33m- 5052 !!! IGNORED !!![0m
[33m- 5053 !!! IGNORED !!![0m
[33m- 5054 !!! IGNORED !!![0m
[33m- 5055 !!! IGNORED !!![0m
[33m- 5056 !!! IGNORED !!![0m
[33m- 5057 !!! IGNORED !!![0m
[33m- 5058 !!! IGNORED !!![0m
[33m- 5059 !!! IGNORED !!![0m
[33m- 5060 !!! IGNORED !!![0m
[33m- 5061 !!! IGNORED !!![0m
[33m- 5062 !!! IGNORED !!![0m
[33m- 5063 !!! IGNORED !!![0m
[33m- 5064 !!! IGNORED !!![0m
[33m- 5065 !!! IGNORED !!![0m
[33m- 5066 !!! IGNORED !!![0m
[33m- 5067 !!! IGNORED !!![0m
[33m- 5068 !!! IGNORED !!![0m
[33m- 5069 !!! IGNORED !!![0m
[33m- 5070 !!! IGNORED !!![0m
[33m- 5071 !!! IGNORED !!![0m
[33m- 5072 !!! IGNORED !!![0m
[33m- 5073 !!! IGNORED !!![0m
[33m- 5074 !!! IGNORED !!![0m
[33m- 5075 !!! IGNORED !!![0m
[33m- 5076 !!! IGNORED !!![0m
[33m- 5077 !!! IGNORED !!![0m
[33m- 5078 !!! IGNORED !!![0m
[33m- 5079 !!! IGNORED !!![0m
[33m- 5080 !!! IGNORED !!![0m
[33m- 5081 !!! IGNORED !!![0m
[33m- 5082 !!! IGNORED !!![0m
[33m- 5083 !!! IGNORED !!![0m
[33m- 5084 !!! IGNORED !!![0m
[33m- 5085 !!! IGNORED !!![0m
[33m- 5086 !!! IGNORED !!![0m
[33m- 5087 !!! IGNORED !!![0m
[33m- 5088 !!! IGNORED !!![0m
[33m- 5089 !!! IGNORED !!![0m
[33m- 5090 !!! IGNORED !!![0m
[33m- 5091 !!! IGNORED !!![0m
[33m- 5092 !!! IGNORED !!![0m
[33m- 5093 !!! IGNORED !!![0m
[33m- 5094 !!! IGNORED !!![0m
[33m- 5095 !!! IGNORED !!![0m
[33m- 5096 !!! IGNORED !!![0m
[33m- 5097 !!! IGNORED !!![0m
[33m- 5098 !!! IGNORED !!![0m
[33m- 5099 !!! IGNORED !!![0m
[33m- 5100 !!! IGNORED !!![0m
[33m- 5101 !!! IGNORED !!![0m
[33m- 5102 !!! IGNORED !!![0m
[33m- 5103 !!! IGNORED !!![0m
[33m- 5104 !!! IGNORED !!![0m
[33m- 5105 !!! IGNORED !!![0m
[33m- 5106 !!! IGNORED !!![0m
[33m- 5107 !!! IGNORED !!![0m
[33m- 5108 !!! IGNORED !!![0m
[33m- 5109 !!! IGNORED !!![0m
[33m- 5110 !!! IGNORED !!![0m
[33m- 5111 !!! IGNORED !!![0m
[33m- 5112 !!! IGNORED !!![0m
[33m- 5113 !!! IGNORED !!![0m
[33m- 5114 !!! IGNORED !!![0m
[33m- 5115 !!! IGNORED !!![0m
[33m- 5116 !!! IGNORED !!![0m
[33m- 5117 !!! IGNORED !!![0m
[33m- 5118 !!! IGNORED !!![0m
[33m- 5119 !!! IGNORED !!![0m
[33m- 5120 !!! IGNORED !!![0m
[33m- 5121 !!! IGNORED !!![0m
[33m- 5122 !!! IGNORED !!![0m
[33m- 5123 !!! IGNORED !!![0m
[33m- 5124 !!! IGNORED !!![0m
[33m- 5125 !!! IGNORED !!![0m
[33m- 5126 !!! IGNORED !!![0m
[33m- 5127 !!! IGNORED !!![0m
[33m- 5128 !!! IGNORED !!![0m
[33m- 5129 !!! IGNORED !!![0m
[33m- 5130 !!! IGNORED !!![0m
[33m- 5131 !!! IGNORED !!![0m
[33m- 5132 !!! IGNORED !!![0m
[33m- 5133 !!! IGNORED !!![0m
[33m- 5134 !!! IGNORED !!![0m
[33m- 5135 !!! IGNORED !!![0m
[33m- 5136 !!! IGNORED !!![0m
[33m- 5137 !!! IGNORED !!![0m
[33m- 5138 !!! IGNORED !!![0m
[33m- 5139 !!! IGNORED !!![0m
[33m- 5140 !!! IGNORED !!![0m
[33m- 5141 !!! IGNORED !!![0m
[33m- 5142 !!! IGNORED !!![0m
[33m- 5143 !!! IGNORED !!![0m
[33m- 5144 !!! IGNORED !!![0m
[33m- 5145 !!! IGNORED !!![0m
[33m- 5146 !!! IGNORED !!![0m
[33m- 5147 !!! IGNORED !!![0m
[33m- 5148 !!! IGNORED !!![0m
[33m- 5149 !!! IGNORED !!![0m
[33m- 5150 !!! IGNORED !!![0m
[33m- 5151 !!! IGNORED !!![0m
[33m- 5152 !!! IGNORED !!![0m
[33m- 5153 !!! IGNORED !!![0m
[33m- 5154 !!! IGNORED !!![0m
[33m- 5155 !!! IGNORED !!![0m
[33m- 5156 !!! IGNORED !!![0m
[33m- 5157 !!! IGNORED !!![0m
[33m- 5158 !!! IGNORED !!![0m
[33m- 5159 !!! IGNORED !!![0m
[33m- 5160 !!! IGNORED !!![0m
[33m- 5161 !!! IGNORED !!![0m
[33m- 5162 !!! IGNORED !!![0m
[33m- 5163 !!! IGNORED !!![0m
[33m- 5164 !!! IGNORED !!![0m
[33m- 5165 !!! IGNORED !!![0m
[33m- 5166 !!! IGNORED !!![0m
[33m- 5167 !!! IGNORED !!![0m
[33m- 5168 !!! IGNORED !!![0m
[33m- 5169 !!! IGNORED !!![0m
[33m- 5170 !!! IGNORED !!![0m
[33m- 5171 !!! IGNORED !!![0m
[33m- 5172 !!! IGNORED !!![0m
[33m- 5173 !!! IGNORED !!![0m
[33m- 5174 !!! IGNORED !!![0m
[33m- 5175 !!! IGNORED !!![0m
[33m- 5176 !!! IGNORED !!![0m
[33m- 5177 !!! IGNORED !!![0m
[33m- 5178 !!! IGNORED !!![0m
[33m- 5179 !!! IGNORED !!![0m
[33m- 5180 !!! IGNORED !!![0m
[33m- 5181 !!! IGNORED !!![0m
[33m- 5182 !!! IGNORED !!![0m
[33m- 5183 !!! IGNORED !!![0m
[33m- 5184 !!! IGNORED !!![0m
[33m- 5185 !!! IGNORED !!![0m
[33m- 5186 !!! IGNORED !!![0m
[33m- 5187 !!! IGNORED !!![0m
[33m- 5188 !!! IGNORED !!![0m
[33m- 5189 !!! IGNORED !!![0m
[33m- 5190 !!! IGNORED !!![0m
[33m- 5191 !!! IGNORED !!![0m
[33m- 5192 !!! IGNORED !!![0m
[33m- 5193 !!! IGNORED !!![0m
[33m- 5194 !!! IGNORED !!![0m
[33m- 5195 !!! IGNORED !!![0m
[33m- 5196 !!! IGNORED !!![0m
[33m- 5197 !!! IGNORED !!![0m
[33m- 5198 !!! IGNORED !!![0m
[33m- 5199 !!! IGNORED !!![0m
[33m- 5200 !!! IGNORED !!![0m
[33m- 5201 !!! IGNORED !!![0m
[33m- 5202 !!! IGNORED !!![0m
[33m- 5203 !!! IGNORED !!![0m
[33m- 5204 !!! IGNORED !!![0m
[33m- 5205 !!! IGNORED !!![0m
[33m- 5206 !!! IGNORED !!![0m
[33m- 5207 !!! IGNORED !!![0m
[33m- 5208 !!! IGNORED !!![0m
[33m- 5209 !!! IGNORED !!![0m
[33m- 5210 !!! IGNORED !!![0m
[33m- 5211 !!! IGNORED !!![0m
[33m- 5212 !!! IGNORED !!![0m
[33m- 5213 !!! IGNORED !!![0m
[33m- 5214 !!! IGNORED !!![0m
[33m- 5215 !!! IGNORED !!![0m
[33m- 5216 !!! IGNORED !!![0m
[33m- 5217 !!! IGNORED !!![0m
[33m- 5218 !!! IGNORED !!![0m
[33m- 5219 !!! IGNORED !!![0m
[33m- 5220 !!! IGNORED !!![0m
[33m- 5221 !!! IGNORED !!![0m
[33m- 5222 !!! IGNORED !!![0m
[33m- 5223 !!! IGNORED !!![0m
[33m- 5224 !!! IGNORED !!![0m
[33m- 5225 !!! IGNORED !!![0m
[33m- 5226 !!! IGNORED !!![0m
[33m- 5227 !!! IGNORED !!![0m
[33m- 5228 !!! IGNORED !!![0m
[33m- 5229 !!! IGNORED !!![0m
[33m- 5230 !!! IGNORED !!![0m
[33m- 5231 !!! IGNORED !!![0m
[33m- 5232 !!! IGNORED !!![0m
[33m- 5233 !!! IGNORED !!![0m
[33m- 5234 !!! IGNORED !!![0m
[33m- 5235 !!! IGNORED !!![0m
[33m- 5236 !!! IGNORED !!![0m
[33m- 5237 !!! IGNORED !!![0m
[33m- 5238 !!! IGNORED !!![0m
[33m- 5239 !!! IGNORED !!![0m
[33m- 5240 !!! IGNORED !!![0m
[33m- 5241 !!! IGNORED !!![0m
[33m- 5242 !!! IGNORED !!![0m
[33m- 5243 !!! IGNORED !!![0m
[33m- 5244 !!! IGNORED !!![0m
[33m- 5245 !!! IGNORED !!![0m
[33m- 5246 !!! IGNORED !!![0m
[33m- 5247 !!! IGNORED !!![0m
[33m- 5248 !!! IGNORED !!![0m
[33m- 5249 !!! IGNORED !!![0m
[33m- 5250 !!! IGNORED !!![0m
[33m- 5251 !!! IGNORED !!![0m
[33m- 5252 !!! IGNORED !!![0m
[33m- 5253 !!! IGNORED !!![0m
[33m- 5254 !!! IGNORED !!![0m
[33m- 5255 !!! IGNORED !!![0m
[33m- 5256 !!! IGNORED !!![0m
[33m- 5257 !!! IGNORED !!![0m
[33m- 5258 !!! IGNORED !!![0m
[33m- 5259 !!! IGNORED !!![0m
[33m- 5260 !!! IGNORED !!![0m
[33m- 5261 !!! IGNORED !!![0m
[33m- 5262 !!! IGNORED !!![0m
[33m- 5263 !!! IGNORED !!![0m
[33m- 5264 !!! IGNORED !!![0m
[33m- 5265 !!! IGNORED !!![0m
[33m- 5266 !!! IGNORED !!![0m
[33m- 5267 !!! IGNORED !!![0m
[33m- 5268 !!! IGNORED !!![0m
[33m- 5269 !!! IGNORED !!![0m
[33m- 5270 !!! IGNORED !!![0m
[33m- 5271 !!! IGNORED !!![0m
[33m- 5272 !!! IGNORED !!![0m
[33m- 5273 !!! IGNORED !!![0m
[33m- 5274 !!! IGNORED !!![0m
[33m- 5275 !!! IGNORED !!![0m
[33m- 5276 !!! IGNORED !!![0m
[33m- 5277 !!! IGNORED !!![0m
[33m- 5278 !!! IGNORED !!![0m
[33m- 5279 !!! IGNORED !!![0m
[33m- 5280 !!! IGNORED !!![0m
[33m- 5281 !!! IGNORED !!![0m
[33m- 5282 !!! IGNORED !!![0m
[33m- 5283 !!! IGNORED !!![0m
[33m- 5284 !!! IGNORED !!![0m
[33m- 5285 !!! IGNORED !!![0m
[33m- 5286 !!! IGNORED !!![0m
[33m- 5287 !!! IGNORED !!![0m
[33m- 5288 !!! IGNORED !!![0m
[33m- 5289 !!! IGNORED !!![0m
[33m- 5290 !!! IGNORED !!![0m
[33m- 5291 !!! IGNORED !!![0m
[33m- 5292 !!! IGNORED !!![0m
[33m- 5293 !!! IGNORED !!![0m
[33m- 5294 !!! IGNORED !!![0m
[33m- 5295 !!! IGNORED !!![0m
[33m- 5296 !!! IGNORED !!![0m
[33m- 5297 !!! IGNORED !!![0m
[33m- 5298 !!! IGNORED !!![0m
[33m- 5299 !!! IGNORED !!![0m
[33m- 5300 !!! IGNORED !!![0m
[33m- 5301 !!! IGNORED !!![0m
[33m- 5302 !!! IGNORED !!![0m
[33m- 5303 !!! IGNORED !!![0m
[33m- 5304 !!! IGNORED !!![0m
[33m- 5305 !!! IGNORED !!![0m
[33m- 5306 !!! IGNORED !!![0m
[33m- 5307 !!! IGNORED !!![0m
[33m- 5308 !!! IGNORED !!![0m
[33m- 5309 !!! IGNORED !!![0m
[33m- 5310 !!! IGNORED !!![0m
[33m- 5311 !!! IGNORED !!![0m
[33m- 5312 !!! IGNORED !!![0m
[33m- 5313 !!! IGNORED !!![0m
[33m- 5314 !!! IGNORED !!![0m
[33m- 5315 !!! IGNORED !!![0m
[33m- 5316 !!! IGNORED !!![0m
[33m- 5317 !!! IGNORED !!![0m
[33m- 5318 !!! IGNORED !!![0m
[33m- 5319 !!! IGNORED !!![0m
[33m- 5320 !!! IGNORED !!![0m
[33m- 5321 !!! IGNORED !!![0m
[33m- 5322 !!! IGNORED !!![0m
[33m- 5323 !!! IGNORED !!![0m
[33m- 5324 !!! IGNORED !!![0m
[33m- 5325 !!! IGNORED !!![0m
[33m- 5326 !!! IGNORED !!![0m
[33m- 5327 !!! IGNORED !!![0m
[33m- 5328 !!! IGNORED !!![0m
[33m- 5329 !!! IGNORED !!![0m
[33m- 5330 !!! IGNORED !!![0m
[33m- 5331 !!! IGNORED !!![0m
[33m- 5332 !!! IGNORED !!![0m
[33m- 5333 !!! IGNORED !!![0m
[33m- 5334 !!! IGNORED !!![0m
[33m- 5335 !!! IGNORED !!![0m
[33m- 5336 !!! IGNORED !!![0m
[33m- 5337 !!! IGNORED !!![0m
[33m- 5338 !!! IGNORED !!![0m
[33m- 5339 !!! IGNORED !!![0m
[33m- 5340 !!! IGNORED !!![0m
[33m- 5341 !!! IGNORED !!![0m
[33m- 5342 !!! IGNORED !!![0m
[33m- 5343 !!! IGNORED !!![0m
[33m- 5344 !!! IGNORED !!![0m
[33m- 5345 !!! IGNORED !!![0m
[33m- 5346 !!! IGNORED !!![0m
[33m- 5347 !!! IGNORED !!![0m
[33m- 5348 !!! IGNORED !!![0m
[33m- 5349 !!! IGNORED !!![0m
[33m- 5350 !!! IGNORED !!![0m
[33m- 5351 !!! IGNORED !!![0m
[33m- 5352 !!! IGNORED !!![0m
[33m- 5353 !!! IGNORED !!![0m
[33m- 5354 !!! IGNORED !!![0m
[33m- 5355 !!! IGNORED !!![0m
[33m- 5356 !!! IGNORED !!![0m
[33m- 5357 !!! IGNORED !!![0m
[33m- 5358 !!! IGNORED !!![0m
[33m- 5359 !!! IGNORED !!![0m
[33m- 5360 !!! IGNORED !!![0m
[33m- 5361 !!! IGNORED !!![0m
[33m- 5362 !!! IGNORED !!![0m
[33m- 5363 !!! IGNORED !!![0m
[33m- 5364 !!! IGNORED !!![0m
[33m- 5365 !!! IGNORED !!![0m
[33m- 5366 !!! IGNORED !!![0m
[33m- 5367 !!! IGNORED !!![0m
[33m- 5368 !!! IGNORED !!![0m
[33m- 5369 !!! IGNORED !!![0m
[33m- 5370 !!! IGNORED !!![0m
[33m- 5371 !!! IGNORED !!![0m
[33m- 5372 !!! IGNORED !!![0m
[33m- 5373 !!! IGNORED !!![0m
[33m- 5374 !!! IGNORED !!![0m
[33m- 5375 !!! IGNORED !!![0m
[33m- 5376 !!! IGNORED !!![0m
[33m- 5377 !!! IGNORED !!![0m
[33m- 5378 !!! IGNORED !!![0m
[33m- 5379 !!! IGNORED !!![0m
[33m- 5380 !!! IGNORED !!![0m
[33m- 5381 !!! IGNORED !!![0m
[33m- 5382 !!! IGNORED !!![0m
[33m- 5383 !!! IGNORED !!![0m
[33m- 5384 !!! IGNORED !!![0m
[33m- 5385 !!! IGNORED !!![0m
[33m- 5386 !!! IGNORED !!![0m
[33m- 5387 !!! IGNORED !!![0m
[33m- 5388 !!! IGNORED !!![0m
[33m- 5389 !!! IGNORED !!![0m
[33m- 5390 !!! IGNORED !!![0m
[33m- 5391 !!! IGNORED !!![0m
[33m- 5392 !!! IGNORED !!![0m
[33m- 5393 !!! IGNORED !!![0m
[33m- 5394 !!! IGNORED !!![0m
[33m- 5395 !!! IGNORED !!![0m
[33m- 5396 !!! IGNORED !!![0m
[33m- 5397 !!! IGNORED !!![0m
[33m- 5398 !!! IGNORED !!![0m
[33m- 5399 !!! IGNORED !!![0m
[33m- 5400 !!! IGNORED !!![0m
[33m- 5401 !!! IGNORED !!![0m
[33m- 5402 !!! IGNORED !!![0m
[33m- 5403 !!! IGNORED !!![0m
[33m- 5404 !!! IGNORED !!![0m
[33m- 5405 !!! IGNORED !!![0m
[33m- 5406 !!! IGNORED !!![0m
[33m- 5407 !!! IGNORED !!![0m
[33m- 5408 !!! IGNORED !!![0m
[33m- 5409 !!! IGNORED !!![0m
[33m- 5410 !!! IGNORED !!![0m
[33m- 5411 !!! IGNORED !!![0m
[33m- 5412 !!! IGNORED !!![0m
[33m- 5413 !!! IGNORED !!![0m
[33m- 5414 !!! IGNORED !!![0m
[33m- 5415 !!! IGNORED !!![0m
[33m- 5416 !!! IGNORED !!![0m
[33m- 5417 !!! IGNORED !!![0m
[33m- 5418 !!! IGNORED !!![0m
[33m- 5419 !!! IGNORED !!![0m
[33m- 5420 !!! IGNORED !!![0m
[33m- 5421 !!! IGNORED !!![0m
[33m- 5422 !!! IGNORED !!![0m
[33m- 5423 !!! IGNORED !!![0m
[33m- 5424 !!! IGNORED !!![0m
[33m- 5425 !!! IGNORED !!![0m
[33m- 5426 !!! IGNORED !!![0m
[33m- 5427 !!! IGNORED !!![0m
[33m- 5428 !!! IGNORED !!![0m
[33m- 5429 !!! IGNORED !!![0m
[33m- 5430 !!! IGNORED !!![0m
[33m- 5431 !!! IGNORED !!![0m
[33m- 5432 !!! IGNORED !!![0m
[33m- 5433 !!! IGNORED !!![0m
[33m- 5434 !!! IGNORED !!![0m
[33m- 5435 !!! IGNORED !!![0m
[33m- 5436 !!! IGNORED !!![0m
[33m- 5437 !!! IGNORED !!![0m
[33m- 5438 !!! IGNORED !!![0m
[33m- 5439 !!! IGNORED !!![0m
[33m- 5440 !!! IGNORED !!![0m
[33m- 5441 !!! IGNORED !!![0m
[33m- 5442 !!! IGNORED !!![0m
[33m- 5443 !!! IGNORED !!![0m
[33m- 5444 !!! IGNORED !!![0m
[33m- 5445 !!! IGNORED !!![0m
[33m- 5446 !!! IGNORED !!![0m
[33m- 5447 !!! IGNORED !!![0m
[33m- 5448 !!! IGNORED !!![0m
[33m- 5449 !!! IGNORED !!![0m
[33m- 5450 !!! IGNORED !!![0m
[33m- 5451 !!! IGNORED !!![0m
[33m- 5452 !!! IGNORED !!![0m
[33m- 5453 !!! IGNORED !!![0m
[33m- 5454 !!! IGNORED !!![0m
[33m- 5455 !!! IGNORED !!![0m
[33m- 5456 !!! IGNORED !!![0m
[33m- 5457 !!! IGNORED !!![0m
[33m- 5458 !!! IGNORED !!![0m
[33m- 5459 !!! IGNORED !!![0m
[33m- 5460 !!! IGNORED !!![0m
[33m- 5461 !!! IGNORED !!![0m
[33m- 5462 !!! IGNORED !!![0m
[33m- 5463 !!! IGNORED !!![0m
[33m- 5464 !!! IGNORED !!![0m
[33m- 5465 !!! IGNORED !!![0m
[33m- 5466 !!! IGNORED !!![0m
[33m- 5467 !!! IGNORED !!![0m
[33m- 5468 !!! IGNORED !!![0m
[33m- 5469 !!! IGNORED !!![0m
[33m- 5470 !!! IGNORED !!![0m
[33m- 5471 !!! IGNORED !!![0m
[33m- 5472 !!! IGNORED !!![0m
[33m- 5473 !!! IGNORED !!![0m
[33m- 5474 !!! IGNORED !!![0m
[33m- 5475 !!! IGNORED !!![0m
[33m- 5476 !!! IGNORED !!![0m
[33m- 5477 !!! IGNORED !!![0m
[33m- 5478 !!! IGNORED !!![0m
[33m- 5479 !!! IGNORED !!![0m
[33m- 5480 !!! IGNORED !!![0m
[33m- 5481 !!! IGNORED !!![0m
[33m- 5482 !!! IGNORED !!![0m
[33m- 5483 !!! IGNORED !!![0m
[33m- 5484 !!! IGNORED !!![0m
[33m- 5485 !!! IGNORED !!![0m
[33m- 5486 !!! IGNORED !!![0m
[33m- 5487 !!! IGNORED !!![0m
[33m- 5488 !!! IGNORED !!![0m
[33m- 5489 !!! IGNORED !!![0m
[33m- 5490 !!! IGNORED !!![0m
[33m- 5491 !!! IGNORED !!![0m
[33m- 5492 !!! IGNORED !!![0m
[33m- 5493 !!! IGNORED !!![0m
[33m- 5494 !!! IGNORED !!![0m
[33m- 5495 !!! IGNORED !!![0m
[33m- 5496 !!! IGNORED !!![0m
[33m- 5497 !!! IGNORED !!![0m
[33m- 5498 !!! IGNORED !!![0m
[33m- 5499 !!! IGNORED !!![0m
[33m- 5500 !!! IGNORED !!![0m
[33m- 5501 !!! IGNORED !!![0m
[33m- 5502 !!! IGNORED !!![0m
[33m- 5503 !!! IGNORED !!![0m
[33m- 5504 !!! IGNORED !!![0m
[33m- 5505 !!! IGNORED !!![0m
[33m- 5506 !!! IGNORED !!![0m
[33m- 5507 !!! IGNORED !!![0m
[33m- 5508 !!! IGNORED !!![0m
[33m- 5509 !!! IGNORED !!![0m
[33m- 5510 !!! IGNORED !!![0m
[33m- 5511 !!! IGNORED !!![0m
[33m- 5512 !!! IGNORED !!![0m
[33m- 5513 !!! IGNORED !!![0m
[33m- 5514 !!! IGNORED !!![0m
[33m- 5515 !!! IGNORED !!![0m
[33m- 5516 !!! IGNORED !!![0m
[33m- 5517 !!! IGNORED !!![0m
[33m- 5518 !!! IGNORED !!![0m
[33m- 5519 !!! IGNORED !!![0m
[33m- 5520 !!! IGNORED !!![0m
[33m- 5521 !!! IGNORED !!![0m
[33m- 5522 !!! IGNORED !!![0m
[33m- 5523 !!! IGNORED !!![0m
[33m- 5524 !!! IGNORED !!![0m
[33m- 5525 !!! IGNORED !!![0m
[33m- 5526 !!! IGNORED !!![0m
[33m- 5527 !!! IGNORED !!![0m
[33m- 5528 !!! IGNORED !!![0m
[33m- 5529 !!! IGNORED !!![0m
[33m- 5530 !!! IGNORED !!![0m
[33m- 5531 !!! IGNORED !!![0m
[33m- 5532 !!! IGNORED !!![0m
[33m- 5533 !!! IGNORED !!![0m
[33m- 5534 !!! IGNORED !!![0m
[33m- 5535 !!! IGNORED !!![0m
[33m- 5536 !!! IGNORED !!![0m
[33m- 5537 !!! IGNORED !!![0m
[33m- 5538 !!! IGNORED !!![0m
[33m- 5539 !!! IGNORED !!![0m
[33m- 5540 !!! IGNORED !!![0m
[33m- 5541 !!! IGNORED !!![0m
[33m- 5542 !!! IGNORED !!![0m
[33m- 5543 !!! IGNORED !!![0m
[33m- 5544 !!! IGNORED !!![0m
[33m- 5545 !!! IGNORED !!![0m
[33m- 5546 !!! IGNORED !!![0m
[33m- 5547 !!! IGNORED !!![0m
[33m- 5548 !!! IGNORED !!![0m
[33m- 5549 !!! IGNORED !!![0m
[33m- 5550 !!! IGNORED !!![0m
[33m- 5551 !!! IGNORED !!![0m
[33m- 5552 !!! IGNORED !!![0m
[33m- 5553 !!! IGNORED !!![0m
[33m- 5554 !!! IGNORED !!![0m
[33m- 5555 !!! IGNORED !!![0m
[33m- 5556 !!! IGNORED !!![0m
[33m- 5557 !!! IGNORED !!![0m
[33m- 5558 !!! IGNORED !!![0m
[33m- 5559 !!! IGNORED !!![0m
[33m- 5560 !!! IGNORED !!![0m
[33m- 5561 !!! IGNORED !!![0m
[33m- 5562 !!! IGNORED !!![0m
[33m- 5563 !!! IGNORED !!![0m
[33m- 5564 !!! IGNORED !!![0m
[33m- 5565 !!! IGNORED !!![0m
[33m- 5566 !!! IGNORED !!![0m
[33m- 5567 !!! IGNORED !!![0m
[33m- 5568 !!! IGNORED !!![0m
[33m- 5569 !!! IGNORED !!![0m
[33m- 5570 !!! IGNORED !!![0m
[33m- 5571 !!! IGNORED !!![0m
[33m- 5572 !!! IGNORED !!![0m
[33m- 5573 !!! IGNORED !!![0m
[33m- 5574 !!! IGNORED !!![0m
[33m- 5575 !!! IGNORED !!![0m
[33m- 5576 !!! IGNORED !!![0m
[33m- 5577 !!! IGNORED !!![0m
[33m- 5578 !!! IGNORED !!![0m
[33m- 5579 !!! IGNORED !!![0m
[33m- 5580 !!! IGNORED !!![0m
[33m- 5581 !!! IGNORED !!![0m
[33m- 5582 !!! IGNORED !!![0m
[33m- 5583 !!! IGNORED !!![0m
[33m- 5584 !!! IGNORED !!![0m
[33m- 5585 !!! IGNORED !!![0m
[33m- 5586 !!! IGNORED !!![0m
[33m- 5587 !!! IGNORED !!![0m
[33m- 5588 !!! IGNORED !!![0m
[33m- 5589 !!! IGNORED !!![0m
[33m- 5590 !!! IGNORED !!![0m
[33m- 5591 !!! IGNORED !!![0m
[33m- 5592 !!! IGNORED !!![0m
[33m- 5593 !!! IGNORED !!![0m
[33m- 5594 !!! IGNORED !!![0m
[33m- 5595 !!! IGNORED !!![0m
[33m- 5596 !!! IGNORED !!![0m
[33m- 5597 !!! IGNORED !!![0m
[33m- 5598 !!! IGNORED !!![0m
[33m- 5599 !!! IGNORED !!![0m
[33m- 5600 !!! IGNORED !!![0m
[33m- 5601 !!! IGNORED !!![0m
[33m- 5602 !!! IGNORED !!![0m
[33m- 5603 !!! IGNORED !!![0m
[33m- 5604 !!! IGNORED !!![0m
[33m- 5605 !!! IGNORED !!![0m
[33m- 5606 !!! IGNORED !!![0m
[33m- 5607 !!! IGNORED !!![0m
[33m- 5608 !!! IGNORED !!![0m
[33m- 5609 !!! IGNORED !!![0m
[33m- 5610 !!! IGNORED !!![0m
[33m- 5611 !!! IGNORED !!![0m
[33m- 5612 !!! IGNORED !!![0m
[33m- 5613 !!! IGNORED !!![0m
[33m- 5614 !!! IGNORED !!![0m
[33m- 5615 !!! IGNORED !!![0m
[33m- 5616 !!! IGNORED !!![0m
[33m- 5617 !!! IGNORED !!![0m
[33m- 5618 !!! IGNORED !!![0m
[33m- 5619 !!! IGNORED !!![0m
[33m- 5620 !!! IGNORED !!![0m
[33m- 5621 !!! IGNORED !!![0m
[33m- 5622 !!! IGNORED !!![0m
[33m- 5623 !!! IGNORED !!![0m
[33m- 5624 !!! IGNORED !!![0m
[33m- 5625 !!! IGNORED !!![0m
[33m- 5626 !!! IGNORED !!![0m
[33m- 5627 !!! IGNORED !!![0m
[33m- 5628 !!! IGNORED !!![0m
[33m- 5629 !!! IGNORED !!![0m
[33m- 5630 !!! IGNORED !!![0m
[33m- 5631 !!! IGNORED !!![0m
[33m- 5632 !!! IGNORED !!![0m
[33m- 5633 !!! IGNORED !!![0m
[33m- 5634 !!! IGNORED !!![0m
[33m- 5635 !!! IGNORED !!![0m
[33m- 5636 !!! IGNORED !!![0m
[33m- 5637 !!! IGNORED !!![0m
[33m- 5638 !!! IGNORED !!![0m
[33m- 5639 !!! IGNORED !!![0m
[33m- 5640 !!! IGNORED !!![0m
[33m- 5641 !!! IGNORED !!![0m
[33m- 5642 !!! IGNORED !!![0m
[33m- 5643 !!! IGNORED !!![0m
[33m- 5644 !!! IGNORED !!![0m
[33m- 5645 !!! IGNORED !!![0m
[33m- 5646 !!! IGNORED !!![0m
[33m- 5647 !!! IGNORED !!![0m
[33m- 5648 !!! IGNORED !!![0m
[33m- 5649 !!! IGNORED !!![0m
[33m- 5650 !!! IGNORED !!![0m
[33m- 5651 !!! IGNORED !!![0m
[33m- 5652 !!! IGNORED !!![0m
[33m- 5653 !!! IGNORED !!![0m
[33m- 5654 !!! IGNORED !!![0m
[33m- 5655 !!! IGNORED !!![0m
[33m- 5656 !!! IGNORED !!![0m
[33m- 5657 !!! IGNORED !!![0m
[33m- 5658 !!! IGNORED !!![0m
[33m- 5659 !!! IGNORED !!![0m
[33m- 5660 !!! IGNORED !!![0m
[33m- 5661 !!! IGNORED !!![0m
[33m- 5662 !!! IGNORED !!![0m
[33m- 5663 !!! IGNORED !!![0m
[33m- 5664 !!! IGNORED !!![0m
[33m- 5665 !!! IGNORED !!![0m
[33m- 5666 !!! IGNORED !!![0m
[33m- 5667 !!! IGNORED !!![0m
[33m- 5668 !!! IGNORED !!![0m
[33m- 5669 !!! IGNORED !!![0m
[33m- 5670 !!! IGNORED !!![0m
[33m- 5671 !!! IGNORED !!![0m
[33m- 5672 !!! IGNORED !!![0m
[33m- 5673 !!! IGNORED !!![0m
[33m- 5674 !!! IGNORED !!![0m
[33m- 5675 !!! IGNORED !!![0m
[33m- 5676 !!! IGNORED !!![0m
[33m- 5677 !!! IGNORED !!![0m
[33m- 5678 !!! IGNORED !!![0m
[33m- 5679 !!! IGNORED !!![0m
[33m- 5680 !!! IGNORED !!![0m
[33m- 5681 !!! IGNORED !!![0m
[33m- 5682 !!! IGNORED !!![0m
[33m- 5683 !!! IGNORED !!![0m
[33m- 5684 !!! IGNORED !!![0m
[33m- 5685 !!! IGNORED !!![0m
[33m- 5686 !!! IGNORED !!![0m
[33m- 5687 !!! IGNORED !!![0m
[33m- 5688 !!! IGNORED !!![0m
[33m- 5689 !!! IGNORED !!![0m
[33m- 5690 !!! IGNORED !!![0m
[33m- 5691 !!! IGNORED !!![0m
[33m- 5692 !!! IGNORED !!![0m
[33m- 5693 !!! IGNORED !!![0m
[33m- 5694 !!! IGNORED !!![0m
[33m- 5695 !!! IGNORED !!![0m
[33m- 5696 !!! IGNORED !!![0m
[33m- 5697 !!! IGNORED !!![0m
[33m- 5698 !!! IGNORED !!![0m
[33m- 5699 !!! IGNORED !!![0m
[33m- 5700 !!! IGNORED !!![0m
[33m- 5701 !!! IGNORED !!![0m
[33m- 5702 !!! IGNORED !!![0m
[33m- 5703 !!! IGNORED !!![0m
[33m- 5704 !!! IGNORED !!![0m
[33m- 5705 !!! IGNORED !!![0m
[33m- 5706 !!! IGNORED !!![0m
[33m- 5707 !!! IGNORED !!![0m
[33m- 5708 !!! IGNORED !!![0m
[33m- 5709 !!! IGNORED !!![0m
[33m- 5710 !!! IGNORED !!![0m
[33m- 5711 !!! IGNORED !!![0m
[33m- 5712 !!! IGNORED !!![0m
[33m- 5713 !!! IGNORED !!![0m
[33m- 5714 !!! IGNORED !!![0m
[33m- 5715 !!! IGNORED !!![0m
[33m- 5716 !!! IGNORED !!![0m
[33m- 5717 !!! IGNORED !!![0m
[33m- 5718 !!! IGNORED !!![0m
[33m- 5719 !!! IGNORED !!![0m
[33m- 5720 !!! IGNORED !!![0m
[33m- 5721 !!! IGNORED !!![0m
[33m- 5722 !!! IGNORED !!![0m
[33m- 5723 !!! IGNORED !!![0m
[33m- 5724 !!! IGNORED !!![0m
[33m- 5725 !!! IGNORED !!![0m
[33m- 5726 !!! IGNORED !!![0m
[33m- 5727 !!! IGNORED !!![0m
[33m- 5728 !!! IGNORED !!![0m
[33m- 5729 !!! IGNORED !!![0m
[33m- 5730 !!! IGNORED !!![0m
[33m- 5731 !!! IGNORED !!![0m
[33m- 5732 !!! IGNORED !!![0m
[33m- 5733 !!! IGNORED !!![0m
[33m- 5734 !!! IGNORED !!![0m
[33m- 5735 !!! IGNORED !!![0m
[33m- 5736 !!! IGNORED !!![0m
[33m- 5737 !!! IGNORED !!![0m
[33m- 5738 !!! IGNORED !!![0m
[33m- 5739 !!! IGNORED !!![0m
[33m- 5740 !!! IGNORED !!![0m
[33m- 5741 !!! IGNORED !!![0m
[33m- 5742 !!! IGNORED !!![0m
[33m- 5743 !!! IGNORED !!![0m
[33m- 5744 !!! IGNORED !!![0m
[33m- 5745 !!! IGNORED !!![0m
[33m- 5746 !!! IGNORED !!![0m
[33m- 5747 !!! IGNORED !!![0m
[33m- 5748 !!! IGNORED !!![0m
[33m- 5749 !!! IGNORED !!![0m
[33m- 5750 !!! IGNORED !!![0m
[33m- 5751 !!! IGNORED !!![0m
[33m- 5752 !!! IGNORED !!![0m
[33m- 5753 !!! IGNORED !!![0m
[33m- 5754 !!! IGNORED !!![0m
[33m- 5755 !!! IGNORED !!![0m
[33m- 5756 !!! IGNORED !!![0m
[33m- 5757 !!! IGNORED !!![0m
[33m- 5758 !!! IGNORED !!![0m
[33m- 5759 !!! IGNORED !!![0m
[33m- 5760 !!! IGNORED !!![0m
[33m- 5761 !!! IGNORED !!![0m
[33m- 5762 !!! IGNORED !!![0m
[33m- 5763 !!! IGNORED !!![0m
[33m- 5764 !!! IGNORED !!![0m
[33m- 5765 !!! IGNORED !!![0m
[33m- 5766 !!! IGNORED !!![0m
[33m- 5767 !!! IGNORED !!![0m
[33m- 5768 !!! IGNORED !!![0m
[33m- 5769 !!! IGNORED !!![0m
[33m- 5770 !!! IGNORED !!![0m
[33m- 5771 !!! IGNORED !!![0m
[33m- 5772 !!! IGNORED !!![0m
[33m- 5773 !!! IGNORED !!![0m
[33m- 5774 !!! IGNORED !!![0m
[33m- 5775 !!! IGNORED !!![0m
[33m- 5776 !!! IGNORED !!![0m
[33m- 5777 !!! IGNORED !!![0m
[33m- 5778 !!! IGNORED !!![0m
[33m- 5779 !!! IGNORED !!![0m
[33m- 5780 !!! IGNORED !!![0m
[33m- 5781 !!! IGNORED !!![0m
[33m- 5782 !!! IGNORED !!![0m
[33m- 5783 !!! IGNORED !!![0m
[33m- 5784 !!! IGNORED !!![0m
[33m- 5785 !!! IGNORED !!![0m
[33m- 5786 !!! IGNORED !!![0m
[33m- 5787 !!! IGNORED !!![0m
[33m- 5788 !!! IGNORED !!![0m
[33m- 5789 !!! IGNORED !!![0m
[33m- 5790 !!! IGNORED !!![0m
[33m- 5791 !!! IGNORED !!![0m
[33m- 5792 !!! IGNORED !!![0m
[33m- 5793 !!! IGNORED !!![0m
[33m- 5794 !!! IGNORED !!![0m
[33m- 5795 !!! IGNORED !!![0m
[33m- 5796 !!! IGNORED !!![0m
[33m- 5797 !!! IGNORED !!![0m
[33m- 5798 !!! IGNORED !!![0m
[33m- 5799 !!! IGNORED !!![0m
[33m- 5800 !!! IGNORED !!![0m
[33m- 5801 !!! IGNORED !!![0m
[33m- 5802 !!! IGNORED !!![0m
[33m- 5803 !!! IGNORED !!![0m
[33m- 5804 !!! IGNORED !!![0m
[33m- 5805 !!! IGNORED !!![0m
[33m- 5806 !!! IGNORED !!![0m
[33m- 5807 !!! IGNORED !!![0m
[33m- 5808 !!! IGNORED !!![0m
[33m- 5809 !!! IGNORED !!![0m
[33m- 5810 !!! IGNORED !!![0m
[33m- 5811 !!! IGNORED !!![0m
[33m- 5812 !!! IGNORED !!![0m
[33m- 5813 !!! IGNORED !!![0m
[33m- 5814 !!! IGNORED !!![0m
[33m- 5815 !!! IGNORED !!![0m
[33m- 5816 !!! IGNORED !!![0m
[33m- 5817 !!! IGNORED !!![0m
[33m- 5818 !!! IGNORED !!![0m
[33m- 5819 !!! IGNORED !!![0m
[33m- 5820 !!! IGNORED !!![0m
[33m- 5821 !!! IGNORED !!![0m
[33m- 5822 !!! IGNORED !!![0m
[33m- 5823 !!! IGNORED !!![0m
[33m- 5824 !!! IGNORED !!![0m
[33m- 5825 !!! IGNORED !!![0m
[33m- 5826 !!! IGNORED !!![0m
[33m- 5827 !!! IGNORED !!![0m
[33m- 5828 !!! IGNORED !!![0m
[33m- 5829 !!! IGNORED !!![0m
[33m- 5830 !!! IGNORED !!![0m
[33m- 5831 !!! IGNORED !!![0m
[33m- 5832 !!! IGNORED !!![0m
[33m- 5833 !!! IGNORED !!![0m
[33m- 5834 !!! IGNORED !!![0m
[33m- 5835 !!! IGNORED !!![0m
[33m- 5836 !!! IGNORED !!![0m
[33m- 5837 !!! IGNORED !!![0m
[33m- 5838 !!! IGNORED !!![0m
[33m- 5839 !!! IGNORED !!![0m
[33m- 5840 !!! IGNORED !!![0m
[33m- 5841 !!! IGNORED !!![0m
[33m- 5842 !!! IGNORED !!![0m
[33m- 5843 !!! IGNORED !!![0m
[33m- 5844 !!! IGNORED !!![0m
[33m- 5845 !!! IGNORED !!![0m
[33m- 5846 !!! IGNORED !!![0m
[33m- 5847 !!! IGNORED !!![0m
[33m- 5848 !!! IGNORED !!![0m
[33m- 5849 !!! IGNORED !!![0m
[33m- 5850 !!! IGNORED !!![0m
[33m- 5851 !!! IGNORED !!![0m
[33m- 5852 !!! IGNORED !!![0m
[33m- 5853 !!! IGNORED !!![0m
[33m- 5854 !!! IGNORED !!![0m
[33m- 5855 !!! IGNORED !!![0m
[33m- 5856 !!! IGNORED !!![0m
[33m- 5857 !!! IGNORED !!![0m
[33m- 5858 !!! IGNORED !!![0m
[33m- 5859 !!! IGNORED !!![0m
[33m- 5860 !!! IGNORED !!![0m
[33m- 5861 !!! IGNORED !!![0m
[33m- 5862 !!! IGNORED !!![0m
[33m- 5863 !!! IGNORED !!![0m
[33m- 5864 !!! IGNORED !!![0m
[33m- 5865 !!! IGNORED !!![0m
[33m- 5866 !!! IGNORED !!![0m
[33m- 5867 !!! IGNORED !!![0m
[33m- 5868 !!! IGNORED !!![0m
[33m- 5869 !!! IGNORED !!![0m
[33m- 5870 !!! IGNORED !!![0m
[33m- 5871 !!! IGNORED !!![0m
[33m- 5872 !!! IGNORED !!![0m
[33m- 5873 !!! IGNORED !!![0m
[33m- 5874 !!! IGNORED !!![0m
[33m- 5875 !!! IGNORED !!![0m
[33m- 5876 !!! IGNORED !!![0m
[33m- 5877 !!! IGNORED !!![0m
[33m- 5878 !!! IGNORED !!![0m
[33m- 5879 !!! IGNORED !!![0m
[33m- 5880 !!! IGNORED !!![0m
[33m- 5881 !!! IGNORED !!![0m
[33m- 5882 !!! IGNORED !!![0m
[33m- 5883 !!! IGNORED !!![0m
[33m- 5884 !!! IGNORED !!![0m
[33m- 5885 !!! IGNORED !!![0m
[33m- 5886 !!! IGNORED !!![0m
[33m- 5887 !!! IGNORED !!![0m
[33m- 5888 !!! IGNORED !!![0m
[33m- 5889 !!! IGNORED !!![0m
[33m- 5890 !!! IGNORED !!![0m
[33m- 5891 !!! IGNORED !!![0m
[33m- 5892 !!! IGNORED !!![0m
[33m- 5893 !!! IGNORED !!![0m
[33m- 5894 !!! IGNORED !!![0m
[33m- 5895 !!! IGNORED !!![0m
[33m- 5896 !!! IGNORED !!![0m
[33m- 5897 !!! IGNORED !!![0m
[33m- 5898 !!! IGNORED !!![0m
[33m- 5899 !!! IGNORED !!![0m
[33m- 5900 !!! IGNORED !!![0m
[33m- 5901 !!! IGNORED !!![0m
[33m- 5902 !!! IGNORED !!![0m
[33m- 5903 !!! IGNORED !!![0m
[33m- 5904 !!! IGNORED !!![0m
[33m- 5905 !!! IGNORED !!![0m
[33m- 5906 !!! IGNORED !!![0m
[33m- 5907 !!! IGNORED !!![0m
[33m- 5908 !!! IGNORED !!![0m
[33m- 5909 !!! IGNORED !!![0m
[33m- 5910 !!! IGNORED !!![0m
[33m- 5911 !!! IGNORED !!![0m
[33m- 5912 !!! IGNORED !!![0m
[33m- 5913 !!! IGNORED !!![0m
[33m- 5914 !!! IGNORED !!![0m
[33m- 5915 !!! IGNORED !!![0m
[33m- 5916 !!! IGNORED !!![0m
[33m- 5917 !!! IGNORED !!![0m
[33m- 5918 !!! IGNORED !!![0m
[33m- 5919 !!! IGNORED !!![0m
[33m- 5920 !!! IGNORED !!![0m
[33m- 5921 !!! IGNORED !!![0m
[33m- 5922 !!! IGNORED !!![0m
[33m- 5923 !!! IGNORED !!![0m
[33m- 5924 !!! IGNORED !!![0m
[33m- 5925 !!! IGNORED !!![0m
[33m- 5926 !!! IGNORED !!![0m
[33m- 5927 !!! IGNORED !!![0m
[33m- 5928 !!! IGNORED !!![0m
[33m- 5929 !!! IGNORED !!![0m
[33m- 5930 !!! IGNORED !!![0m
[33m- 5931 !!! IGNORED !!![0m
[33m- 5932 !!! IGNORED !!![0m
[33m- 5933 !!! IGNORED !!![0m
[33m- 5934 !!! IGNORED !!![0m
[33m- 5935 !!! IGNORED !!![0m
[33m- 5936 !!! IGNORED !!![0m
[33m- 5937 !!! IGNORED !!![0m
[33m- 5938 !!! IGNORED !!![0m
[33m- 5939 !!! IGNORED !!![0m
[33m- 5940 !!! IGNORED !!![0m
[33m- 5941 !!! IGNORED !!![0m
[33m- 5942 !!! IGNORED !!![0m
[33m- 5943 !!! IGNORED !!![0m
[33m- 5944 !!! IGNORED !!![0m
[33m- 5945 !!! IGNORED !!![0m
[33m- 5946 !!! IGNORED !!![0m
[33m- 5947 !!! IGNORED !!![0m
[33m- 5948 !!! IGNORED !!![0m
[33m- 5949 !!! IGNORED !!![0m
[33m- 5950 !!! IGNORED !!![0m
[33m- 5951 !!! IGNORED !!![0m
[33m- 5952 !!! IGNORED !!![0m
[33m- 5953 !!! IGNORED !!![0m
[33m- 5954 !!! IGNORED !!![0m
[33m- 5955 !!! IGNORED !!![0m
[33m- 5956 !!! IGNORED !!![0m
[33m- 5957 !!! IGNORED !!![0m
[33m- 5958 !!! IGNORED !!![0m
[33m- 5959 !!! IGNORED !!![0m
[33m- 5960 !!! IGNORED !!![0m
[33m- 5961 !!! IGNORED !!![0m
[33m- 5962 !!! IGNORED !!![0m
[33m- 5963 !!! IGNORED !!![0m
[33m- 5964 !!! IGNORED !!![0m
[33m- 5965 !!! IGNORED !!![0m
[33m- 5966 !!! IGNORED !!![0m
[33m- 5967 !!! IGNORED !!![0m
[33m- 5968 !!! IGNORED !!![0m
[33m- 5969 !!! IGNORED !!![0m
[33m- 5970 !!! IGNORED !!![0m
[33m- 5971 !!! IGNORED !!![0m
[33m- 5972 !!! IGNORED !!![0m
[33m- 5973 !!! IGNORED !!![0m
[33m- 5974 !!! IGNORED !!![0m
[33m- 5975 !!! IGNORED !!![0m
[33m- 5976 !!! IGNORED !!![0m
[33m- 5977 !!! IGNORED !!![0m
[33m- 5978 !!! IGNORED !!![0m
[33m- 5979 !!! IGNORED !!![0m
[33m- 5980 !!! IGNORED !!![0m
[33m- 5981 !!! IGNORED !!![0m
[33m- 5982 !!! IGNORED !!![0m
[33m- 5983 !!! IGNORED !!![0m
[33m- 5984 !!! IGNORED !!![0m
[33m- 5985 !!! IGNORED !!![0m
[33m- 5986 !!! IGNORED !!![0m
[33m- 5987 !!! IGNORED !!![0m
[33m- 5988 !!! IGNORED !!![0m
[33m- 5989 !!! IGNORED !!![0m
[33m- 5990 !!! IGNORED !!![0m
[33m- 5991 !!! IGNORED !!![0m
[33m- 5992 !!! IGNORED !!![0m
[33m- 5993 !!! IGNORED !!![0m
[33m- 5994 !!! IGNORED !!![0m
[33m- 5995 !!! IGNORED !!![0m
[33m- 5996 !!! IGNORED !!![0m
[33m- 5997 !!! IGNORED !!![0m
[33m- 5998 !!! IGNORED !!![0m
[33m- 5999 !!! IGNORED !!![0m
[33m- 6000 !!! IGNORED !!![0m
[33m- 6001 !!! IGNORED !!![0m
[33m- 6002 !!! IGNORED !!![0m
[33m- 6003 !!! IGNORED !!![0m
[33m- 6004 !!! IGNORED !!![0m
[33m- 6005 !!! IGNORED !!![0m
[33m- 6006 !!! IGNORED !!![0m
[33m- 6007 !!! IGNORED !!![0m
[33m- 6008 !!! IGNORED !!![0m
[33m- 6009 !!! IGNORED !!![0m
[33m- 6010 !!! IGNORED !!![0m
[33m- 6011 !!! IGNORED !!![0m
[33m- 6012 !!! IGNORED !!![0m
[33m- 6013 !!! IGNORED !!![0m
[33m- 6014 !!! IGNORED !!![0m
[33m- 6015 !!! IGNORED !!![0m
[33m- 6016 !!! IGNORED !!![0m
[33m- 6017 !!! IGNORED !!![0m
[33m- 6018 !!! IGNORED !!![0m
[33m- 6019 !!! IGNORED !!![0m
[33m- 6020 !!! IGNORED !!![0m
[33m- 6021 !!! IGNORED !!![0m
[33m- 6022 !!! IGNORED !!![0m
[33m- 6023 !!! IGNORED !!![0m
[33m- 6024 !!! IGNORED !!![0m
[33m- 6025 !!! IGNORED !!![0m
[33m- 6026 !!! IGNORED !!![0m
[33m- 6027 !!! IGNORED !!![0m
[33m- 6028 !!! IGNORED !!![0m
[33m- 6029 !!! IGNORED !!![0m
[33m- 6030 !!! IGNORED !!![0m
[33m- 6031 !!! IGNORED !!![0m
[33m- 6032 !!! IGNORED !!![0m
[33m- 6033 !!! IGNORED !!![0m
[33m- 6034 !!! IGNORED !!![0m
[33m- 6035 !!! IGNORED !!![0m
[33m- 6036 !!! IGNORED !!![0m
[33m- 6037 !!! IGNORED !!![0m
[33m- 6038 !!! IGNORED !!![0m
[33m- 6039 !!! IGNORED !!![0m
[33m- 6040 !!! IGNORED !!![0m
[33m- 6041 !!! IGNORED !!![0m
[33m- 6042 !!! IGNORED !!![0m
[33m- 6043 !!! IGNORED !!![0m
[33m- 6044 !!! IGNORED !!![0m
[33m- 6045 !!! IGNORED !!![0m
[33m- 6046 !!! IGNORED !!![0m
[33m- 6047 !!! IGNORED !!![0m
[33m- 6048 !!! IGNORED !!![0m
[33m- 6049 !!! IGNORED !!![0m
[33m- 6050 !!! IGNORED !!![0m
[33m- 6051 !!! IGNORED !!![0m
[33m- 6052 !!! IGNORED !!![0m
[33m- 6053 !!! IGNORED !!![0m
[33m- 6054 !!! IGNORED !!![0m
[33m- 6055 !!! IGNORED !!![0m
[33m- 6056 !!! IGNORED !!![0m
[33m- 6057 !!! IGNORED !!![0m
[33m- 6058 !!! IGNORED !!![0m
[33m- 6059 !!! IGNORED !!![0m
[33m- 6060 !!! IGNORED !!![0m
[33m- 6061 !!! IGNORED !!![0m
[33m- 6062 !!! IGNORED !!![0m
[33m- 6063 !!! IGNORED !!![0m
[33m- 6064 !!! IGNORED !!![0m
[33m- 6065 !!! IGNORED !!![0m
[33m- 6066 !!! IGNORED !!![0m
[33m- 6067 !!! IGNORED !!![0m
[33m- 6068 !!! IGNORED !!![0m
[33m- 6069 !!! IGNORED !!![0m
[33m- 6070 !!! IGNORED !!![0m
[33m- 6071 !!! IGNORED !!![0m
[33m- 6072 !!! IGNORED !!![0m
[33m- 6073 !!! IGNORED !!![0m
[33m- 6074 !!! IGNORED !!![0m
[33m- 6075 !!! IGNORED !!![0m
[33m- 6076 !!! IGNORED !!![0m
[33m- 6077 !!! IGNORED !!![0m
[33m- 6078 !!! IGNORED !!![0m
[33m- 6079 !!! IGNORED !!![0m
[33m- 6080 !!! IGNORED !!![0m
[33m- 6081 !!! IGNORED !!![0m
[33m- 6082 !!! IGNORED !!![0m
[33m- 6083 !!! IGNORED !!![0m
[33m- 6084 !!! IGNORED !!![0m
[33m- 6085 !!! IGNORED !!![0m
[33m- 6086 !!! IGNORED !!![0m
[33m- 6087 !!! IGNORED !!![0m
[33m- 6088 !!! IGNORED !!![0m
[33m- 6089 !!! IGNORED !!![0m
[33m- 6090 !!! IGNORED !!![0m
[33m- 6091 !!! IGNORED !!![0m
[33m- 6092 !!! IGNORED !!![0m
[33m- 6093 !!! IGNORED !!![0m
[33m- 6094 !!! IGNORED !!![0m
[33m- 6095 !!! IGNORED !!![0m
[33m- 6096 !!! IGNORED !!![0m
[33m- 6097 !!! IGNORED !!![0m
[33m- 6098 !!! IGNORED !!![0m
[33m- 6099 !!! IGNORED !!![0m
[33m- 6100 !!! IGNORED !!![0m
[33m- 6101 !!! IGNORED !!![0m
[33m- 6102 !!! IGNORED !!![0m
[33m- 6103 !!! IGNORED !!![0m
[33m- 6104 !!! IGNORED !!![0m
[36mRun completed in 21 minutes, 54 seconds.[0m
[36mTotal number of tests run: 1015[0m
[36mSuites: completed 2, aborted 0[0m
[36mTests: succeeded 886, failed 129, canceled 0, ignored 1137, pending 0[0m
[31m*** 129 TESTS FAILED ***[0m
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 22:29 min
[INFO] Finished at: 2015-11-06T11:12:12+08:00
[INFO] Final Memory: 36M/2475M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.scalatest:scalatest-maven-plugin:1.0:test (test) on project spark-calcite-parser: There are test failures -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
